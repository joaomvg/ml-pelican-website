<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>"Convolutional Neural Network" â€” Data Science and Machine Learning</title>
	<meta name="description" content="Title: "Convolutional Neural Network"; Date: 2021-01-25; Author: Joao Gomes">
	<meta name="author" content="Joao Gomes">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
	<!--[if lt IE 9]>
		<script src="/theme/html5.js"></script>
		<![endif]-->
	<link href="/theme/css/ipython.css" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/bootswatch/3.2.0/simplex/bootstrap.min.css" rel="stylesheet">
	<link href="/theme/css/local.css" rel="stylesheet">
	<link href="/theme/css/pygments.css" rel="stylesheet">
</head>
<body>
<div class="container">
	<div class="page-header">
		<h1><a href="/">Data Science and Machine Learning</a>
			<br>	</div>
	<div class="row">
		<div class="col-md-8 col-md-offset-2">
<div class="article" itemscope itemtype="http://schema.org/BlogPosting">
	<div class="text-center article-header">
		<h1 itemprop="name headline" class="article-title">"Convolutional Neural Network"</h1>
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<h4 itemprop="name">Joao Gomes</h4>
		</span>
		<time datetime="2021-01-25T00:00:00+01:00" itemprop="datePublished">Mon 25 January 2021</time>
	</div>
	<div>
		Category:
		<span itemprop="articleSection">
			<a href="/category/machine-learning.html" rel="category">Machine Learning</a>
		</span>
	</div>
 
	<div>
		Tags:
		<span itemprop="keywords">
			<a href="/tag/data-science.html" rel="tag">data science</a>
		</span>
	</div>
	<div itemprop="articleBody" class="article-body"><ol>
<li><a href="#def1">Convolutional Neural Network</a></li>
<li><a href="#python">Python implementation</a></li>
</ol>
<p><a name="def1"></a></p>
<h3><strong>1. Convolutional neural network</strong></h3>
<p>A convolutional neural network incorporates geometrical features in the learning algorithm. The neural network uses the convolution operation to capture the spatial correlation between data at different positions. This type of architecture is most suitable for vision tasks like image classification. The convolution amounts to a linear operation over smaller regions of the image, much like a moving average, except that this is not normalized. More explicitly, for a matrix of pixels <span class="math">\(m_{ij}\)</span> with size <span class="math">\(N\times N\)</span>, and a kernel <span class="math">\(K\)</span> with weights <span class="math">\(w_{\mu\nu}\)</span> and size <span class="math">\(k\times k\)</span> we calculate</p>
<div class="math">$$ K(m_{ij})=\sum_{\mu,\nu=0}^{k-1} w_{\mu\nu}m_{i+\mu,j+\nu}+b$$</div>
<p>where <span class="math">\(b\)</span> is a bias. The convolution operation is depicted in the picture below for a kernel of size <span class="math">\(4\times4\)</span>.</p>
<p><img alt="" height="350" src="/images/conv_img.png" style="display: block; margin: 0 auto" width="350"> </p>
<p>The convolution runs over the pixels where the kernel is allowed to be inside the image. That is, we have <span class="math">\(m_{i+\mu,j+\nu}=m_{i'j'}\)</span>. We can relax this condition and allow for a padding <span class="math">\(P\)</span> that we add at the beginning and end of the image. Then we ensure that all pixels living in the padding regions are zero, that is, <span class="math">\(m_{ij}=0\)</span> for <span class="math">\(i,j\in [-P,0[\,\cup\, ]N-1,N-1+P]\)</span>. One can also define a stride <span class="math">\(S\)</span> that determines the pixels the kernel runs over, that is, <span class="math">\((i,j)=(-P\text{ mod}(S),-P \text{ mod(S)})\)</span>. Therefore, if the pixel matrix has size <span class="math">\(N\times N\)</span> then the output of the convolution has shape <span class="math">\(N'\times N'\)</span> with</p>
<div class="math">$$N'=(N+2P-k+1)//S+1$$</div>
<p>After the convolution operation, we apply a non-linear activation function on each element of the matrix <span class="math">\(m'_{i'j'}\)</span>. The result is the output of a convolutional layer.  Besides, one can create channels whereby we apply multiple kernels to the same input. So if the kernel <span class="math">\(K^c\)</span> has C channels, the convolutional layer's output is <span class="math">\(C\)</span> matrices <span class="math">\(m^c_{i'j'}\)</span>. Similarly, for each matrix <span class="math">\(m^c_{i'j'}\)</span> one can further apply a kernel with <span class="math">\(C'\)</span> channels. The result of using first the kernel <span class="math">\(K^c\)</span>, and then <span class="math">\(K^{c'}\)</span> is <span class="math">\(C\times C'\)</span> matrices.</p>
<p>After the convolutional layers, the resulting matrix is flattened and added as an input to a feed-forward neural network. Below we show this series of operations.</p>
<p><img alt="" height="350" src="/images/conv_layers.png" style="display: block; margin: 0 auto" width="350"> </p>
<p><a name="python"></a></p>
<h3><strong>2. Python implementation</strong></h3>
<p>Using the <strong>grad_lib</strong> library that we have built in the previous post, we can build neural networks more easily. First we define a convolutional neural network</p>
<h3>Convolutional Neural Network (1 channel)</h3>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">grad_lib</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">ReLU</span><span class="p">,</span> <span class="n">FeedForward</span><span class="p">,</span> <span class="n">Softmax</span><span class="p">,</span> <span class="n">LinearLayer</span><span class="p">,</span> <span class="n">Log</span><span class="p">,</span> <span class="n">DataSet</span> 

<span class="k">class</span> <span class="nc">ConvNet2D</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">kernel_size</span><span class="p">,</span><span class="n">img_size</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">),</span><span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Convolutional Layer with 2D kernel</span>

<span class="sd">        Args:</span>
<span class="sd">            kernel_size ([type]): [description]</span>
<span class="sd">            img_size (tuple, optional): [description]. Defaults to (8,8).</span>
<span class="sd">            stride (int, optional): [description]. Defaults to 1.</span>
<span class="sd">            padding (int, optional): [description]. Defaults to 0.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">img_size</span><span class="o">=</span><span class="n">img_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="o">=</span><span class="n">stride</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="o">=</span><span class="n">padding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_shape</span><span class="o">=</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span><span class="n">kernel_size</span><span class="p">)</span>

        <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">init_param</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">=</span><span class="n">Tensor</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span><span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="o">=</span><span class="n">Tensor</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">get_out_dim</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="o">=</span><span class="n">ReLU</span><span class="p">()</span>

        <span class="sd">&quot;&quot;&quot;trainable Tensors</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span><span class="o">=</span><span class="p">{</span><span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span>
                        <span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
                        <span class="p">}</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;forward</span>

<span class="sd">        Args:</span>
<span class="sd">            x (Tensor): (batch,S,S)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor: (batch,num_neurons)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x_tensor</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">array</span><span class="p">)</span>
        <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">*</span><span class="n">x_tensor</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
        <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> 

    <span class="k">def</span> <span class="nf">init_param</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">weight</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_shape</span><span class="p">)</span>
        <span class="n">bias</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span> 

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;transform batch of images</span>

<span class="sd">        Args:</span>
<span class="sd">            x (numpy.array): (batch,S,S)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor: (kernel_size**2,batch,num_neurons)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># x: array</span>
        <span class="n">i_f</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">j_f</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">out_list</span><span class="o">=</span><span class="p">[]</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">out</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">i_f</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">j_f</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">):</span>
                    <span class="n">z</span><span class="o">=</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">j</span><span class="p">:</span><span class="n">j</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">out</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">out</span><span class="p">,</span><span class="n">z</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">out_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>

        <span class="c1">#out_list: [batch,num_neurons,kernel_size**2]</span>
        <span class="n">out</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">out_list</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_out_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">test</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">img_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="bp">self</span><span class="o">.</span><span class="n">img_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">test</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">return</span> <span class="n">size</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
</pre></div>


<h3>Convolutional neural network model for multi-class problem</h3>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ConvClassifier</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">img_size</span><span class="p">,</span><span class="n">hidden_dim</span><span class="p">,</span><span class="n">out_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Convolutional Neural Network Classifier</span>

<span class="sd">        Args:</span>
<span class="sd">            img_size (tuple): (width,height)</span>
<span class="sd">            hidden_dim (int): number of hidden neurons</span>
<span class="sd">            out_dim (int, optional): number of class. Defaults to 1.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="o">=</span><span class="n">ConvNet2D</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">img_size</span><span class="o">=</span><span class="n">img_size</span><span class="p">)</span>
        <span class="n">in_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="o">.</span><span class="n">out_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_layer</span><span class="o">=</span><span class="n">LinearLayer</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span><span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span><span class="o">=</span><span class="n">LinearLayer</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span><span class="n">out_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="o">=</span><span class="n">Softmax</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="o">=</span><span class="n">ReLU</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;[summary]</span>

<span class="sd">        Args:</span>
<span class="sd">            x (numpy.array): input must be array, not Tensor</span>

<span class="sd">        Returns:</span>
<span class="sd">            probability: (batch,out_dim)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">in_layer</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">prob</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">prob</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">train</span><span class="p">():</span>
        <span class="n">Tensor</span><span class="o">.</span><span class="n">calc_grad</span><span class="o">=</span><span class="kc">True</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">eval</span><span class="p">():</span>
        <span class="n">Tensor</span><span class="o">.</span><span class="n">calc_grad</span><span class="o">=</span><span class="kc">False</span> 
</pre></div>


<h3>Cross-Entropy loss and Optimizer</h3>
<p>We also need to write the cross-entropy loss function and optimizer.</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">CrossEntropyLoss</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">model</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">=</span><span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="o">=</span><span class="n">Log</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">prob</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;loss function</span>

<span class="sd">        Args:</span>
<span class="sd">            prob (probability Tensor): (batch,num_classes)</span>
<span class="sd">            y (array): (batch,1)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">bsz</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">prob_</span><span class="o">=</span><span class="n">prob</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">bsz</span><span class="p">),</span><span class="n">y</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]]</span>
        <span class="n">loss</span><span class="o">=-</span><span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">prob_</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">bsz</span><span class="p">)</span><span class="o">*</span><span class="n">loss</span> 

        <span class="bp">self</span><span class="o">.</span><span class="n">back_grads</span><span class="o">=</span><span class="n">loss</span><span class="o">.</span><span class="n">grad</span>

        <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">array</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">grads</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">back_grads</span>

<span class="k">class</span> <span class="nc">Optimizer</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">model</span><span class="p">,</span><span class="n">lr</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">=</span><span class="n">model</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="o">=</span><span class="n">lr</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">tensors</span><span class="o">=</span><span class="p">{}</span>
        <span class="k">for</span> <span class="n">_</span><span class="p">,</span><span class="n">obj</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span><span class="s1">&#39;trainable&#39;</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">tensors</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">obj</span><span class="o">.</span><span class="n">trainable</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensors</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
                <span class="n">grad</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">=</span><span class="n">tensor</span><span class="o">.</span><span class="n">grad</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
                <span class="n">tensor</span><span class="o">.</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">grad</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;none&#39;</span><span class="p">:</span><span class="mi">0</span><span class="p">}</span>
                <span class="n">tensor</span><span class="o">.</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span> 

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">grads</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensors</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">idx</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">grads</span><span class="p">:</span>
                    <span class="n">tensor</span><span class="o">.</span><span class="n">array</span><span class="o">-=</span><span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">grads</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;No grads!&#39;</span><span class="p">)</span>
</pre></div>


<h3>Training function</h3>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">loss</span><span class="p">,</span><span class="n">optimizer</span><span class="p">,</span><span class="n">data_loader</span><span class="p">,</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">L</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)):</span>
        <span class="n">total_loss</span><span class="o">=</span><span class="mi">0</span>

        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
            <span class="n">batch_x</span><span class="p">,</span><span class="n">batch_y</span><span class="o">=</span><span class="n">batch</span>
            <span class="n">bsz</span><span class="o">=</span><span class="n">batch_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">pred</span><span class="o">=</span><span class="n">model</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span>
            <span class="n">total_loss</span><span class="o">+=</span><span class="n">loss</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">batch_y</span><span class="o">.</span><span class="n">array</span><span class="p">)</span><span class="o">*</span><span class="n">bsz</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Epoch: &#39;</span><span class="p">,</span><span class="n">epoch</span><span class="p">,</span><span class="s2">&quot; Loss: &quot;</span><span class="p">,</span><span class="n">total_loss</span><span class="o">/</span><span class="n">L</span><span class="p">)</span>
</pre></div>


<h2>Example</h2>
<p>Use the <span class="math">\(8\times8\)</span> version of the MNIST dataset as a toy-model. </p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>

<span class="n">data</span><span class="o">=</span><span class="n">load_digits</span><span class="p">()</span>
<span class="n">imgs</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>
<span class="n">target</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span>

<span class="c1">#normalize the pixels for easier training</span>
<span class="n">img_norm</span><span class="o">=</span><span class="n">imgs</span><span class="o">/</span><span class="mi">16</span>

<span class="c1">#create iterator</span>
<span class="n">data_loader</span><span class="o">=</span><span class="n">DataSet</span><span class="p">(</span><span class="n">img_norm</span><span class="p">,</span><span class="n">target</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>

<span class="n">model</span><span class="o">=</span><span class="n">ConvClassifier</span><span class="p">(</span><span class="n">img_size</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">),</span><span class="n">hidden_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">out_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">loss</span><span class="o">=</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">opt</span><span class="o">=</span><span class="n">Optimizer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">loss</span><span class="p">,</span><span class="n">opt</span><span class="p">,</span><span class="n">data_loader</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
</pre></div>


<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
	<hr>
	<h2>Comments</h2>
</div>
		</div>
	</div> 	<!-- <hr> -->
</div> <!-- /container -->
<footer class="aw-footer bg-danger">
	<div class="container"> <!-- footer -->
		<div class="row">
			<div class="col-md-10 col-md-offset-1">
				<div class="row">
					<div class="col-md-3">
						<h4>Navigation</h4>
						<ul class="list-unstyled my-list-style">
							<li><a href="">Data Science and Machine Learning</a></li>
							<li><a href="/pages/about-me.html"><i class="fa fa-About me "></i> About me</a></li>
							<li><a href="/feeds/all.atom.xml" type="application/atom+xml"><i class="fa fa-rss "></i> atom</a></li>
						</ul>
					</div>
					<div class="col-md-3">
						<h4>Author</h4>
						<ul class="list-unstyled my-list-style">
							<li><a href="https://www.linkedin.com/in/joaomvg/">LinkedIn</a></li>
							<li><a href="https://github.com/joaomvg">GitHub</a></li>
						</ul>
					</div>
					<div class="col-md-3">
						<h4>Categories</h4>
						<ul class="list-unstyled my-list-style">
							<li><a href="/category/data-science.html">Data Science (1)</a></li>
							<li><a href="/category/machine-learning.html">Machine Learning (21)</a></li>
							<li><a href="/category/python.html">Python (1)</a></li>
							<li><a href="/category/statistics.html">Statistics (2)</a></li>
						</ul>
					</div>
					<div class="col-md-3">
						<h4>Links</h4>
						<ul class="list-unstyled my-list-style">
							<li><a href="archives.html">Archives</a></li>
							<li><a href="tags.html">Tags</a></li>
						</ul>
					</div>
				</div>
			</div>
		</div>
	</div>
</footer>
<div class="container">
	<div class="row">
		<div class="col-md-12 text-center center-block aw-bottom">
			<p>&copy; Joao Gomes 2016</p>
			<p>Powered by Pelican</p>
		</div>
	</div>
</div>
<!-- JavaScript -->
<script src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
<script type="text/javascript">
jQuery(document).ready(function($) {
	$("div.collapseheader").click(function () {
		$header = $(this).children("span").first();
		$codearea = $(this).children(".input_area");
		$codearea.slideToggle(500, function () {
			$header.text(function () {
				return $codearea.is(":visible") ? "Collapse Code" : "Expand Code";
			});
		});
	});
});
</script>
</body>
</html>