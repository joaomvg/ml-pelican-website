<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>"Expectation-Maximization" â€” Data Science and Machine Learning</title>
	<meta name="description" content="Title: "Expectation-Maximization"; Date: 2020-07-15; Author: Joao Gomes">
	<meta name="author" content="Joao Gomes">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
	<!--[if lt IE 9]>
		<script src="/theme/html5.js"></script>
		<![endif]-->
	<link href="/theme/css/ipython.css" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/bootswatch/3.2.0/simplex/bootstrap.min.css" rel="stylesheet">
	<link href="/theme/css/local.css" rel="stylesheet">
	<link href="/theme/css/pygments.css" rel="stylesheet">
</head>
<body>
<div class="container">
	<div class="page-header">
		<h1><a href="/">Data Science and Machine Learning</a>
			<br>	</div>
	<div class="row">
		<div class="col-md-8 col-md-offset-2">
<div class="article" itemscope itemtype="http://schema.org/BlogPosting">
	<div class="text-center article-header">
		<h1 itemprop="name headline" class="article-title">"Expectation-Maximization"</h1>
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<h4 itemprop="name">Joao Gomes</h4>
		</span>
		<time datetime="2020-07-15T00:00:00+02:00" itemprop="datePublished">Wed 15 July 2020</time>
	</div>
	<div>
		Category:
		<span itemprop="articleSection">
			<a href="/category/data-science.html" rel="category">Data Science</a>
		</span>
	</div>
 
	<div>
		Tags:
		<span itemprop="keywords">
			<a href="/tag/data-science.html" rel="tag">data science</a>
		</span>
	</div>
	<div itemprop="articleBody" class="article-body"><p>Often we have to deal with hidden variables in machine learning problems. The maximum-likelihood algorithm requires "integrating" over these hidden variables if we want to compare with the observed distribution. However this can lead to a serious problem since we have to deal with sums inside the logarithms. That is, we are instructed to maximize the log-likelihood quantity
</p>
<div class="math">$$\sum_i\ln p(x_i)=\sum_i\ln\Big( \sum_h p(x_i,h)\Big)$$</div>
<p>
where <span class="math">\(h\)</span> is the hidden variable and <span class="math">\(x_i\)</span> is the observed one. Except for simple problems, having two sums turns the problem computationally infeasible, especially if the hidden variable is continuous. To deal with this issue we use the concavity property of the logarithm to approximate
</p>
<div class="math">$$\ln\Big( \sum_h p(x_i,h)\Big)\geq \sum_hq(h)\ln\Big(\frac{p(x_i,h)}{q(h)}\Big)$$</div>
<p>
where <span class="math">\(q(h)\)</span> is an unknown distribution that we will want to fix. Further we write
</p>
<div class="math">$$\ln p(x_i)=\sum_hq(h)\ln\Big(\frac{p(x_i,h)}{q(h)}\Big)+R_i$$</div>
<p>
where the remaining <span class="math">\(R_i\)</span> is given by
</p>
<div class="math">$$R_i=-\sum_h q(h)\ln\Big(\frac{p(h|x_i)}{q(h)}\Big)=KL(p(h|x_i)||q(h))$$</div>
<p>
which is the Kullback-Leibler divergence. Since <span class="math">\(R_i\geq 0\)</span> by definition, we have that
</p>
<div class="math">$$\ln p(x_i|\theta)\geq \langle \ln p(x_i,h|\theta)\rangle_{q(h)}-\langle \ln q(h)\rangle_{q(h)}$$</div>
<p>
where we have introduced prior parameters <span class="math">\(\theta\)</span>, without lack of generality. The lower bound is saturated provided we choose 
</p>
<div class="math">$$\text{E-step:}\quad q(h_i)=p(h_i|x_i,\theta_0)$$</div>
<p>
This is also known as expectation E-step. Note that we have a distribution <span class="math">\(q(h_i)\)</span> for each sample, as it is determined by <span class="math">\(x_i,\theta_0\)</span>. However, this step does not solve the maximum-likelihood problem because we still have to fix the parameter <span class="math">\(\theta\)</span>. What we do next is to maximize the lower bound by choosing <span class="math">\(\theta\)</span> keeping <span class="math">\(q(h)\)</span> fixed, that is,
</p>
<div class="math">$$\text{M-step:}\quad \frac{\partial}{\partial \theta}\langle \ln p(x_i,h|\theta)\rangle_{q(h)}=0$$</div>
<p>Lets take an example that can help clarify some of these ideas. Consider the model which is a mixture of two normal distributions:
</p>
<div class="math">$$p(x,c)=\phi(x|\mu_c,\sigma_c)\pi_c,\quad c=0,1$$</div>
<p>
where <span class="math">\(\phi(x|\mu,\sigma)\)</span> is a normal distribution with mean <span class="math">\(\mu\)</span> and variance <span class="math">\(\sigma\)</span>, and <span class="math">\(\pi_c=p(c)\)</span> with <span class="math">\(\pi_0+\pi_1=1\)</span>. In this example <span class="math">\(\theta\equiv \mu,\sigma\)</span>, and the hidden variable is <span class="math">\(h\equiv c\)</span>. </p>
<p>In the E-step we calculate:
</p>
<div class="math">$$\text{E-step:}\quad q(h)=p(h|x,\mu_h,\sigma_h)=\frac{\phi(x|\mu_h,\sigma_h)\pi_h}{\sum_c \phi(x|\mu_c,\sigma_c)\pi_c}$$</div>
<p>
We write <span class="math">\(q(h_i=0)=\gamma_i(x_i)\)</span> and <span class="math">\(q(h_i=1)=1-\gamma_i(x_i)\)</span> for each sample <span class="math">\(x_i\)</span>, with <span class="math">\(\gamma\)</span> given by the ratio above. The initial parameters <span class="math">\(\mu,\sigma\)</span> are arbitrary.</p>
<p>The maximization step consists in maximizing the lower bound of the log-likelihood, hence
</p>
<div class="math">$$\begin{aligned}\text{M-step:}\quad &amp;\gamma\ln p(x,h=0|\mu,\sigma)+(1-\gamma)\ln p(x,h=1|\mu,\sigma)\\
=&amp;\gamma \ln \phi(x|\mu_0,\sigma_0)+(1-\gamma)\ln \phi(x|\mu_1,\sigma_1)-\gamma\frac{1}{2}\ln\sigma_0-(1-\gamma)\frac{1}{2}\ln\sigma_1+\ldots\\
=&amp; -\gamma \frac{(x-\mu_0)^2}{2\sigma_0^2}-(1-\gamma) \frac{(x-\mu_1)^2}{2\sigma_1^2}-\gamma\frac{1}{2}\ln\sigma_0-(1-\gamma)\frac{1}{2}\ln\sigma_1+\ldots\end{aligned}$$</div>
<p>
where <span class="math">\(\ldots\)</span> do not depend on <span class="math">\(\mu,\sigma\)</span>. We need to sum over all samples, so the maximum is calculated
</p>
<div class="math">$$\mu_0=\frac{\sum_i x_i\gamma_i}{\sum_i \gamma_i},\;\mu_1=\frac{\sum_i x_i(1-\gamma_i)}{\sum_i (1-\gamma_i)}$$</div>
<p>
and 
</p>
<div class="math">$$\sigma_0=\frac{\sum_i\gamma_i(x_i-\mu_0)^2}{\sum_i\gamma_i},\quad \sigma_1=\frac{\sum_i(1-\gamma_i)(x_i-\mu_1)^2}{\sum_i(1-\gamma_i)}$$</div>
<p>
Maximizing relatively to the probabilities <span class="math">\(\pi\)</span> gives
</p>
<div class="math">$$\pi_0=\frac{1}{n}\sum_i\gamma_i,\;\pi_1=1-\pi_0$$</div>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
	<hr>
	<h2>Comments</h2>
</div>
		</div>
	</div> 	<!-- <hr> -->
</div> <!-- /container -->
<footer class="aw-footer bg-danger">
	<div class="container"> <!-- footer -->
		<div class="row">
			<div class="col-md-10 col-md-offset-1">
				<div class="row">
					<div class="col-md-3">
						<h4>Navigation</h4>
						<ul class="list-unstyled my-list-style">
							<li><a href="">Data Science and Machine Learning</a></li>
							<li><a href="/pages/about-me.html"><i class="fa fa-About me "></i> About me</a></li>
							<li><a href="/feeds/all.atom.xml" type="application/atom+xml"><i class="fa fa-rss "></i> atom</a></li>
						</ul>
					</div>
					<div class="col-md-3">
						<h4>Author</h4>
						<ul class="list-unstyled my-list-style">
							<li><a href="https://www.linkedin.com/in/joaomvg/">LinkedIn</a></li>
							<li><a href="https://github.com/joaomvg">GitHub</a></li>
						</ul>
					</div>
					<div class="col-md-3">
						<h4>Categories</h4>
						<ul class="list-unstyled my-list-style">
							<li><a href="/category/data-science.html">Data Science (1)</a></li>
							<li><a href="/category/machine-learning.html">Machine Learning (10)</a></li>
							<li><a href="/category/statistics.html">Statistics (2)</a></li>
						</ul>
					</div>
					<div class="col-md-3">
						<h4>Links</h4>
						<ul class="list-unstyled my-list-style">
							<li><a href="archives.html">Archives</a></li>
							<li><a href="tags.html">Tags</a></li>
						</ul>
					</div>
				</div>
			</div>
		</div>
	</div>
</footer>
<div class="container">
	<div class="row">
		<div class="col-md-12 text-center center-block aw-bottom">
			<p>&copy; Joao Gomes 2016</p>
			<p>Powered by Pelican</p>
		</div>
	</div>
</div>
<!-- JavaScript -->
<script src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
<script type="text/javascript">
jQuery(document).ready(function($) {
	$("div.collapseheader").click(function () {
		$header = $(this).children("span").first();
		$codearea = $(this).children(".input_area");
		$codearea.slideToggle(500, function () {
			$header.text(function () {
				return $codearea.is(":visible") ? "Collapse Code" : "Expand Code";
			});
		});
	});
});
</script>
</body>
</html>