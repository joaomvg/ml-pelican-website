<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Data Science and Machine Learning</title><link href="/" rel="alternate"></link><link href="/feeds/all.atom.xml" rel="self"></link><id>/</id><updated>2020-07-29T00:00:00+02:00</updated><entry><title>"K-Nearest Neighbors"</title><link href="/k-nearest-neighbors.html" rel="alternate"></link><published>2020-07-29T00:00:00+02:00</published><updated>2020-07-29T00:00:00+02:00</updated><author><name>Joao Gomes</name></author><id>tag:None,2020-07-29:/k-nearest-neighbors.html</id><summary type="html">&lt;p&gt;The Nearest Neighbors algorithm is explained. This includes theoretical derivation, python implementation and also decision boundary.&lt;/p&gt;</summary><content type="html">&lt;ol&gt;
&lt;li&gt;&lt;a href="#def1"&gt;KNN Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#decision"&gt;Decision Boundary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#python"&gt;Python implementation: Classification&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a name="def1"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;1. KNN algorithm&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The nearest-neighbors algorithm considers the &lt;span class="math"&gt;\(K\)&lt;/span&gt; nearest neighbors of a datapoint &lt;span class="math"&gt;\(x\)&lt;/span&gt; to predict its label. In the figure below, we have represented a binary classification problem (colors red and green for classes 0,1 respectively) with datapoints living in a 2-dimensional feature space .&lt;/p&gt;
&lt;p&gt;&lt;img alt="" height="300" src="/images/knn.png" style="display: block; margin: 0 auto" width="300"&gt;&lt;/p&gt;
&lt;p&gt;The algorithm consists in attributing the majority class amongts the &lt;span class="math"&gt;\(K\)&lt;/span&gt;-nearest neighbors. In the example above we consider the 3 nearest neighbors using euclidean distances. Mathematically the predictor &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt; is given by
&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{y}(x)=\text{argmax}_{0,1}\{n_0(x),n_1(x): x\in D_K(x)\}$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(D_K(x)\)&lt;/span&gt; is the set of &lt;span class="math"&gt;\(K\)&lt;/span&gt;-nearest neighbors and &lt;span class="math"&gt;\(n_{0,1}(x)\)&lt;/span&gt; are the number of neighbors in &lt;span class="math"&gt;\(D_K\)&lt;/span&gt; with class &lt;span class="math"&gt;\(0,1\)&lt;/span&gt; respectively. The ratio &lt;span class="math"&gt;\(n_{0,1}/K\)&lt;/span&gt; are the corresponding probabilities. For a multiclass problem the predictor follows a similar logic except that we choose the majority class for which &lt;span class="math"&gt;\(n_i(x)\)&lt;/span&gt; is the maximum, with &lt;span class="math"&gt;\(i\)&lt;/span&gt; denoting the possible classes. &lt;/p&gt;
&lt;p&gt;A probabilistic approach to nearest neighbors is as follows. We consider the distribution
&lt;/p&gt;
&lt;div class="math"&gt;$$p(x|c)=\frac{1}{N_c\sqrt{2\pi\sigma^2}^{D/2}}\sum_{n\in\text{class c},n=1}^{n=N_c}e^{-\frac{\|x-\mu_n\|^2}{2\sigma^2}}$$&lt;/div&gt;
&lt;p&gt;
with &lt;span class="math"&gt;\(N_c\)&lt;/span&gt; the number of points with class &lt;span class="math"&gt;\(c\)&lt;/span&gt; which have coordinates &lt;span class="math"&gt;\(\mu_c\)&lt;/span&gt;, and &lt;span class="math"&gt;\(x\)&lt;/span&gt; lives in &lt;span class="math"&gt;\(D\)&lt;/span&gt; dimensions. The probabilities &lt;span class="math"&gt;\(p(c)\)&lt;/span&gt; are determined from the observed frequencies, that is,
&lt;/p&gt;
&lt;div class="math"&gt;$$p(c=0)=\frac{N_0}{N_0+N_1},\;p(c=1)=\frac{N_1}{N_0+N_1}$$&lt;/div&gt;
&lt;p&gt;
The ratio of the likelihoods is then&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{p(c=1|x)}{p(c=0|x)}=\frac{p(x|c=1)p(c=1)}{p(x|c=0)p(c=0)}=\frac{\sum_{n=1}^{n=N_1}e^{-\frac{\|x-\mu_n\|^2}{2\sigma^2}}}{\sum_{n=1}^{n=N_0}e^{-\frac{\|x-\mu_n\|^2}{2\sigma^2}}}$$&lt;/div&gt;
&lt;p&gt;Take &lt;span class="math"&gt;\(d(x)\)&lt;/span&gt; as the largest distance within the set of &lt;span class="math"&gt;\(K\)&lt;/span&gt;-nearest neighbors of the datapoint &lt;span class="math"&gt;\(x\)&lt;/span&gt;. If the variance &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; is of order &lt;span class="math"&gt;\(\sim d\)&lt;/span&gt; then the exponentials with arguments &lt;span class="math"&gt;\(\|x-\mu\|^2&amp;gt;d^2\)&lt;/span&gt; can be neglected while for &lt;span class="math"&gt;\(\|x-\mu\|^2&amp;lt;d^2\)&lt;/span&gt; the exponential becomes of order one, and so we approximate
&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{\sum_{n=1}^{n=N_1}e^{-\frac{\|x-\mu_n\|^2}{2\sigma^2}}}{\sum_{n=1}^{n=N_0}e^{-\frac{\|x-\mu_n\|^2}{2\sigma^2}}}\simeq \frac{\sum_{i\in D_K^1(x)} e^{-\frac{\|x-\mu_i\|^2}{2\sigma^2}}}{\sum_{j\in D_K^0(x)} e^{-\frac{\|x-\mu_j\|^2}{2\sigma^2}}}\sim\frac{\#i}{\#j}$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(D^{0,1}_K(x)\)&lt;/span&gt; are the nearest neihgbors of &lt;span class="math"&gt;\(x\)&lt;/span&gt; with classes &lt;span class="math"&gt;\(0,1\)&lt;/span&gt; respectively, and &lt;span class="math"&gt;\(\#i+\#j=K\)&lt;/span&gt;. In theory this would reproduce the K-nearest neighbors predictor. However, this would require that for each &lt;span class="math"&gt;\(x\)&lt;/span&gt; the threshold &lt;span class="math"&gt;\(d\)&lt;/span&gt; is approximately constant, which may not happen in practice. The algorithm is however exact as &lt;span class="math"&gt;\(\sigma\rightarrow 0\)&lt;/span&gt; for which only the nearest neighbor is picked.&lt;/p&gt;
&lt;p&gt;In regression we calculate instead the average of &lt;span class="math"&gt;\(K\)&lt;/span&gt;-nearest neighbor targets. That is,
&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{y}(x)=\frac{1}{K}\sum_{i\in D_K(x)}y_i$$&lt;/div&gt;
&lt;p&gt;Consider different datasets whereby the positions of the datapoints &lt;span class="math"&gt;\(x\)&lt;/span&gt; do not change but the target &lt;span class="math"&gt;\(y\)&lt;/span&gt; is drawn randomly as &lt;span class="math"&gt;\(f+\epsilon\)&lt;/span&gt; where &lt;span class="math"&gt;\(f\)&lt;/span&gt; is the true target and &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; is a normally distributed random variable with mean zero and variance &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt;. The bias is thus calculated as
&lt;/p&gt;
&lt;div class="math"&gt;$$\text{Bias}(x)=f(x)-\text{E}[\hat{f}(x)]=f(x)-\frac{1}{K}\sum_{i\in D_K(x)}f(x_i)$$&lt;/div&gt;
&lt;p&gt; 
For &lt;span class="math"&gt;\(K\)&lt;/span&gt; small the nearest neighbors will have targets &lt;span class="math"&gt;\(f(x_i)\)&lt;/span&gt; that are approximately equal to &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;, by continuity. As such, the bias is small for small values of &lt;span class="math"&gt;\(K\)&lt;/span&gt;. However, as &lt;span class="math"&gt;\(K\)&lt;/span&gt; grows we are probing datapoints that are farther and farther away and thus more distinct from &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;, which in general will make the bias increase. &lt;/p&gt;
&lt;p&gt;On the other hand, the variance at a point &lt;span class="math"&gt;\(x\)&lt;/span&gt;, that is,
&lt;/p&gt;
&lt;div class="math"&gt;$$\text{Var}(\hat{f})|_x=\text{E}[(\hat{f}(x)-\text{E}[\hat{f}(x)])^2]$$&lt;/div&gt;
&lt;p&gt;
becomes equal to
&lt;/p&gt;
&lt;div class="math"&gt;$$\text{Var}(\hat{f})=\frac{\sigma^2}{K}$$&lt;/div&gt;
&lt;p&gt;
Therefore, for large values of &lt;span class="math"&gt;\(K\)&lt;/span&gt; the variance decreases, while it is larger for smaller values of &lt;span class="math"&gt;\(K\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a name="decision"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;2. Decision Boundary&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;In the picture below, we draw the decision boundary for a &lt;span class="math"&gt;\(K=1\)&lt;/span&gt; nearest neighbor. For any point located inside the polygon (hard lines) the nearest neighbor is &lt;span class="math"&gt;\(P_1\)&lt;/span&gt; and so the predicted target is &lt;span class="math"&gt;\(f(P_1)\)&lt;/span&gt; in that region.
&lt;img alt="" height="400" src="/images/knn_decision.png" style="display: block; margin: 0 auto" width="400"&gt;&lt;/p&gt;
&lt;p&gt;To construct the decision boundary we draw lines joining each point to &lt;span class="math"&gt;\(P_1\)&lt;/span&gt; and for each of these we draw the corresponding bisector. For example, consider the points &lt;span class="math"&gt;\(P_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(P_2\)&lt;/span&gt;. For any point along the bisector of &lt;span class="math"&gt;\(\overline{P_1P_2}\)&lt;/span&gt;, the distance to &lt;span class="math"&gt;\(P_1\)&lt;/span&gt; is the same as the distance to &lt;span class="math"&gt;\(P_2\)&lt;/span&gt;. Therefore, the polygon formed by drawing all the bisectors bounds a region in which the nearest point is &lt;span class="math"&gt;\(P_1\)&lt;/span&gt;. &lt;/p&gt;
&lt;p&gt;For &lt;span class="math"&gt;\(K&amp;gt;1\)&lt;/span&gt;, we have to proceed slightly different. First we construct the &lt;span class="math"&gt;\(K=1\)&lt;/span&gt; decision boundary- this determines the nearest neighbor. Call this point &lt;span class="math"&gt;\(N_1\)&lt;/span&gt;, the first neighbor. Second, we pretend that the point &lt;span class="math"&gt;\(N_1\)&lt;/span&gt; is not part of the dataset and proceed as in the first step. The corresponding nearest neighbor &lt;span class="math"&gt;\(N_2\)&lt;/span&gt; is then the second nearest neighbor while including &lt;span class="math"&gt;\(N_1\)&lt;/span&gt;. We proceed iteratively after &lt;span class="math"&gt;\(K\)&lt;/span&gt; steps. The decision boundary is then determined by joining the &lt;span class="math"&gt;\(K=1\)&lt;/span&gt; polygons of each &lt;span class="math"&gt;\(N_1,N_2,\ldots N_K\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a name="python"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;3. Python Implementation: Classification&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Define KNN class with fit and call methods. The fit method memorizes the training data and the call method retrieves the predictor.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;KNN&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;classes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;
        &lt;span class="n"&gt;classes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;classes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classes&lt;/span&gt;&lt;span class="p"&gt;)}&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classes&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;classes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__call__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;
        &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;
            &lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argpartition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)[:&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="n"&gt;ypred&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;ypred&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ypred&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
            &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ypred&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;As an example, load Iris dataset and also the built-in SKlearn K-nearest neighbors algorithm.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;load_iris&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.neighbors&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;KNeighborsClassifier&lt;/span&gt;

&lt;span class="n"&gt;iris&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;load_iris&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;iris&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;iris&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;target&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;#train &amp;amp; test split&lt;/span&gt;
&lt;span class="n"&gt;indices&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;indices&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;xtrain&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;indices&lt;/span&gt;&lt;span class="p"&gt;][:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;xtest&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;indices&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;

&lt;span class="n"&gt;ytrain&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;indices&lt;/span&gt;&lt;span class="p"&gt;][:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;ytest&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;indices&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;

&lt;span class="n"&gt;knn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;KNN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;#the class above&lt;/span&gt;
&lt;span class="n"&gt;Kneighbor&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;KNeighborsClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_neighbors&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;#the SKlearn class&lt;/span&gt;

&lt;span class="n"&gt;knn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xtrain&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;ytrain&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ytest&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;knn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xtest&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;Kneighbor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xtrain&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;ytrain&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ytest&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;Kneighbor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xtest&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Retrieving exactly the same accuracy.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Machine Learning"></category><category term="data science"></category></entry><entry><title>"Expectation-Maximization"</title><link href="/expectation-maximization.html" rel="alternate"></link><published>2020-07-15T00:00:00+02:00</published><updated>2020-07-15T00:00:00+02:00</updated><author><name>Joao Gomes</name></author><id>tag:None,2020-07-15:/expectation-maximization.html</id><summary type="html">&lt;p&gt;The expectation-maximization algorithm.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Often we have to deal with hidden variables in machine learning problems. The maximum-likelihood algorithm requires "integrating" over these hidden variables if we want to compare with the observed distribution. However this can lead to a serious problem since we have to deal with sums inside the logarithms. That is, we are instructed to maximize the log-likelihood quantity
&lt;/p&gt;
&lt;div class="math"&gt;$$\sum_i\ln p(x_i)=\sum_i\ln\Big( \sum_h p(x_i,h)\Big)$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(h\)&lt;/span&gt; is the hidden variable and &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; is the observed one. Except for simple problems, having two sums turns the problem computationally infeasible, especially if the hidden variable is continuous. To deal with this issue we use the concavity property of the logarithm to approximate
&lt;/p&gt;
&lt;div class="math"&gt;$$\ln\Big( \sum_h p(x_i,h)\Big)\geq \sum_hq(h)\ln\Big(\frac{p(x_i,h)}{q(h)}\Big)$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(q(h)\)&lt;/span&gt; is an unknown distribution that we will want to fix. Further we write
&lt;/p&gt;
&lt;div class="math"&gt;$$\ln p(x_i)=\sum_hq(h)\ln\Big(\frac{p(x_i,h)}{q(h)}\Big)+R_i$$&lt;/div&gt;
&lt;p&gt;
where the remaining &lt;span class="math"&gt;\(R_i\)&lt;/span&gt; is given by
&lt;/p&gt;
&lt;div class="math"&gt;$$R_i=-\sum_h q(h)\ln\Big(\frac{p(h|x_i)}{q(h)}\Big)=KL(p(h|x_i)||q(h))$$&lt;/div&gt;
&lt;p&gt;
which is the Kullback-Leibler divergence. Since &lt;span class="math"&gt;\(R_i\geq 0\)&lt;/span&gt; by definition, we have that
&lt;/p&gt;
&lt;div class="math"&gt;$$\ln p(x_i|\theta)\geq \langle \ln p(x_i,h|\theta)\rangle_{q(h)}-\langle \ln q(h)\rangle_{q(h)}$$&lt;/div&gt;
&lt;p&gt;
where we have introduced prior parameters &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, without lack of generality. The lower bound is saturated provided we choose 
&lt;/p&gt;
&lt;div class="math"&gt;$$\text{E-step:}\quad q(h_i)=p(h_i|x_i,\theta_0)$$&lt;/div&gt;
&lt;p&gt;
This is also known as expectation E-step. Note that we have a distribution &lt;span class="math"&gt;\(q(h_i)\)&lt;/span&gt; for each sample, as it is determined by &lt;span class="math"&gt;\(x_i,\theta_0\)&lt;/span&gt;. However, this step does not solve the maximum-likelihood problem because we still have to fix the parameter &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;. What we do next is to maximize the lower bound by choosing &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; keeping &lt;span class="math"&gt;\(q(h)\)&lt;/span&gt; fixed, that is,
&lt;/p&gt;
&lt;div class="math"&gt;$$\text{M-step:}\quad \frac{\partial}{\partial \theta}\langle \ln p(x_i,h|\theta)\rangle_{q(h)}=0$$&lt;/div&gt;
&lt;p&gt;Lets take an example that can help clarify some of these ideas. Consider the model which is a mixture of two normal distributions:
&lt;/p&gt;
&lt;div class="math"&gt;$$p(x,c)=\phi(x|\mu_c,\sigma_c)\pi_c,\quad c=0,1$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\phi(x|\mu,\sigma)\)&lt;/span&gt; is a normal distribution with mean &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; and variance &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;, and &lt;span class="math"&gt;\(\pi_c=p(c)\)&lt;/span&gt; with &lt;span class="math"&gt;\(\pi_0+\pi_1=1\)&lt;/span&gt;. In this example &lt;span class="math"&gt;\(\theta\equiv \mu,\sigma\)&lt;/span&gt;, and the hidden variable is &lt;span class="math"&gt;\(h\equiv c\)&lt;/span&gt;. &lt;/p&gt;
&lt;p&gt;In the E-step we calculate:
&lt;/p&gt;
&lt;div class="math"&gt;$$\text{E-step:}\quad q(h)=p(h|x,\mu_h,\sigma_h)=\frac{\phi(x|\mu_h,\sigma_h)\pi_h}{\sum_c \phi(x|\mu_c,\sigma_c)\pi_c}$$&lt;/div&gt;
&lt;p&gt;
We write &lt;span class="math"&gt;\(q(h_i=0)=\gamma_i(x_i)\)&lt;/span&gt; and &lt;span class="math"&gt;\(q(h_i=1)=1-\gamma_i(x_i)\)&lt;/span&gt; for each sample &lt;span class="math"&gt;\(x_i\)&lt;/span&gt;, with &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt; given by the ratio above. The initial parameters &lt;span class="math"&gt;\(\mu,\sigma\)&lt;/span&gt; are arbitrary.&lt;/p&gt;
&lt;p&gt;The maximization step consists in maximizing the lower bound of the log-likelihood, hence
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}\text{M-step:}\quad &amp;amp;\gamma\ln p(x,h=0|\mu,\sigma)+(1-\gamma)\ln p(x,h=1|\mu,\sigma)\\
=&amp;amp;\gamma \ln \phi(x|\mu_0,\sigma_0)+(1-\gamma)\ln \phi(x|\mu_1,\sigma_1)-\gamma\frac{1}{2}\ln\sigma_0-(1-\gamma)\frac{1}{2}\ln\sigma_1+\ldots\\
=&amp;amp; -\gamma \frac{(x-\mu_0)^2}{2\sigma_0^2}-(1-\gamma) \frac{(x-\mu_1)^2}{2\sigma_1^2}-\gamma\frac{1}{2}\ln\sigma_0-(1-\gamma)\frac{1}{2}\ln\sigma_1+\ldots\end{aligned}$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\ldots\)&lt;/span&gt; do not depend on &lt;span class="math"&gt;\(\mu,\sigma\)&lt;/span&gt;. We need to sum over all samples, so the maximum is calculated
&lt;/p&gt;
&lt;div class="math"&gt;$$\mu_0=\frac{\sum_i x_i\gamma_i}{\sum_i \gamma_i},\;\mu_1=\frac{\sum_i x_i(1-\gamma_i)}{\sum_i (1-\gamma_i)}$$&lt;/div&gt;
&lt;p&gt;
and 
&lt;/p&gt;
&lt;div class="math"&gt;$$\sigma_0=\frac{\sum_i\gamma_i(x_i-\mu_0)^2}{\sum_i\gamma_i},\quad \sigma_1=\frac{\sum_i(1-\gamma_i)(x_i-\mu_1)^2}{\sum_i(1-\gamma_i)}$$&lt;/div&gt;
&lt;p&gt;
Maximizing relatively to the probabilities &lt;span class="math"&gt;\(\pi\)&lt;/span&gt; gives
&lt;/p&gt;
&lt;div class="math"&gt;$$\pi_0=\frac{1}{n}\sum_i\gamma_i,\;\pi_1=1-\pi_0$$&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Data Science"></category><category term="data science"></category></entry><entry><title>"Statistical Testing"</title><link href="/statistical-testing.html" rel="alternate"></link><published>2020-06-30T00:00:00+02:00</published><updated>2020-06-30T00:00:00+02:00</updated><author><name>Joao Gomes</name></author><id>tag:None,2020-06-30:/statistical-testing.html</id><summary type="html">&lt;p&gt;We explain in detail the Student's t-statistic and the &lt;span class="math"&gt;\(\chi^2\)&lt;/span&gt; statistic.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="#def1"&gt;Student's t-test&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;One-sample mean&lt;/li&gt;
&lt;li&gt;Two-sample mean &lt;/li&gt;
&lt;li&gt;Regression coefficient&lt;/li&gt;
&lt;li&gt;Correlation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="#def2"&gt;Chi square test&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pearson's Chi-square test&lt;/li&gt;
&lt;li&gt;Variance&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a name="def1"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;1. Student's t-test&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;One-sample mean&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Consider &lt;span class="math"&gt;\(n\)&lt;/span&gt; random variables distributed i.i.d., each following a normal distribution with mean &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; and variance &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;. The joint probability density function is
&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{1}{(\sqrt{2\pi}\sigma)^n} e^{-\sum_{i=1}^{n}\frac{(x_i-\mu)^2}{2\sigma^2}}\prod_{i=1}^n dx_i$$&lt;/div&gt;
&lt;p&gt;We want to write a density distribution as a function of &lt;span class="math"&gt;\(\bar{x}=\frac{\sum_i x_i}{n}\)&lt;/span&gt;, the sample mean. As such, use the equality
&lt;/p&gt;
&lt;div class="math"&gt;$$\sum_{i=1}^n(x_i-\mu)^2=\sum_{i=1}^n (x_i-\bar{x})^2+n(\bar{x}-\mu)^2$$&lt;/div&gt;
&lt;p&gt;and change variables &lt;span class="math"&gt;\((x_1,\ldots,x_n)\rightarrow (x_1,\ldots,x_{n-1},\bar{x})\)&lt;/span&gt; - the jacobian of the coordinate transformation is &lt;span class="math"&gt;\(n\)&lt;/span&gt;. The density function becomes
&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{1}{(\sqrt{2\pi}\sigma)^n} e^{-\sum_{i=1}^{n}\frac{(x_i-\bar{x})^2}{2\sigma^2}-n\frac{(\bar{x}-\mu)^2}{2\sigma^2}}d\bar{x}\prod_{i=1}^{n-1} dx_i$$&lt;/div&gt;
&lt;p&gt;Because &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; and &lt;span class="math"&gt;\(\bar{x}\)&lt;/span&gt; are independent, we can shift the variables &lt;span class="math"&gt;\(x_i\rightarrow x_i+\bar{x}\)&lt;/span&gt;, after which the term &lt;span class="math"&gt;\(\sum_{i=1}^{n}(x_i-\bar{x})^2\)&lt;/span&gt; becomes &lt;span class="math"&gt;\(\sum_{i=1}^{n-1}x_i^2+(\sum_i^{n-1}x_i)^2\)&lt;/span&gt;. Since this is quadratic in the &lt;span class="math"&gt;\(x_i\)&lt;/span&gt;, it can be safely integrated out. However, before doing that we write &lt;span class="math"&gt;\(x_i=\frac{s}{\sqrt{n-1}}u_i\)&lt;/span&gt;, with &lt;span class="math"&gt;\(\sum_{i=1}^{n-1}u_i^2+(\sum_i^{n-1}u_i)^2=1\)&lt;/span&gt;, that is, &lt;span class="math"&gt;\((s,u_i)\)&lt;/span&gt; play a similar role to spherical coordinates. The density distribution becomes
&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{1}{(\sqrt{2\pi}\sigma)^n} e^{-(n-1)\frac{s^2}{2\sigma^2}-n\frac{(\bar{x}-\mu)^2}{2\sigma^2}}s^{n-2}\,\Omega(u_i)dsd\bar{x}\prod_{i=1}^{n-1} du_i$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\Omega(u_i)\)&lt;/span&gt; is a measure for the variables &lt;span class="math"&gt;\(u_i\)&lt;/span&gt;- it gives an overall constant that we determine at the end instead.&lt;/p&gt;
&lt;p&gt;To remove dependence on the variance &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; we consider the variable &lt;span class="math"&gt;\(t=(\bar{x}-\mu)\sqrt{n}/s\)&lt;/span&gt;, which gives the Jacobian &lt;span class="math"&gt;\(s/\sqrt{n}\)&lt;/span&gt;. We scale &lt;span class="math"&gt;\(s\rightarrow \sqrt{\frac{2}{n-1}}s\sigma\)&lt;/span&gt; to obtain 
&lt;/p&gt;
&lt;div class="math"&gt;$$\propto \int_{s=0}^{\infty}e^{-s^2(1+\frac{1}{n-1}t^2)}s^{n-1}\,dsdt$$&lt;/div&gt;
&lt;p&gt;By changing &lt;span class="math"&gt;\(s\rightarrow \sqrt{s}\)&lt;/span&gt; we obtain
&lt;/p&gt;
&lt;div class="math"&gt;$$\propto\Big(1+\frac{1}{n-1}t^2\Big)^{-\frac{n}{2}}\Gamma(n/2)dt$$&lt;/div&gt;
&lt;p&gt;
and integrating over &lt;span class="math"&gt;\(t: (-\infty,\infty)\)&lt;/span&gt; we fix the overall constant
&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{\Gamma(n/2)}{\sqrt{(n-1)\pi}\Gamma(\frac{n-1}{2})}\Big (1+\frac{1}{n-1}t^2\Big)^{-\frac{n}{2}}$$&lt;/div&gt;
&lt;p&gt;This is known as the &lt;strong&gt;Student's t-distribution&lt;/strong&gt; with &lt;span class="math"&gt;\(\nu=n-1\)&lt;/span&gt; degrees of freedom.
&lt;img alt="" height="300" src="/images/Student_t.png" style="display: block; margin: 0 auto" width="300"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Two-sample mean (equal variance)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For two samples with sizes &lt;span class="math"&gt;\(n_1,n_2\)&lt;/span&gt; the idea is roughly the same. We follow similar steps as in the previous case. After some algebra, the exponential contains the terms&lt;/p&gt;
&lt;div class="math"&gt;$$-(n_1-1)\frac{s_1^2}{2\sigma^2}-(n_2-1)\frac{s_2^2}{2\sigma^2}-n_1\frac{(\bar{x}_1-\mu_1)^2}{2\sigma^2}-n_2\frac{(\bar{x}_2-\mu_2)^2}{2\sigma^2}$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(s_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(s_2\)&lt;/span&gt; are the two sample means.&lt;/p&gt;
&lt;p&gt;Now we write &lt;span class="math"&gt;\(\bar{x}_1-\mu_1=(\bar{x}_{+}+\bar{x}_{-})/2\)&lt;/span&gt; and &lt;span class="math"&gt;\(\bar{x}_2-\mu_2=(\bar{x}_{+}-\bar{x}_{-})/2\)&lt;/span&gt;, because we will want to integrate over &lt;span class="math"&gt;\(\bar{x}_{+}\)&lt;/span&gt;. We use the equality
&lt;/p&gt;
&lt;div class="math"&gt;$$-n_1(\bar{x}_1-\mu_1)^2-n_2(\bar{x}_2-\mu_2)^2=-\frac{\bar{x}_{-}^2}{1/n_1+1/n_2}-\frac{n_1+n_2}{4}\Big(\bar{x}_{+}+\frac{n_1-n_2}{n_1+n_2}\bar{x}_{-}\Big)^2$$&lt;/div&gt;
&lt;p&gt;
and integrate over &lt;span class="math"&gt;\(\bar{x}_{+}\)&lt;/span&gt;. So we are left with&lt;/p&gt;
&lt;div class="math"&gt;$$-(n_1-1)\frac{s_1^2}{2\sigma^2}-(n_2-1)\frac{s_2^2}{2\sigma^2}-\frac{\bar{x}_{-}^2}{(1/n_1+1/n_2)2\sigma^2}$$&lt;/div&gt;
&lt;p&gt;By writing 
&lt;/p&gt;
&lt;div class="math"&gt;$$s^2=\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\;t=\frac{\bar{x}_{-}}{s\sqrt{1/n_1+1/n_2}}$$&lt;/div&gt;
&lt;p&gt;we obtain again the t-distribution with &lt;span class="math"&gt;\(\nu=n_1+n_2-2\)&lt;/span&gt; degrees of freedom.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Regression coefficient&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In linear regression, we assume that the target &lt;span class="math"&gt;\(y\)&lt;/span&gt; is a linear combination of the feature &lt;span class="math"&gt;\(x\)&lt;/span&gt; up to a gaussian noise, that is,
&lt;/p&gt;
&lt;div class="math"&gt;$$y=ax+b+\epsilon$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; is the noise distributed i.i.d according to a normal distribution with mean zero. Here &lt;span class="math"&gt;\(a,b\)&lt;/span&gt; are the true parameters that we want to estimate. In linear regression we use least square error to determine the estimators
&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{a}=\frac{\sum_i(y_i-\bar{y})(x_i-\bar{x})}{\sum_i(x_i-\bar{x})^2},\;\hat{b}=\bar{y}-\hat{a}\bar{x}$$&lt;/div&gt;
&lt;p&gt;We want to calculate a probability for the difference &lt;span class="math"&gt;\(\hat{a}-a\)&lt;/span&gt;. To do this we substitute &lt;span class="math"&gt;\(y_i=ax_i+b+\epsilon_i\)&lt;/span&gt; in the estimator equation. This gives
&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{a}-a=\frac{\sum_i (\epsilon_i-\bar{\epsilon})(x_i-\bar{x})}{\sum_i(x_i-\bar{x})^2},\;\; \hat{b}-b=(a-\hat{a})\bar{x}+\bar{\epsilon}$$&lt;/div&gt;
&lt;p&gt;
Since &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; is normally distributed we want determine the probability of the quantity above. To facilitate the algebra we use vectorial notation. As such
&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{\overrightarrow{\zeta}\cdot\overrightarrow{\gamma}}{\|\overrightarrow{\gamma}\|^2}\equiv\frac{\sum_i (\epsilon_i-\bar{\epsilon})(x_i-\bar{x})}{\sum_i(x_i-\bar{x})^2},\;\;\hat{b}-b=-\frac{\overrightarrow{\zeta}\cdot\overrightarrow{\gamma}}{\|\overrightarrow{\gamma}\|^2}\bar{x}+\overrightarrow{\epsilon}\cdot \overrightarrow{1}$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\overrightarrow{\gamma}\equiv x_i-\bar{x}\)&lt;/span&gt;, &lt;span class="math"&gt;\(\zeta\equiv \epsilon_i-\bar{\epsilon}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\overrightarrow{1}=(1,1,1,\ldots,1)/n\)&lt;/span&gt;, a vector of ones divided by the number of datapoints. Note that
&lt;/p&gt;
&lt;div class="math"&gt;$$\overrightarrow{\gamma}\cdot \overrightarrow{1}=0,\;\;\overrightarrow{\zeta}\cdot \overrightarrow{1}=0$$&lt;/div&gt;
&lt;p&gt;The probability density function is proportional to the exponential of
&lt;/p&gt;
&lt;div class="math"&gt;$$-\frac{\|\overrightarrow{\epsilon}\|^2}{2\sigma^2}$$&lt;/div&gt;
&lt;p&gt;We write &lt;span class="math"&gt;\(\overrightarrow{\epsilon}=\overrightarrow{\epsilon}_{\perp}+\alpha\overrightarrow{\gamma}+\beta\overrightarrow{1}\)&lt;/span&gt; with &lt;span class="math"&gt;\(\overrightarrow{\epsilon}_{\perp}\cdot \overrightarrow{\gamma}=\overrightarrow{\epsilon}_{\perp}\cdot \overrightarrow{1}=0\)&lt;/span&gt;. We calculate
&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{\overrightarrow{\zeta}\cdot\overrightarrow{\gamma}}{\|\overrightarrow{\gamma}\|^2}=\alpha,\;\; \|\overrightarrow{\epsilon}\|^2=\|\overrightarrow{\epsilon}_{\perp}\|^2+\alpha^2\|\overrightarrow{\gamma}\|^2+\frac{\beta^2}{n}$$&lt;/div&gt;
&lt;p&gt;Integrating out &lt;span class="math"&gt;\(\beta\)&lt;/span&gt; we can build a t-test like variable with &lt;span class="math"&gt;\(n-2\)&lt;/span&gt; degrees of freedom, since &lt;span class="math"&gt;\(\overrightarrow{\epsilon}_{\perp}\)&lt;/span&gt; lives in a &lt;span class="math"&gt;\(n-2\)&lt;/span&gt; dimensional vector space. That is, 
&lt;/p&gt;
&lt;div class="math"&gt;$$t=\frac{\alpha\|\overrightarrow{\gamma}\|}{\|\overrightarrow{\epsilon}_{\perp}\|}\sqrt{n-2}$$&lt;/div&gt;
&lt;p&gt;One can show that &lt;span class="math"&gt;\(\|\overrightarrow{\epsilon}_{\perp}\|^2=\sum_i(y_i-\hat{y}_i)^2\)&lt;/span&gt;, and therefore
&lt;/p&gt;
&lt;div class="math"&gt;$$t=\frac{\hat{a}-a}{\sqrt{\frac{\sum_i(y_i-\hat{y}_i)^2}{\sum_i(x_i-\bar{x}_i)^2}}}\sqrt{n-2}$$&lt;/div&gt;
&lt;p&gt;For the intercept the logic is similar.  We have
&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{b}-b=-\frac{\overrightarrow{\zeta}\cdot\overrightarrow{\gamma}}{\|\overrightarrow{\gamma}\|^2}\bar{x}+\overrightarrow{\epsilon}\cdot \overrightarrow{1}=-\alpha\bar{x}+\frac{\beta}{n}$$&lt;/div&gt;
&lt;p&gt;
and thus
&lt;/p&gt;
&lt;div class="math"&gt;$$\|\overrightarrow{\epsilon}\|^2=\|\overrightarrow{\epsilon}_{\perp}\|^2+\alpha^2\|\overrightarrow{\gamma}\|^2+n(\hat{b}-b+\alpha\bar{x})^2$$&lt;/div&gt;
&lt;p&gt;Integrating out &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; one finds that
&lt;/p&gt;
&lt;div class="math"&gt;$$t_{\text{intercept}}=\frac{(\hat{b}-b)\|\overrightarrow{\gamma}\|\sqrt{n-2}}{\|\overrightarrow{\epsilon}_{\perp}\|\sqrt{\|\overrightarrow{\gamma}\|^2/n+\bar{x}^2}}$$&lt;/div&gt;
&lt;p&gt;follows the Student's t-distribution with &lt;span class="math"&gt;\(n-2\)&lt;/span&gt; degrees of freedom.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Correlation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We want to test whether two variables  &lt;span class="math"&gt;\(y\)&lt;/span&gt; and &lt;span class="math"&gt;\(x\)&lt;/span&gt; have zero correlation, statistically speaking. Essentialy this accounts to fit &lt;span class="math"&gt;\(y\sim ax+b\)&lt;/span&gt;. We have seen that the regression coefficient &lt;span class="math"&gt;\(a\)&lt;/span&gt; is proportional to the sample correlation coefficient, that is,&lt;/p&gt;
&lt;div class="math"&gt;$$a=\frac{\langle yx\rangle -\langle y\rangle \langle x\rangle}{\langle x^2\rangle -\langle x\rangle^2 }=r\frac{\sigma(y)}{\sigma(x)}$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\sigma(y)^2=\sum_{i}(y_i-\bar{y})^2/n\)&lt;/span&gt; and &lt;span class="math"&gt;\(\sigma(x)^2=\sum_{i}(x_i-\bar{x})^2/n\)&lt;/span&gt;, and &lt;span class="math"&gt;\(r\)&lt;/span&gt; is the Pearson's correlation coefficient. Then we use the equality
&lt;/p&gt;
&lt;div class="math"&gt;$$\sum_{i}(y_i-\hat{y}_i)^2/n=\sigma(y)^2(1-r^2)$$&lt;/div&gt;
&lt;p&gt;
to find that the t-statistic for the regression coefficient &lt;span class="math"&gt;\(a\)&lt;/span&gt; can be written as
&lt;/p&gt;
&lt;div class="math"&gt;$$t=\frac{r\sqrt{n-2}}{\sqrt{1-r^2}}$$&lt;/div&gt;
&lt;p&gt;
assuming that true coefficient is zero, that is, &lt;span class="math"&gt;\(a=0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a name="def2"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;2. Chi square test&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Let each &lt;span class="math"&gt;\(X_i,\,i=1\ldots n\)&lt;/span&gt; be a random variable following a standard normal distribution. Then the sum of squares
&lt;/p&gt;
&lt;div class="math"&gt;$$\chi^2=\sum_{i=1}^nX^2_i$$&lt;/div&gt;
&lt;p&gt;
follows a chi-distribution with &lt;span class="math"&gt;\(k\)&lt;/span&gt; degrees of freedom. To understand this, consider the joint probability density function of &lt;span class="math"&gt;\(n\)&lt;/span&gt; standard normal random variables
&lt;/p&gt;
&lt;div class="math"&gt;$$e^{-\frac{1}{2}\sum_{i=1}^n X_i^2}\prod_{i=1}^n dX_i$$&lt;/div&gt;
&lt;p&gt;
If we use spherical coordinates with
&lt;/p&gt;
&lt;div class="math"&gt;$$X_i=ru_i,\;\sum_{i=1}^n u_i^2=1$$&lt;/div&gt;
&lt;p&gt;
the probability density becomes
&lt;/p&gt;
&lt;div class="math"&gt;$$e^{-\frac{r^2}{2}}drr^{n-1}\Omega$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\Omega\)&lt;/span&gt; comes from integrating out &lt;span class="math"&gt;\(u_i\)&lt;/span&gt;. Since &lt;span class="math"&gt;\(r\)&lt;/span&gt; is never negative we further use &lt;span class="math"&gt;\(s=r^{2}\)&lt;/span&gt; and  obtain 
&lt;/p&gt;
&lt;div class="math"&gt;$$\propto e^{-\frac{s}{2}}s^{\frac{n}{2}-1}ds$$&lt;/div&gt;
&lt;p&gt;
Therefore the chi-square variable &lt;span class="math"&gt;\(\chi^2\equiv s\)&lt;/span&gt; with &lt;span class="math"&gt;\(k\)&lt;/span&gt; degrees of freedom follows the distribution
&lt;/p&gt;
&lt;div class="math"&gt;$$\chi^2\sim \frac{s^{\frac{n}{2}-1}}{2^{n/2}\Gamma(n/2)}e^{-\frac{s}{2}}$$&lt;/div&gt;
&lt;p&gt;
This distribution has the following shape (from Wikipedia):
&lt;img alt="" height="400" src="/images/Chi-square_pdf.svg" style="display: block; margin: 0 auto" width="400"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pearson's Chi-square test&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This test gives a measure of goodness of fit for a categorical variable with &lt;span class="math"&gt;\(k\)&lt;/span&gt; classes. Suppose we have &lt;span class="math"&gt;\(n\)&lt;/span&gt; observations with &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; (&lt;span class="math"&gt;\(i=1\ldots k\)&lt;/span&gt;) observed numbers, that is, &lt;span class="math"&gt;\(\sum_{i=1}^k x_i=n\)&lt;/span&gt;. We want to test the hypotheses that each category is drawn with probability &lt;span class="math"&gt;\(p_i\)&lt;/span&gt;. Under this assumption, the joint probability of observing &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; numbers follows a multinomial distribution
&lt;/p&gt;
&lt;div class="math"&gt;$$P(x_1,x_2,\ldots,x_n)=\frac{n!}{x_1!x_2!\ldots x_k!}p_1^{x_1}p_2^{x_2}\ldots p_k^{x_k}$$&lt;/div&gt;
&lt;p&gt; 
We want to understand the behaviour of this probability when &lt;span class="math"&gt;\(n\)&lt;/span&gt; is very large. Assume that &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; is also sufficiently large, which is ok to do for typical observations. In this case use stirling's approximation of the factorial, that is,
&lt;/p&gt;
&lt;div class="math"&gt;$$n!\simeq \sqrt{2\pi n}\Big(\frac{n}{e}\Big)^n$$&lt;/div&gt;
&lt;p&gt;
to write
&lt;/p&gt;
&lt;div class="math"&gt;$$P(x_1,x_2,\ldots,x_n)\propto \Big(\frac{n}{e}\Big)^n \prod_{i=1}^k \Big(\frac{x_i}{e}\Big)^{-x_i}p_i^{x_i}$$&lt;/div&gt;
&lt;p&gt;
In taking &lt;span class="math"&gt;\(n\)&lt;/span&gt; very large, we want to keep the frequency &lt;span class="math"&gt;\(\lambda_i=x_i/ n\)&lt;/span&gt; fixed. Then the logarithm of the above expression becomes
&lt;/p&gt;
&lt;div class="math"&gt;$$\ln P(x_1,x_2,\ldots,x_n)=-\sum_{i=1}^k\lambda_in\ln(\lambda_i)+\sum_{i=1}^k\lambda_i n\ln(p_i)$$&lt;/div&gt;
&lt;p&gt;
Since this is proportional to &lt;span class="math"&gt;\(n\)&lt;/span&gt; we can perform an asymptotic expansion as &lt;span class="math"&gt;\(n\gg 1\)&lt;/span&gt;. We perform the expansion around the maximum of &lt;span class="math"&gt;\(\ln P\)&lt;/span&gt; (note that &lt;span class="math"&gt;\(\ln P\)&lt;/span&gt; is a concave function of &lt;span class="math"&gt;\(\lambda_i\)&lt;/span&gt; ), that is,
&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{\partial P}{\partial \lambda_i}=0,\;i=1\ldots n-1$$&lt;/div&gt;
&lt;p&gt;
Using the fact that we have &lt;span class="math"&gt;\(n-1\)&lt;/span&gt; independent variables since &lt;span class="math"&gt;\(\sum_i \lambda_i=1\)&lt;/span&gt;, the solution is &lt;span class="math"&gt;\(\lambda_i^*=p_i\)&lt;/span&gt;. Expanding around this solution we find
&lt;/p&gt;
&lt;div class="math"&gt;$$\ln P(\lambda_1,\lambda_2,\ldots,\lambda_n)=-n\sum_{i=1}^k\frac{(\lambda_i-p_i)^2}{2p_i}$$&lt;/div&gt;
&lt;p&gt;
In terms of &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; this gives
&lt;/p&gt;
&lt;div class="math"&gt;$$\ln P(x_1,x_2,\ldots,x_n)=-\sum_{i=1}^k\frac{(x_i-m_i)^2}{2m_i}$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(m_i=np_i\)&lt;/span&gt; is the expected observed number. Therefore the quantity
&lt;/p&gt;
&lt;div class="math"&gt;$$\sum_{i=1}^k\frac{(x_i-m_i)^2}{m_i}$$&lt;/div&gt;
&lt;p&gt;
follows a &lt;span class="math"&gt;\(\chi^2\)&lt;/span&gt; distribution with &lt;span class="math"&gt;\(k-1\)&lt;/span&gt; degrees of fredom, since only &lt;span class="math"&gt;\(k-1\)&lt;/span&gt; of the &lt;span class="math"&gt;\(x\)&lt;/span&gt;'s are independent.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Variance&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In order to investigate the difference between the sample variance &lt;span class="math"&gt;\(s^2=\sum_i(x_i-\bar{x})^2/n-1\)&lt;/span&gt; and the assumed variance &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt; of the distribution. We calculate
&lt;/p&gt;
&lt;div class="math"&gt;$$(n-1)\frac{s^2}{\sigma^2}$$&lt;/div&gt;
&lt;p&gt;
Remember that for a normally distributed random variable &lt;span class="math"&gt;\(x_i\)&lt;/span&gt;, the sum &lt;span class="math"&gt;\(\sum_i(x_i-\bar{x})^2\)&lt;/span&gt; also follows a normal distribution. In particular, the combination &lt;span class="math"&gt;\(\sum_i(x_i-\bar{x})^2/\sigma^2\)&lt;/span&gt; follows a &lt;span class="math"&gt;\(\chi^2\)&lt;/span&gt; distribution with &lt;span class="math"&gt;\(n-1\)&lt;/span&gt; degrees of freedom, because we have integrated out &lt;span class="math"&gt;\(\bar{x}\)&lt;/span&gt; as explained in the beginning of the post.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Statistics"></category><category term="data science"></category></entry><entry><title>"Linear regression classifier"</title><link href="/linear-regression-classifier.html" rel="alternate"></link><published>2020-06-20T00:00:00+02:00</published><updated>2020-06-20T00:00:00+02:00</updated><author><name>Joao Gomes</name></author><id>tag:None,2020-06-20:/linear-regression-classifier.html</id><summary type="html">&lt;p&gt;Linear regression/classifier basics&lt;/p&gt;</summary><content type="html">&lt;ol&gt;
&lt;li&gt;&lt;a href="#def1"&gt;Linear regression and classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#python"&gt;Python implementation&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a name="def1"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;1. Linear regression and classification&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Suppose we have a dataset with n features and k classes. We want to fit an hyperplane. For that purpose we write the target variable &lt;span class="math"&gt;\(y\)&lt;/span&gt; in a one-hot-encoded way, that is, as a vector &lt;span class="math"&gt;\(y_k\)&lt;/span&gt; with only one entry equal to one and &lt;span class="math"&gt;\(k-1\)&lt;/span&gt; others equal zero, and fit:
&lt;/p&gt;
&lt;div class="math"&gt;$$y^k\sim w^k_{\mu}x^{\mu}+w^k_0$$&lt;/div&gt;
&lt;p&gt; 
where &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; is the feature dimension and &lt;span class="math"&gt;\(w^k_0\)&lt;/span&gt; is the bias. Next we consider the mean square loss:
&lt;/p&gt;
&lt;div class="math"&gt;$$L=\frac{1}{m}\sum_{i=1}^{m}||(y^k_i-w^k_{\mu}x^{\mu}_i-w^k_0)||^2$$&lt;/div&gt;
&lt;p&gt;
and find its minima, that is,
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}&amp;amp;\frac{\partial L}{\partial w^k_{\mu}}=-\frac{2}{m}\sum_{i=1}^{m}(y^k_i-w^k_{\nu}x^{\nu}_i-w^k_0)x^{\mu}_i=0\\
&amp;amp;\frac{\partial L}{\partial w^k_{0}}=-\frac{2}{m}\sum_{i=1}^{m}(y^k_i-w^k_{\mu}x^{\mu}_i-w^k_0)=0
\end{aligned}$$&lt;/div&gt;
&lt;p&gt;Alternatively
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}&amp;amp; \langle y^kx^{\mu}\rangle-w^k_{\nu}\langle x^{\nu}x^{\mu}\rangle -w^k_0\langle x^{\mu}\rangle=0\\
&amp;amp;\langle y^k\rangle-w^k_{\mu}\langle x^u\rangle-w^k_0=0
\end{aligned}$$&lt;/div&gt;
&lt;p&gt;It is best to write &lt;span class="math"&gt;\(w^k_a=(w^k_{\mu},w^k_0)\)&lt;/span&gt; and &lt;span class="math"&gt;\(x^{a}=(x^{\mu},1)\)&lt;/span&gt;, so that the equations for &lt;span class="math"&gt;\(w^k_{\mu}\)&lt;/span&gt; and the bias merge into a single equation:
&lt;/p&gt;
&lt;div class="math"&gt;$$\langle y^kx^{a}\rangle-w^k_{b}\langle x^{b}x^{a}\rangle=0$$&lt;/div&gt;
&lt;p&gt;The solution is
&lt;/p&gt;
&lt;div class="math"&gt;$$w=Y^{T}X(X^{T}X)^{-1}$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(Y=y_{ik}\)&lt;/span&gt; and &lt;span class="math"&gt;\(X=x_{ia}\)&lt;/span&gt;. The predictor becomes:
&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{Y}\equiv Xw^T=X(X^TX)^{-1}X^TY$$&lt;/div&gt;
&lt;p&gt;When is it guaranteed that there exists a solution? Or in other words, when is &lt;span class="math"&gt;\(X^TX\)&lt;/span&gt; invertible? We need to look at the vector space spanned by the columns of &lt;span class="math"&gt;\(X\)&lt;/span&gt;, that is, &lt;span class="math"&gt;\(\text{Span}=\{v_a\equiv X_{ia}\}\)&lt;/span&gt;. If the dimension of this vector space is less than the number of features then some of the vectors &lt;span class="math"&gt;\(v_a\)&lt;/span&gt; are not linearly independent and thus the matrix &lt;span class="math"&gt;\(X^TX\)&lt;/span&gt; will have determinant zero. Or in other words, there are coefficients &lt;span class="math"&gt;\(c_a\)&lt;/span&gt; such that &lt;span class="math"&gt;\(\sum_ac_av_a=0\)&lt;/span&gt;, which means that &lt;span class="math"&gt;\(Xc=0\)&lt;/span&gt; and thus &lt;span class="math"&gt;\(X^TXc=0\)&lt;/span&gt;. If there is a large number of datapoints as compared to the number of features then it becomes harder to find linearly dependent vectors &lt;span class="math"&gt;\(v_a\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Note that 
&lt;/p&gt;
&lt;div class="math"&gt;$$X^TX \Big[\begin{array}{c}
   0_{\mu}  \\
   1  \\
  \end{array} \Big]_{a\times 1}=N\Big[\begin{array}{c}
   \langle x^{\mu}\rangle  \\
   1  \\
  \end{array} \Big]_{a\times 1}$$&lt;/div&gt;
&lt;p&gt; 
  and therefore
  &lt;/p&gt;
&lt;div class="math"&gt;$$X(X^TX)^{-1}X^TY \Big[\begin{array}{c}
   1_{k} 
  \end{array}\Big]_{k\times 1}=\Big[\begin{array}{c}
   1_{i} 
  \end{array}\Big]_{i\times 1}$$&lt;/div&gt;
&lt;p&gt;that is, the predictions &lt;span class="math"&gt;\(\hat{Y}_i\)&lt;/span&gt; sum up to one just like a probability. However, it is not guaranteed that &lt;span class="math"&gt;\(\hat{Y}\)&lt;/span&gt; is always positive. To predict the class of a datapoint we use the rule:
  &lt;/p&gt;
&lt;div class="math"&gt;$$k=\text{argmax}_{k'}\hat{Y}(x)$$&lt;/div&gt;
&lt;p&gt;We can work out in more detail the inverse matrix &lt;span class="math"&gt;\((X^TX)^{-1}\)&lt;/span&gt;.
  &lt;/p&gt;
&lt;div class="math"&gt;$$X^TX=N\Big[\begin{array}{cc}
   \langle x^{\mu}x^{\nu}\rangle &amp;amp; \langle x^{\mu}\rangle\\
   \langle x^{\nu}\rangle &amp;amp; 1
  \end{array}\Big]$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(N\)&lt;/span&gt; is the number of datapoints. Now we use the result 
&lt;/p&gt;
&lt;div class="math"&gt;$$\Big[\begin{array}{cc}
   A_{ij} &amp;amp; v_i\\
   v_j &amp;amp; 1
  \end{array}\Big]^{-1}=\Big[\begin{array}{cc}
   A^{-1}+\frac{A^{-1}vv^TA^{-1}}{(1-v^TA^{-1}v)} &amp;amp; -\frac{A^{-1}v}{(1-v^TA^{-1}v)}\\
   -\frac{v^TA^{-1}}{(1-v^TA^{-1}v)} &amp;amp; \frac{1}{(1-v^TA^{-1}v)}
  \end{array}\Big]$$&lt;/div&gt;
&lt;p&gt;
to find that
&lt;/p&gt;
&lt;div class="math"&gt;$$(X^TX)^{-1}=N^{-1}\Big[\begin{array}{cc}
   \text{Var}^{-1}_{\mu\nu} &amp;amp; -\sum_{\nu}\text{Var}^{-1}_{\mu\nu}\langle x^{\nu}\rangle\\
    -\sum_{\mu}\langle x^{\mu}\rangle\text{Var}^{-1}_{\mu\nu}&amp;amp; 1+\sum_{\mu\nu}\langle x^{\mu}\rangle\text{Var}^{-1}_{\mu\nu}\langle x^{\nu}\rangle
  \end{array}\Big]$$&lt;/div&gt;
&lt;p&gt;
where 
&lt;/p&gt;
&lt;div class="math"&gt;$$\text{Var}_{\mu\nu}=\langle x^{\mu}x^{\nu}\rangle-\langle x^{\mu}\rangle \langle x^{\nu}\rangle$$&lt;/div&gt;
&lt;p&gt;
is the variance matrix. On the other hand, the weight matrix &lt;span class="math"&gt;\(w^T\)&lt;/span&gt; becomes
&lt;/p&gt;
&lt;div class="math"&gt;$$\Big[\begin{array}{cc}
   \text{Var}^{-1}_{\mu\nu} &amp;amp; -\sum_{\nu}\text{Var}^{-1}_{\mu\nu}\langle x^{\nu}\rangle\\
    -\sum_{\mu}\langle x^{\mu}\rangle\text{Var}^{-1}_{\mu\nu}&amp;amp; 1+\sum_{\mu\nu}\langle x^{\mu}\rangle\text{Var}^{-1}_{\mu\nu}\langle x^{\nu}\rangle
  \end{array}\Big] \Big[\begin{array}{c}
   \langle x^{\nu}y^k\rangle\\
   \langle y^k \rangle
  \end{array}\Big]$$&lt;/div&gt;
&lt;p&gt;Lets see how this works in practice. We build artificial data using the normal distribution in two dimensions. We consider first the case with two classes and later the multi-class case.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" height="300" src="/images/lr_2classes.png" style="display: block; margin: 0 auto" width="300"&gt;&lt;/p&gt;
&lt;p&gt;One can see that despite a very simple model the linear classifier can separate very clearly all the points. The trouble happens with more classes. Consider now the case with three classes.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" height="400" src="/images/lr_3classes.png" style="display: block; margin: 0 auto" width="400"&gt;
  We see that the linear model cannot differentiate between classes &lt;span class="math"&gt;\(0/1\)&lt;/span&gt; and &lt;span class="math"&gt;\(1/2\)&lt;/span&gt;, as the decision boundaries almost overlap.&lt;/p&gt;
&lt;p&gt;&lt;a name="python"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;2. Python implementation&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Create data (three classes)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;
&lt;span class="n"&gt;n1&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multivariate_normal&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],[[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;n2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multivariate_normal&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;],[[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;n3&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multivariate_normal&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;],[[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;n1&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concatenate&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;n1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)],&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;n2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concatenate&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;n2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)],&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;n3&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concatenate&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;n3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)],&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concatenate&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;n1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;n2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;n3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;target&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Regression:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.linear_model&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LinearRegression&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;OneHotEncoder&lt;/span&gt;

&lt;span class="c1"&gt;#One-hot-encoding&lt;/span&gt;
&lt;span class="n"&gt;enc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;OneHotEncoder&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;enc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;target&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;LinearRegression&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;toarray&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Decision boundary:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;decision&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__call__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;cl1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;cl2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;coef_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;cl1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;coef_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;cl2&lt;/span&gt;&lt;span class="p"&gt;])[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;coef_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;cl1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;coef_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;cl2&lt;/span&gt;&lt;span class="p"&gt;])[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;intercept_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;cl1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;intercept_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;cl2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;coef_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;cl1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;coef_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;cl2&lt;/span&gt;&lt;span class="p"&gt;])[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;

&lt;span class="n"&gt;lr_bnd&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;decision&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;#draw line from (p1[0],p2[0]) to (p1[1],p2[1]), and so on&lt;/span&gt;
&lt;span class="n"&gt;p1&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;p2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;lr_bnd&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p1&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;lr_bnd&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p1&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

&lt;span class="n"&gt;p3&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;p4&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;lr_bnd&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p3&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;lr_bnd&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p3&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

&lt;span class="n"&gt;p5&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;p6&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;lr_bnd&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p5&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;lr_bnd&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p5&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Plot:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;sns&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatterplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;hue&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;target&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;p2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;linewidth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;decision bnd 0/1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;p4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;linewidth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;decision bnd 1/2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;p6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;linewidth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;decision bnd 0/2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;best&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Linear Regression 3 classes Decision Boundary&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Machine Learning"></category><category term="data science"></category></entry><entry><title>"Bias-Variance/Complexity trade-off"</title><link href="/bias-variancecomplexity-trade-off.html" rel="alternate"></link><published>2020-06-05T00:00:00+02:00</published><updated>2020-06-05T00:00:00+02:00</updated><author><name>Joao Gomes</name></author><id>tag:None,2020-06-05:/bias-variancecomplexity-trade-off.html</id><summary type="html">&lt;p&gt;We explain the trade-off between bias and variance or complexity.&lt;/p&gt;</summary><content type="html">&lt;ol&gt;
&lt;li&gt;&lt;a href="#def1"&gt;Basic concept&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#python"&gt;Python implementation: Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#python2"&gt;Python implementation: Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a name="def1"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;1. Basic concept&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Let &lt;span class="math"&gt;\(h_S\)&lt;/span&gt; be the solution of an ERM algorithm. We decompose the generalization error as 
&lt;/p&gt;
&lt;div class="math"&gt;$$L_D(h_S)=\epsilon_{app}+\epsilon_{est}$$&lt;/div&gt;
&lt;p&gt;
with 
&lt;/p&gt;
&lt;div class="math"&gt;$$\epsilon_{app}=\text{min}_{h\in \mathcal{H}}L_D(h),\;\;\epsilon_{est}=L_D(h_S)-\text{min}_{h\in \mathcal{H}}L_D(h)$$&lt;/div&gt;
&lt;p&gt;Here &lt;span class="math"&gt;\(\epsilon_{app}\)&lt;/span&gt; is the &lt;strong&gt;approximation error&lt;/strong&gt; which is the smallest error one can achieve using the hypothesis class &lt;span class="math"&gt;\(\mathcal{H}\)&lt;/span&gt;. This error is independent of the data and depends only on the choice of the hypothesis class. On the other hand, &lt;span class="math"&gt;\(\epsilon_{est}\)&lt;/span&gt; is the &lt;strong&gt;estimation error&lt;/strong&gt;, that is, it measuers how far the generalization error is from the approximation error. Since &lt;span class="math"&gt;\(h_S\)&lt;/span&gt; depends on the training set, the estimation error depends strongly on the training data. &lt;/p&gt;
&lt;p&gt;In order to reduce the approximation error we need a more complex hypothesis class but this might make the estimation error worse, since a more complex hypothesis may lead to overfitting. On the other hand, a smaller hypothesis class, that is, less complex, reduces the estimation error, because &lt;span class="math"&gt;\(h_S\)&lt;/span&gt; and &lt;span class="math"&gt;\(\text{argmin}_hL_D(h)\)&lt;/span&gt; are now closer, but it will increase the approximation error because of underfitting. This tradeoff is known as &lt;strong&gt;bias-complexity&lt;/strong&gt; tradeoff.&lt;/p&gt;
&lt;p&gt;Lets see how this works in practice. We create artificial data of around 1million samples in a 10 dimensional feature space, according to the classification rule:&lt;/p&gt;
&lt;div class="math"&gt;$$y(x)=\text{sign}(w^0_1\tanh(w^1_ix^i)+w^0_2\tanh(w^2_ix^i))$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(w^1,w^2\)&lt;/span&gt; are 10 dimensional parameters and &lt;span class="math"&gt;\((w^0_1,w^0_2)\)&lt;/span&gt; is a two parameter. For the classification we use a decision tree and adjust its max depth and number of features used in order to obtain different levels of complexity. Below we show the behaviour of the estimation (est_error), approximation (app_error) and generalization errors (gen_error):&lt;/p&gt;
&lt;p&gt;&lt;img alt="" height="400" src="/images/bias_vs_complexity.png" style="display: block; margin: 0 auto" width="400"&gt;&lt;/p&gt;
&lt;p&gt;To determine the approximation error we train the decision tree on the full data keeping fixed the number of features used and adjusting the depth of the tree (max_depth in the picture above). On the other hand, to determine the estimation error we train the decision tree on 10% of the data (around 100k samples) for a variety of depths and number of features. The generalization error is calculated on the remaining 90% of the data.&lt;/p&gt;
&lt;p&gt;The generalization error curves shows a tradeoff between bias and complexity. When the depth is smaller, so bias is larger, the approximation error grows but the estimation error is smaller. In contrast, if we increase the depth, the approximation error becomes smaller but the estimation error grows due to overfitting. The "sweet spot" occurs for an intermediate value of the depth, where the generalization error is a minimum.  &lt;/p&gt;
&lt;p&gt;Similar behaviour is obtained for different number of features (max_features):
&lt;img alt="" height="600" src="/images/bias_vs_complex_multiple.png" style="display: block; margin: 0 auto" width="600"&gt;&lt;/p&gt;
&lt;p&gt;In case of regression, a similar trade-off is observed. Nevertheless the analysis is slightly different. In general we want to model &lt;span class="math"&gt;\(y=f(x)+\epsilon\)&lt;/span&gt; where &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; is noise with mean zero and standard deviation &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;. So we use an algorithm to approximate &lt;span class="math"&gt;\(f(x)\simeq \hat{f}(x)\)&lt;/span&gt;. Here &lt;span class="math"&gt;\(\hat{f}\)&lt;/span&gt; is the output of our algorithm.&lt;/p&gt;
&lt;p&gt;The mean square error of a predictor (regression problem) can be decomposed as follows:
&lt;/p&gt;
&lt;div class="math"&gt;$$E_D[(y-\hat{f}(x_0;D))^2]=\text{Bias}^2+\text{Var}^2+\sigma^2$$&lt;/div&gt;
&lt;p&gt;
where
&lt;/p&gt;
&lt;div class="math"&gt;$$\text{Bias}=E_D[\hat{f}(x_0;D)]-f(x_0)$$&lt;/div&gt;
&lt;p&gt;
and
&lt;/p&gt;
&lt;div class="math"&gt;$$\text{Var}^2=E_D[E_D[\hat{f}(x_0;D)]-\hat{f}(x_0;D)]^2$$&lt;/div&gt;
&lt;p&gt;
Note that the expectation &lt;span class="math"&gt;\(E_D\)&lt;/span&gt; is calculated by using different training datasets and &lt;span class="math"&gt;\(x_0\)&lt;/span&gt; is a reference point- the error will depend on this point which is kept fixed while averaging over different training sets.&lt;/p&gt;
&lt;p&gt;We use again fake data (1million samples) in a 10 dimensional feature space and target function
&lt;/p&gt;
&lt;div class="math"&gt;$$f(x)=(x.w)^4+(x.w)^2+x.h$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(w,h\)&lt;/span&gt; are 10 dimensional parameter arrays. We use a decision tree regressor and adjust number of features used and max depth. &lt;/p&gt;
&lt;p&gt;&lt;img alt="" height="400" src="/images/bias_variance_maxfeatures.png" style="display: block; margin: 0 auto" width="400"&gt;&lt;/p&gt;
&lt;p&gt;In each iteration, we sample about 20k datapoints and fit the decision tree,  calculate &lt;span class="math"&gt;\(\hat{f}(x_0)\)&lt;/span&gt; for a determined reference point and store this value. The bias is then calculated from the mean of the difference &lt;span class="math"&gt;\(\hat{f}(x_0)-f(x_0)\)&lt;/span&gt; and for the variance we calculate &lt;span class="math"&gt;\(\hat{f}(x_0)\)&lt;/span&gt; after each training sample and then calculate the variance of that array.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" height="600" src="/images/bias_variance_multiple.png" style="display: block; margin: 0 auto" width="600"&gt;&lt;/p&gt;
&lt;p&gt;We can see that while variance increases with increasing depth, bias decreases. This behaviour translates into a trade-off between bias and variance which explains why the mean square error (mse) reachs its minimum at an intermediate depth. &lt;/p&gt;
&lt;p&gt;&lt;a name="python"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;2. Python implementation: Classification&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Classification with Decision Tree:&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;load_wine&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.model_selection&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.tree&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DecisionTreeClassifier&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;accuracy_score&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;progressbar&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;multiprocessing&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;mp&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Create artificial data&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,(&lt;/span&gt;&lt;span class="mi"&gt;1000000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;w1&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;w2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;w0&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;w0&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tanh&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;w0&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tanh&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Train and test data:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;indices&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;indices&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;indices&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;#keep only 10% of the data&lt;/span&gt;
&lt;span class="n"&gt;train_x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;indices&lt;/span&gt;&lt;span class="p"&gt;][:&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;test_x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;indices&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
&lt;span class="n"&gt;train_y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;indices&lt;/span&gt;&lt;span class="p"&gt;][:&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;test_y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;indices&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;

&lt;span class="c1"&gt;#add some noise to the training data&lt;/span&gt;
&lt;span class="n"&gt;noise&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,(&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;train_x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_x&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;noise&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Train the algorithm on the full dataset (1million). We can then determine the approximation error:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;#Decision Tree parameters&lt;/span&gt;
&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;ccp_alpha&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="s1"&gt;&amp;#39;class_weight&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="s1"&gt;&amp;#39;criterion&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;gini&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="s1"&gt;&amp;#39;max_depth&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="s1"&gt;&amp;#39;max_features&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="s1"&gt;&amp;#39;max_leaf_nodes&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="s1"&gt;&amp;#39;min_impurity_decrease&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="s1"&gt;&amp;#39;min_impurity_split&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="s1"&gt;&amp;#39;min_samples_leaf&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="s1"&gt;&amp;#39;min_samples_split&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="s1"&gt;&amp;#39;min_weight_fraction_leaf&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="s1"&gt;&amp;#39;presort&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;deprecated&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="s1"&gt;&amp;#39;random_state&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="s1"&gt;&amp;#39;splitter&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;best&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;complexity&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{}&lt;/span&gt;
&lt;span class="n"&gt;max_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;max_f&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;progressbar&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;progressbar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_features&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;max_features&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;max_f&lt;/span&gt;
    &lt;span class="n"&gt;complexity&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;max_f&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt;

    &lt;span class="n"&gt;DT&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;DecisionTreeClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;pred&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;DT&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_depth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;
        &lt;span class="n"&gt;DT&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;
        &lt;span class="n"&gt;DT&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;acc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;DT&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;acc&lt;/span&gt;

    &lt;span class="c1"&gt;#parallelize the calculation&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;mp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Pool&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cpu_count&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;pool&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; 
        &lt;span class="n"&gt;complexity&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;max_f&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pool&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;31&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now train on the train set:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;max_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;complexity&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;learning&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{}&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;max_features&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;max_features&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;
    &lt;span class="n"&gt;learning&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;depth&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;progressbar&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;progressbar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;31&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;max_depth&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;depth&lt;/span&gt;
        &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;random_state&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;depth&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;
        &lt;span class="n"&gt;DT&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;DecisionTreeClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;DT&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;train_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;acc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;DT&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;test_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;#calculates the generalization error&lt;/span&gt;
        &lt;span class="n"&gt;learning&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;acc&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a name="python2"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;3. Python implementation: Regression&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Regression with Decision Tree:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;progressbar&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;seaborn&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sns&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.tree&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DecisionTreeRegressor&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;mean_squared_error&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;multiprocessing&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;mp&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Data preparation&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,(&lt;/span&gt;&lt;span class="mi"&gt;1000000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;#target function&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fnt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;

&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;fnt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Decision Tree regressor:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;criterion&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;mse&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="s1"&gt;&amp;#39;max_depth&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="s1"&gt;&amp;#39;max_features&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="s1"&gt;&amp;#39;max_leaf_nodes&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="s1"&gt;&amp;#39;min_impurity_decrease&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="s1"&gt;&amp;#39;min_impurity_split&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="s1"&gt;&amp;#39;min_samples_leaf&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="s1"&gt;&amp;#39;min_samples_split&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="s1"&gt;&amp;#39;min_weight_fraction_leaf&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="s1"&gt;&amp;#39;random_state&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="s1"&gt;&amp;#39;splitter&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;best&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;indices&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;indices&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;choice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;indices&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;x0&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="c1"&gt;#reference point&lt;/span&gt;

&lt;span class="n"&gt;max_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;models&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{}&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sampling&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="c1"&gt;#takes in a model and fits on a sample&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;
    &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;
    &lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;choice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;indices&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;:],&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;train_x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;train_y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;train_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;progressbar&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;progressbar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_features&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;max_features&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;
    &lt;span class="n"&gt;models&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{}&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;31&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;max_depth&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;

        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;mp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Pool&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cpu_count&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;pool&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;models&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pool&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sampling&lt;/span&gt;&lt;span class="p"&gt;,[(&lt;/span&gt;&lt;span class="n"&gt;DecisionTreeRegressor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Calculate Bias and Variance:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{}&lt;/span&gt;
&lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{}&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;progressbar&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;progressbar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_features&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{}&lt;/span&gt;
    &lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{}&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;31&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;models&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="c1"&gt;#predictions&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;models&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]:&lt;/span&gt;
            &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concatenate&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;fnt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Machine Learning"></category><category term="data science"></category></entry><entry><title>"Curse of dimensionality"</title><link href="/curse-of-dimensionality.html" rel="alternate"></link><published>2020-05-26T00:00:00+02:00</published><updated>2020-05-26T00:00:00+02:00</updated><author><name>Joao Gomes</name></author><id>tag:None,2020-05-26:/curse-of-dimensionality.html</id><summary type="html">&lt;p&gt;We address the importance of dimensionality in machine learning.&lt;/p&gt;</summary><content type="html">&lt;h3&gt;&lt;strong&gt;Table of contents&lt;/strong&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#def1"&gt;Basic concept&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#def"&gt;Hughes phenomenon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#python"&gt;Python implementation&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a name="def1"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;1. Basic concept&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;All the machine learning models suffer from the same basic problem. If we have too many features as compared to the amount of data, then it is easier to overfit and the model will generalize poorly- this means that the model can more easily memorize the data because there are more features that can be used to differentiate the datapoints. Instead if we have too few features for the same amount of data, then it is  harder for the model to capture the relevant features and it will mostly certainly underfit. &lt;/p&gt;
&lt;p&gt;So what is the right amount of data versus number of features? A simple criterion is the following. Suppose we have a feature &lt;span class="math"&gt;\(x\)&lt;/span&gt; that can take &lt;span class="math"&gt;\(N\)&lt;/span&gt; distinct values. In a binary classification problem for example, if &lt;span class="math"&gt;\(m\)&lt;/span&gt;, the number of data-points, is very large, then we have enough points to calculate the empirical probabilities &lt;span class="math"&gt;\(P(c|x)\)&lt;/span&gt; with relative confidence, where &lt;span class="math"&gt;\(c=0,1\)&lt;/span&gt; is the class. As a classifier model we can use the set of empirical probabilites- the predictor is the class which gives higher probability. On the other hand, if &lt;span class="math"&gt;\(m\)&lt;/span&gt; is smaller than &lt;span class="math"&gt;\(N\)&lt;/span&gt; then the data is too sparse and we cannot rely on the empirical probabilities. Similarly, if we have an additional feature which can also take &lt;span class="math"&gt;\(N\)&lt;/span&gt; values, then we need &lt;span class="math"&gt;\(m\)&lt;/span&gt; to be larger than &lt;span class="math"&gt;\(N^2\)&lt;/span&gt;. In general, for &lt;span class="math"&gt;\(d\)&lt;/span&gt; dimensions we need &lt;span class="math"&gt;\(m\gg N^d\)&lt;/span&gt;. The same applies for continuous features. One can assume that &lt;span class="math"&gt;\(N=2^{64}\)&lt;/span&gt; for a 64-bit computer, and still the necessary data grows exponentially with the number of dimensions.&lt;/p&gt;
&lt;p&gt;A more detailed analysis, as explained in the following section, shows that there exists an optimal &lt;span class="math"&gt;\(N_{opt}\)&lt;/span&gt; for which the accuracy is the best possible. For &lt;span class="math"&gt;\(N&amp;gt;N_{opt}\)&lt;/span&gt; the model prediction deteriorates until it starts performing as well as an empirical model given by the relative frequencies of the classes, disregarding their features. That is, when the number of features is large, the data becomes so sparse that the best we can do is to draw the classes according to their probabilities &lt;span class="math"&gt;\(P(c=0,1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a name="def"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;2. Hughes phenomenon&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Suppose we have a binary classification problem with classes &lt;span class="math"&gt;\(c_1,c_2\)&lt;/span&gt; and a training set of &lt;span class="math"&gt;\(m\)&lt;/span&gt; samples with &lt;span class="math"&gt;\(n\)&lt;/span&gt; features &lt;span class="math"&gt;\(x_i\)&lt;/span&gt;. Intuitively having a very large dataset with only very few features, that is, &lt;span class="math"&gt;\(n\ll m\)&lt;/span&gt; may lead to difficulties to learning because there may not be enough information to correctly classify the samples. On the other hand, a small dataset as compared to a very large number of features, &lt;span class="math"&gt;\(n\gg m\)&lt;/span&gt;, means that we need a very complex hypothesis function which may lead to overfitting. What is the optimal number of features &lt;span class="math"&gt;\(n_{opt}\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;We use the Bayes optimal classifier. In this case we choose the class that has higher probability according to the rule&lt;/p&gt;
&lt;div class="math"&gt;$$\tilde{c}_i=\text{argmax}_{j=1,2}P(c_j|x)$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\tilde{c}_i\)&lt;/span&gt; is the predicted class and &lt;span class="math"&gt;\(P(c,x)\)&lt;/span&gt; is the true distribution. The accuracy of the Bayes optimal classifier is then
&lt;/p&gt;
&lt;div class="math"&gt;$$\sum_{x,c}\mathbb{1}_{c,\tilde{c}}P(c,x)=\sum_{x,\tilde{c}=\text{argmax P(c|x)}} P(\tilde{c},x)=\sum_x[\text{max}_c P(c|x)] P(x) =\sum_x [\text{max}_c P(x|c)P(c)]$$&lt;/div&gt;
&lt;p&gt;
Lets define &lt;span class="math"&gt;\(P(c_1)=p_{c_1}\)&lt;/span&gt; and &lt;span class="math"&gt;\(P(c_2)=p_{c_2}\)&lt;/span&gt;, then the Bayes accuracy has the form:
&lt;/p&gt;
&lt;div class="math"&gt;$$\sum_{x=x_1}^{x_n} \text{max}\left(P(x|c_1)p_{c_1},P(x|c_2)p_{c_2}\right)$$&lt;/div&gt;
&lt;p&gt;We ought to study the Bayes accuracy over all possible environment probabilities &lt;span class="math"&gt;\(P(x|c_1)\)&lt;/span&gt; and &lt;span class="math"&gt;\(P(x|c_2)\)&lt;/span&gt;. To do this we define&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}u_i&amp;amp;\equiv&amp;amp; P(x_i|c_1), i=1\ldots n\\ v_i&amp;amp;\equiv&amp;amp; P(x_i|c_2), i=1\ldots n\end{aligned}$$&lt;/div&gt;
&lt;p&gt;
and assume that &lt;span class="math"&gt;\(u,v\)&lt;/span&gt; are themselves random  variables. 
The measure can be calculated from
&lt;/p&gt;
&lt;div class="math"&gt;$$dP(u_1,u_2,\ldots,u_n,v_1,v_2,\ldots,v_n)=Ndu_1du_2\ldots du_{n-1}dv_1dv_2\ldots dv_{n-1}$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(N\)&lt;/span&gt; is a normalization constant. Note that because of the constraints &lt;span class="math"&gt;\(\sum_i u_i=1\)&lt;/span&gt; and &lt;span class="math"&gt;\(\sum_i v_i=1\)&lt;/span&gt;, the measure does not depend on &lt;span class="math"&gt;\(du_n\)&lt;/span&gt; and &lt;span class="math"&gt;\(dv_n\)&lt;/span&gt;. To find the normalization &lt;span class="math"&gt;\(N\)&lt;/span&gt; note that the variables &lt;span class="math"&gt;\(u_i,v_i\)&lt;/span&gt; live in the hypercube &lt;span class="math"&gt;\(0\leq u_i\leq 1\)&lt;/span&gt; and &lt;span class="math"&gt;\(0\leq v_i\leq 1\)&lt;/span&gt; and must obey the conditions &lt;span class="math"&gt;\(\sum_{i=1}^n u_i= 1\)&lt;/span&gt; and &lt;span class="math"&gt;\(\sum_{i=1}^nv_i= 1\)&lt;/span&gt;, respectively. Given this we calculate
&lt;/p&gt;
&lt;div class="math"&gt;$$1=N\int_0^1 du_1\int_{0}^{1-u_1}du_2\int_0^{1-u_1-u_2}du_3\ldots \int_0^1dv_1\int_0^{1-v_1}dv_2\int_0^{1-v_1-v_2}dv_3\ldots $$&lt;/div&gt;
&lt;p&gt;The integrals can be calculated easily to give &lt;span class="math"&gt;\(N=[(n-1)!]^2\)&lt;/span&gt;. One trick is to use the unconstrained integral &lt;span class="math"&gt;\(\prod_{i=1}^n \int_0^{\infty} dx_i e^{-\alpha x_i}\)&lt;/span&gt; and then use the change of variables &lt;span class="math"&gt;\(x_i=r u_i\)&lt;/span&gt; with &lt;span class="math"&gt;\(\sum_{i=1}^nu_i=1\)&lt;/span&gt; and integrate over &lt;span class="math"&gt;\(r\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The mean Bayes accuracy is therefore
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}&amp;amp;\int\Big(\sum_i \text{max}(u_ip_{c_1},v_ip_{c_2}) \Big)dP(u,v)= \\
 &amp;amp;=n(n-1)^2\int_0^1\int_0^1du_1dv_1(1-u_1)^{n-2}(1-v_1)^{n-2}\text{max}(u_1p_{c_1},v_1p_{c_2})\end{aligned}$$&lt;/div&gt;
&lt;p&gt;
By symmetry, the sum in the first equation splits into &lt;span class="math"&gt;\(n\)&lt;/span&gt; equal terms. The integrals over the remaining &lt;span class="math"&gt;\(u_2,\ldots u_n\)&lt;/span&gt; and &lt;span class="math"&gt;\(v_2,\ldots v_n\)&lt;/span&gt; give the term &lt;span class="math"&gt;\((1-u_1)^{n-2}(1-v_1)^{n-2}\)&lt;/span&gt; (one can use again the trick of the unconstrained integral &lt;span class="math"&gt;\(\prod_{i=1}^{n-1}\int_0^{\infty}dx_ie^{-\alpha x_i}\)&lt;/span&gt;, change variables to &lt;span class="math"&gt;\(x_i=ru_i\)&lt;/span&gt; and then use the constrain &lt;span class="math"&gt;\(\sum_{i=2}^{n}u_i=1-u_1\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;The integral above is relatively easy to calculate. However, we are mostly interested in the limit when &lt;span class="math"&gt;\(n\rightarrow \infty\)&lt;/span&gt;. To do this we change variables &lt;span class="math"&gt;\(u_1\rightarrow u_1/n\)&lt;/span&gt; and &lt;span class="math"&gt;\(v_1\rightarrow v_1/n\)&lt;/span&gt; and take &lt;span class="math"&gt;\(n\gg 1\)&lt;/span&gt;. This gives
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}&amp;amp;\sim \int_0^n\int_0^ndu_1dv_1(1-u_1/n)^{n}(1-v_1/n)^{n}\text{max}(u_1p_{c_1},v_1p_{c_2})\\
&amp;amp;\sim \int_0^{\infty}\int_0^{\infty}du_1dv_1e^{-u_1-v_1}\text{max}(u_1p_{c_1},v_1p_{c_2})\\&amp;amp;=1-p_{c_1}p_{c_2}\end{aligned}$$&lt;/div&gt;
&lt;p&gt;which means that the Bayes accuracy has a limiting value as the number of features is very large. &lt;/p&gt;
&lt;p&gt;In the case of a finite dataset, we can use the empirical distribution of &lt;span class="math"&gt;\(u_i\)&lt;/span&gt; and &lt;span class="math"&gt;\(v_i\)&lt;/span&gt;. Suppose we have &lt;span class="math"&gt;\(m_1\)&lt;/span&gt; datapoints with class &lt;span class="math"&gt;\(c_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(m_2\)&lt;/span&gt; points with class &lt;span class="math"&gt;\(c_2\)&lt;/span&gt;. Then we can estimate &lt;span class="math"&gt;\(P(x_i|c_1)\)&lt;/span&gt; by the fraction of points in class &lt;span class="math"&gt;\(c_1\)&lt;/span&gt; that have feature &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; and similarly for class &lt;span class="math"&gt;\(c_2\)&lt;/span&gt;, that is, 
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}&amp;amp;P(x_i|c_1)\simeq \frac{s_i}{m_1}\\
&amp;amp;P(x_i|c_2)\simeq \frac{r_i}{m_2}\end{aligned}$$&lt;/div&gt;
&lt;p&gt;
In turn the probabilities &lt;span class="math"&gt;\(p_{c_1}\)&lt;/span&gt; and &lt;span class="math"&gt;\(p_{c_2}\)&lt;/span&gt; are given by &lt;span class="math"&gt;\(m_1/m\)&lt;/span&gt; and &lt;span class="math"&gt;\(m_2/m\)&lt;/span&gt; respectively where &lt;span class="math"&gt;\(m\)&lt;/span&gt; is the total number of datapoints. The Bayes classification rule then consists in choosing class &lt;span class="math"&gt;\(c_1\)&lt;/span&gt; for feature &lt;span class="math"&gt;\(x_1\)&lt;/span&gt; provided &lt;span class="math"&gt;\(s_1p_{c_1}/m_1=s_1/m\)&lt;/span&gt; is larger than &lt;span class="math"&gt;\(r_1p_{c_2}/m_2=r_1/m\)&lt;/span&gt;, and class &lt;span class="math"&gt;\(c_2\)&lt;/span&gt; if it is smaller. When &lt;span class="math"&gt;\(s_1=r_1\)&lt;/span&gt; we choose class which has higher prior probability. &lt;/p&gt;
&lt;p&gt;The probability of drawing &lt;span class="math"&gt;\(s_1\)&lt;/span&gt; points in class &lt;span class="math"&gt;\(c_1\)&lt;/span&gt; with feature &lt;span class="math"&gt;\(x_1\)&lt;/span&gt;, &lt;span class="math"&gt;\(s_2\)&lt;/span&gt; points with feature &lt;span class="math"&gt;\(x_2\)&lt;/span&gt;, and so on, follows a multinomial distribution:
&lt;/p&gt;
&lt;div class="math"&gt;$$P(s_1,s_2,\ldots s_n|u_1,u_2,\ldots)=\frac{m_1!}{s_1!s_2!\ldots s_n!}u_1^{s_1}u_2^{s_2}\ldots u_n^{s_n}$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(s_1+s_2+\ldots s_n=m_1\)&lt;/span&gt;. Marginalizing over &lt;span class="math"&gt;\(s_2,\ldots s_n\)&lt;/span&gt; one obtains:
&lt;/p&gt;
&lt;div class="math"&gt;$$P(s_1|u_1)=\frac{m_1!}{s_1!(m_1-s_1)!}u_1^{s_1}(1-u_1)^{m_1-s_1}$$&lt;/div&gt;
&lt;p&gt;The mean Bayes accuracy is then:
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}&amp;amp; n\int\prod_{i=1}^{n-1}du_idv_i \sum_{s_1,r_1}\text{max}(u_1p_{c_1},v_1 p_{c_2})P(s_1|u_1)P(r_1|v_1)dP(u_1,v_1,\ldots)\\
&amp;amp;=n(n-1)^2\sum_{s_1&amp;gt;r_1}{m_1\choose s_1}{m_2\choose r_1}p_{c_1}\int du_1dv_1 u_1^{s_1+1}(1-u_1)^{m_1+n-s_1-2}v_1^{r_1}(1-v_1)^{m_2+n-r_1-2} \\
&amp;amp;+ n(n-1)^2\sum_{s_1\leq r_1}{m_1\choose s_1}{m_2\choose r_1}p_{c_2}\int du_1dv_1 u_1^{s_1}(1-u_1)^{m_1+n-s_1-2}v_1^{r_1+1}(1-v_1)^{m_2+n-r_1-2}\end{aligned}$$&lt;/div&gt;
&lt;p&gt;Using &lt;span class="math"&gt;\(\int_0^1 dx x^a (1-x)^b=a!b!/(a+b+1)!\)&lt;/span&gt; we calculate:
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}n(n-1)^2&amp;amp;\sum_{s_1&amp;gt;r_1}p_{c_1}{m_1\choose s_1}{m_2\choose r_1}\frac{(s_1+1)!(m_1+n-s_1-2)!}{(m_1+n)!}\frac{r_1!(m_2+n-r_1-2)!}{(m_2+n-1)!}\\
+n(n-1)^2&amp;amp;\sum_{s_1\leq r_1}p_{c_2}{m_1\choose s_1}{m_2\choose r_1}\frac{(r_1+1)!(m_2+n-r_1-2)!}{(m_2+n)!}\frac{s_1!(m_1+n-s_1-2)!}{(m_1+n-1)!}\end{aligned}$$&lt;/div&gt;
&lt;p&gt;With some work we can simplify the expression above:
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}n(n-1)^2&amp;amp;\sum_{s_1&amp;gt;r_1}p_{c_1}(s_1+1)\frac{(m_1+n-s_1-2)(m_1+n-s_1-2)\ldots (m_1-s_1+1)}{(m_1+n)(m_1+n-1)\ldots (m_1+1)}\times \frac{(m_2+n-r_1-2)(m_2+n-r_1-2)\ldots (m_2-r_1+1)}{(m_2+n-1)(m_2+n-2)\ldots (m_2+1)}\\
+n(n-1)^2&amp;amp;\sum_{s_1\leq r_1}p_{c_2}(s_1\leftrightarrow r_1,m_1\leftrightarrow m_2)\end{aligned}$$&lt;/div&gt;
&lt;p&gt;We can determine the limit &lt;span class="math"&gt;\(n\rightarrow \infty\)&lt;/span&gt; by using the Stirling's approximation of the factorial function:
&lt;/p&gt;
&lt;div class="math"&gt;$$n!\simeq \sqrt{2\pi n}\Big(\frac{n}{e}\Big)^{n}$$&lt;/div&gt;
&lt;p&gt;
For each &lt;span class="math"&gt;\(s_1,r_1\)&lt;/span&gt; term we calculate:
&lt;/p&gt;
&lt;div class="math"&gt;$${m_1\choose s_1}{m_2\choose r_1}\frac{(s_1+1)!(m_1+n-s_1-2)!}{(m_1+n)!}\frac{r_1!(m_2+n-r_1-2)!}{(m_2+n-1)!}\simeq (s_1+1)\frac{m_1!}{(m_1-s_1)!}\frac{m_2!}{(m_2-r_1)!}n^{-(s_1+r_1+3)}+\mathcal{O}(n^{-(s_1+r_1+4)})$$&lt;/div&gt;
&lt;p&gt;
and for the other term we interchange &lt;span class="math"&gt;\(s_1\leftrightarrow r_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(m_1\leftrightarrow m_2\)&lt;/span&gt;. Only the term with &lt;span class="math"&gt;\(s_1=r_1=0\)&lt;/span&gt; gives an order &lt;span class="math"&gt;\(\mathcal{O}(1)\)&lt;/span&gt; term and so we obtain:
&lt;/p&gt;
&lt;div class="math"&gt;$$\text{lim}_{n\rightarrow \infty}=p_{c_2}$$&lt;/div&gt;
&lt;p&gt;Below a plot of the curve for some values of &lt;span class="math"&gt;\(m=m_1+m_2\)&lt;/span&gt;:
&lt;img alt="" height="400" src="/images/p105p205.png" style="display: block; margin: 0 auto" width="400"&gt;&lt;/p&gt;
&lt;p&gt;ando also for different prior probabilities:
&lt;img alt="" height="400" src="/images/p102p208.png" style="display: block; margin: 0 auto" width="400"&gt;
We see in this case that the mean accuracy first increases then it deteriorates until it reaches a limiting value for large &lt;span class="math"&gt;\(n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a name="python"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;2. Python implementation&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;multiprocessing&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;mp&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;seaborn&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sns&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Define functions:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;term&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;s1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;r1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;frac&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;s1&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m2&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;r1&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m2&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;term&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;s1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;r1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;frac&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;s1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;r1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s1&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m2&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;term&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;s1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;r1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Respectively:
&lt;/p&gt;
&lt;div class="math"&gt;$$\text{term(m1,m2,s1,r1,n)}\equiv \frac{(m_1+n-s_1-2)(m_1+n-s_1-2)\ldots (m_1-s_1+1)}{(m_1+n-2)(m_1+n-3)\ldots (m_1+1)}\times (s_1\leftrightarrow r_1,m_1\leftrightarrow m_2)$$&lt;/div&gt;
&lt;p&gt;
and
&lt;/p&gt;
&lt;div class="math"&gt;$$\text{f(m1,m2,s1,r1,n)}\equiv \frac{n(n-1)^2(s_1+1)}{(m_1+n)(m_1+n-1)(m_2+n-1)}\text{term(m1,m2,s1,r1,n)}$$&lt;/div&gt;
&lt;p&gt;The final expression is calculated as :&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;p1&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;
&lt;span class="n"&gt;p2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;p1&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;g&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;
    &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m2&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;p1&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m2&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;p2&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note that calculating all the sums can be computationally expensive, especially for large values of &lt;span class="math"&gt;\(m_1,m_2\)&lt;/span&gt; and &lt;span class="math"&gt;\(n\)&lt;/span&gt;. We have use parallel processing to handle the calculation faster. Here is an example of how to implement this using the library &lt;em&gt;multiprocessing&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{}&lt;/span&gt;
&lt;span class="n"&gt;m_list&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;m_list&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;p1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;m2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;mp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Pool&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cpu_count&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;pool&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pool&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;,[(&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;
[1] &lt;em&gt;On the mean accuracy of statistical pattern recognizers&lt;/em&gt;, Gordon F. Hughes, "Transactions on information theory", 1968&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Machine Learning"></category><category term="data science"></category></entry><entry><title>"Hoeffding's inequality"</title><link href="/hoeffdings-inequality.html" rel="alternate"></link><published>2020-05-05T00:00:00+02:00</published><updated>2020-05-05T00:00:00+02:00</updated><author><name>Joao Gomes</name></author><id>tag:None,2020-05-05:/hoeffdings-inequality.html</id><summary type="html">&lt;p&gt;We derive the Hoeffding's inequality. This is one of the most used results in machine learning theory.&lt;/p&gt;</summary><content type="html">&lt;h3&gt;&lt;strong&gt;Hoeffding's inequality&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;
Let &lt;span class="math"&gt;\(X_1,\ldots,X_m\)&lt;/span&gt; be &lt;span class="math"&gt;\(m\)&lt;/span&gt; independent random variables (not necessarily identically distributed). All &lt;span class="math"&gt;\(X_i\)&lt;/span&gt; takes values in &lt;span class="math"&gt;\([a_i,b_i]\)&lt;/span&gt;. Then for any &lt;span class="math"&gt;\(\epsilon&amp;gt;0\)&lt;/span&gt; we have&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbb{P}(|S_m-E(S_m)|\geq\epsilon)\leq e^{-2\epsilon^2/\sum_i(b_i-a_i)^2},\;S_m=\sum_{i=1}^mX_i$$&lt;/div&gt;
&lt;p&gt;If we have &lt;span class="math"&gt;\(a_i=a_j=a\)&lt;/span&gt; and &lt;span class="math"&gt;\(b_i=b_j=b\)&lt;/span&gt; for &lt;span class="math"&gt;\(\forall i,j\)&lt;/span&gt; then we have a version of the Hoeffding's inequality which is most known&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbb{P}(|\hat{X}_m-E(\hat{X}_m)|\geq\epsilon)\leq e^{-2m\epsilon^2/(b-a)^2},\; \hat{X}_m=\frac{1}{m}\sum_{i=1}^mX_i$$&lt;/div&gt;
&lt;p&gt;First we show that for &lt;span class="math"&gt;\(t&amp;gt;0\)&lt;/span&gt; we have
&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbb{P}(x\geq y)\leq e^{-ty}E(e^{t x})\label{eq1}\tag{1}$$&lt;/div&gt;
&lt;p&gt;Note that
&lt;/p&gt;
&lt;div class="math"&gt;$$e^{-ty}E(e^{tx})=\sum_{x\in X}e^{t(x-y)}P(x)$$&lt;/div&gt;
&lt;p&gt;
with &lt;span class="math"&gt;\(\sum_{x\in X}P(x)=1\)&lt;/span&gt;. We expand the r.h.s as
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\sum_{x\in X}e^{t(x-y)}P(x)&amp;amp;=&amp;amp;\sum_{x\geq y}e^{t(x-y)}P(x)+\sum_{x&amp;lt;y}e^{t(x-y)}P(x)\\
&amp;amp;\geq &amp;amp;\sum_{x\geq y}e^{t(x-y)}P(x)\\
&amp;amp;\geq &amp;amp; \sum_{x\geq y}e^{t(x-y)}P(x)=\sum_{x\geq y}P(x)=P(x\geq y)\end{eqnarray}&lt;/div&gt;
&lt;p&gt;Then we use the auxiliary distribution &lt;span class="math"&gt;\(P'(a)=(b-x)/(b-a)\)&lt;/span&gt; and &lt;span class="math"&gt;\(P'(b)=(x-a)/(b-a)\)&lt;/span&gt; with &lt;span class="math"&gt;\(a\leq x\leq b\)&lt;/span&gt; and &lt;span class="math"&gt;\(P'(a)+P'(b)=1\)&lt;/span&gt;, to show that
&lt;/p&gt;
&lt;div class="math"&gt;$$e^{tx}\leq \frac{b-x}{b-a}e^{ta}+\frac{x-a}{b-a}e^{tb}$$&lt;/div&gt;
&lt;p&gt;
because of the convexity of &lt;span class="math"&gt;\(e^{tx}\)&lt;/span&gt;. Assuming that &lt;span class="math"&gt;\(E(x)=0\)&lt;/span&gt; (this implies that &lt;span class="math"&gt;\(a&amp;lt;0\)&lt;/span&gt; and &lt;span class="math"&gt;\(b&amp;gt;0\)&lt;/span&gt;), we take the average on &lt;span class="math"&gt;\(x\)&lt;/span&gt; on both sides of the above equation to get&lt;/p&gt;
&lt;div class="math"&gt;$$E(e^{tx})\leq \frac{b}{b-a}e^{ta}-\frac{a}{b-a}e^{tb}=\frac{e^{\phi(t)}}{b-a}$$&lt;/div&gt;
&lt;p&gt;with &lt;span class="math"&gt;\(\phi(t)=\ln(be^{ta}-ae^{tb})\)&lt;/span&gt;. We can show that &lt;span class="math"&gt;\(\phi(t)\)&lt;/span&gt; is a convex function of &lt;span class="math"&gt;\(t\)&lt;/span&gt; with &lt;span class="math"&gt;\(\phi''(t)\leq (b-a)^2/4\)&lt;/span&gt; (essentially we need to show that &lt;span class="math"&gt;\(\phi''(t)\)&lt;/span&gt; has a maximum equal to &lt;span class="math"&gt;\((b-a)^2/4\)&lt;/span&gt;). Using that &lt;span class="math"&gt;\(\phi'(t=0)=0\)&lt;/span&gt; we also have &lt;span class="math"&gt;\(\phi'(t)\leq (b-a)^2t/4\)&lt;/span&gt;. Then integrating again we have &lt;span class="math"&gt;\(\phi(t)\leq \phi(0)+(b-a)^2t^2/8\)&lt;/span&gt;. This gives us&lt;/p&gt;
&lt;div class="math"&gt;$$E(e^{tx})\leq e^{t^2(b-a)^2/8}\label{eq2}\tag{2}$$&lt;/div&gt;
&lt;p&gt;Using inequalities \eqref{eq1} and \eqref{eq2}, we calculate
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}P(\hat{X}_m-E(\hat{X}_m)&amp;gt;\epsilon)&amp;amp;\leq&amp;amp; e^{-t\epsilon}E(e^{t(\hat{X}_m-E(\hat{X}_m))})\\
&amp;amp;=&amp;amp;e^{-t\epsilon}\prod_iE(e^{t(X_i-E(X))})\\
&amp;amp;\leq&amp;amp; e^{-t\epsilon} e^{t^2\sum_i(b_i-a_i)^2/8}\end{eqnarray}&lt;/div&gt;
&lt;p&gt;We can choose &lt;span class="math"&gt;\(t\)&lt;/span&gt; such that the bound is optimal (this corresponds to the minimum of the exponent). We obtain
&lt;/p&gt;
&lt;div class="math"&gt;$$P(\hat{X}_m-E(\hat{X}_m)&amp;gt;\epsilon)\leq e^{-2\epsilon^2/\sum_i(b_i-a_i)^2}$$&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Statistics"></category><category term="machine learning"></category></entry><entry><title>"Rademacher complexity"</title><link href="/rademacher-complexity.html" rel="alternate"></link><published>2020-05-02T00:00:00+02:00</published><updated>2020-05-02T00:00:00+02:00</updated><author><name>Joao Gomes</name></author><id>tag:None,2020-05-02:/rademacher-complexity.html</id><summary type="html">&lt;p&gt;The Rademacher complexity measures how a hypothesis correlates with noise. This gives a way to evaluate the capacity or complexity of a hypothesis class.&lt;/p&gt;</summary><content type="html">&lt;h3&gt;&lt;strong&gt;Table of contents&lt;/strong&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#def"&gt;Definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#bounds"&gt;Bounds&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a name="def"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;1. Definition&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The empirical Rademacher complexity of a hypothesis class &lt;span class="math"&gt;\(G=\{g\}\)&lt;/span&gt; is defined as an average over the training set &lt;span class="math"&gt;\(S=(z_1,\ldots,z_m)\)&lt;/span&gt; in the following way:&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{\mathcal{R}}(G)=E_{\sigma}\left[\text{sup}_{g\in G}\frac{1}{m}\sum_{i=1}^{m}\sigma_i g(z_i)\right]$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\sigma_i\)&lt;/span&gt; are &lt;span class="math"&gt;\(m\)&lt;/span&gt; independently and uniformly distributed random variables in the interval &lt;span class="math"&gt;\([-1,1]\)&lt;/span&gt;. Since &lt;span class="math"&gt;\(E(\sigma)=0\)&lt;/span&gt;, we see that the above average is the correlation between &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; and &lt;span class="math"&gt;\(g(z)\)&lt;/span&gt;. The Rademacher complexity therefore measures how well a hypothesis class correlates with noise. If a class has enough complexity, it will correlate more easily with noise and hence have higher rademacher complexity.&lt;/p&gt;
&lt;p&gt;The Rademacher complexity, rather than the empirical one, is in turn defined as the statistical average over the true distribution &lt;span class="math"&gt;\(D(z)^m\)&lt;/span&gt; on all the possible sets of size &lt;span class="math"&gt;\(m\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{R}_m(G)=E_{\sim D^m}(\hat{\mathcal{R}}(G))$$&lt;/div&gt;
&lt;p&gt;Note that this definition is explicitly dependent on &lt;span class="math"&gt;\(m\)&lt;/span&gt; because one cannot move the expectation in &lt;span class="math"&gt;\(z\)&lt;/span&gt; over to &lt;span class="math"&gt;\(g(z)\)&lt;/span&gt; inside the definition of the empirical Rademacher complexity.&lt;/p&gt;
&lt;p&gt;Example: suppose we have a linear classifier in two dimensions &lt;span class="math"&gt;\(g(x\in \mathbb{R}^2)\)&lt;/span&gt;, which is a line that classifies points as &lt;span class="math"&gt;\(\{-1,1\}\)&lt;/span&gt; depending on whether the point is above or below the line. If we have up to three points one can always choose a line that classifies all the points correctly. This is just a consequence of the fact that the VC dimension of &lt;span class="math"&gt;\(\mathbb{R}^2\)&lt;/span&gt; is three. Then the above supremum is attained by picking a classifier &lt;span class="math"&gt;\(g\)&lt;/span&gt; such that &lt;span class="math"&gt;\(\text{sup}_{g\in G} \sum_{i=1}^{m}\sigma_i g(z_i)=\sum_{i=1}^{m}|\sigma_i|\)&lt;/span&gt;, which is always possible if we have up to three points. The Rademacher complexity is simply &lt;span class="math"&gt;\(\mathcal{R}_{m\leq 3}=E_{\sigma}|\sigma|\)&lt;/span&gt;, and thus independent of &lt;span class="math"&gt;\(m\)&lt;/span&gt;. The same follows in higher dimensions. The Rademacher complexity is independent of &lt;span class="math"&gt;\(m\)&lt;/span&gt; if &lt;span class="math"&gt;\(m\)&lt;/span&gt; is less than the VC dimension. For &lt;span class="math"&gt;\(m\)&lt;/span&gt; bigger than the VC dimension we can find the following bound. &lt;/p&gt;
&lt;p&gt;&lt;a name="bounds"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;1. Bounds&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;One can determine several bounds on the Rademacher complexity. One of particular interest takes into account the growth function. Remember that the growth function &lt;span class="math"&gt;\(\Pi(m)\)&lt;/span&gt; is the maximal number of distinct ways of classifying a set of &lt;span class="math"&gt;\(m\)&lt;/span&gt; points &lt;span class="math"&gt;\(z_1,\ldots,z_m\)&lt;/span&gt; using an hypothesis class &lt;span class="math"&gt;\(\mathcal{H}\)&lt;/span&gt;. In order to calculate this bound we need the following lemma:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Massart's Lemma: let &lt;span class="math"&gt;\(A\subset \mathbb{R}^m\)&lt;/span&gt; be a finite set, and &lt;span class="math"&gt;\(r=\text{max}_{x\in A}\|x\|_2\)&lt;/span&gt;, then&lt;/em&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$E_{\sigma}\left[\frac{1}{m}\text{sup}_{x\in A}\sum_{i=1}^{m} \sigma_ix_i\right]\leq \frac{r\sqrt{2\ln|A|}}{m}$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\sigma_i\)&lt;/span&gt; are independent and uniformly distributed random variables in the interval &lt;span class="math"&gt;\([-1,1]\)&lt;/span&gt;. The proof goes by first using Jensen's inequality:&lt;/p&gt;
&lt;div class="math"&gt;$$\exp(t E_{\sigma}[\text{sup}_{x\in A}\sum_{i=1}^{m} \sigma_i x_i])\leq E_{\sigma}\exp(t\text{sup}_{x\in A}\sum_{i=1}^m \sigma_i x_i)\label{eq1}\tag{1}$$&lt;/div&gt;
&lt;p&gt;
Now since the exponential function is monotically increasing we have that:&lt;/p&gt;
&lt;div class="math"&gt;$$E_{\sigma}\exp(t\text{sup}_{x\in A}\sum_{i=1}^m \sigma_i x_i)=E_{\sigma}\text{sup}_{x\in A}\exp(t\sum_{i=1}^m \sigma_i x_i)\leq \sum_{x\in A} E_{\sigma}\exp(t\sum_{i=1}^m \sigma_i x_i)$$&lt;/div&gt;
&lt;p&gt;Next we use the inequality nr. 2 from Hoeffding's inequality post which states that for a random variable &lt;span class="math"&gt;\(w\in [a,b]\)&lt;/span&gt; with &lt;span class="math"&gt;\(E(w)=0\)&lt;/span&gt; we have:
&lt;/p&gt;
&lt;div class="math"&gt;$$E_w\exp(tw)\leq \exp(t^2(b-a)^2/8)$$&lt;/div&gt;
&lt;p&gt;This means that:
&lt;/p&gt;
&lt;div class="math"&gt;$$\sum_{x\in A} E_{\sigma}\exp(t\sum_{i=1}^m \sigma_i x_i)=\sum_{x\in A}\prod_iE_{\sigma_i}\exp(t \sigma_i x_i)\leq \sum_{x\in A} \exp(t^2x_i^2/2)\leq |A| \exp(t^2 r^2/2)$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(|A|\)&lt;/span&gt; is the "size" of the set &lt;span class="math"&gt;\(A\)&lt;/span&gt; and &lt;span class="math"&gt;\(r^2=\text{max}_{x\in A}\|x\|_2\)&lt;/span&gt;. Using this result in eq.\eqref{eq1} and taking the log on both sides of the inequality:
&lt;/p&gt;
&lt;div class="math"&gt;$$E_{\sigma}[\text{sup}_{x\in A}\sum_{i=1}^{m} \sigma_i x_i]\leq \frac{\ln|A|}{t}+\frac{r^2}{2}t$$&lt;/div&gt;
&lt;p&gt;. 
The optimal bound corresponds to &lt;span class="math"&gt;\(t=\sqrt{2\ln|A|/r^2}\)&lt;/span&gt;, which is the value where the function on the right side obtains its minimum. The final result is:
&lt;/p&gt;
&lt;div class="math"&gt;$$E_{\sigma}[\text{sup}_{x\in A}\sum_{i=1}^{m} \sigma_i x_i]\leq r\sqrt{2\ln |A|}$$&lt;/div&gt;
&lt;p&gt;We can apply this result to determine a bound on the Rademacher complexity for hypothesis classes with target &lt;span class="math"&gt;\(\{-1,1\}\)&lt;/span&gt;. So we have&lt;/p&gt;
&lt;div class="math"&gt;$$E_{D^m(z)}E_{\sigma}\left[\text{sup}_{g\in G}\frac{1}{m}\sum_{i=1}^{m}\sigma_i g(z_i)\right]\leq E_{D^m(z)}\frac{r}{m}\sqrt{2\ln |A|}$$&lt;/div&gt;
&lt;p&gt;
We can easily calculate &lt;span class="math"&gt;\(r^2=\sum_i^mx_i^2=m\)&lt;/span&gt; and thus &lt;span class="math"&gt;\(r=\sqrt{m}\)&lt;/span&gt;. Moreover we know that, by definition, &lt;span class="math"&gt;\(|A|\leq \Pi(m)\)&lt;/span&gt;, the growth function, and hence we find:&lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{R}_m\leq \sqrt{\frac{2\ln \Pi(m)}{m}}$$&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Machine Learning"></category><category term="data science"></category></entry><entry><title>"Hyperplanes and classification"</title><link href="/hyperplanes-and-classification.html" rel="alternate"></link><published>2020-05-01T00:00:00+02:00</published><updated>2020-05-01T00:00:00+02:00</updated><author><name>Joao Gomes</name></author><id>tag:None,2020-05-01:/hyperplanes-and-classification.html</id><summary type="html">&lt;p&gt;We study the &lt;span class="math"&gt;\(\{0,1\}\)&lt;/span&gt; classification problem in &lt;span class="math"&gt;\(\mathbb{R}^d\)&lt;/span&gt; using hyperplanes. We show that the VC dimension is &lt;span class="math"&gt;\(d+1\)&lt;/span&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;hr&gt;
&lt;h3&gt;&lt;strong&gt;1. Hyperplanes&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Consider a set of &lt;span class="math"&gt;\(d+1\)&lt;/span&gt; points in &lt;span class="math"&gt;\(\mathbb{R}^{d}\)&lt;/span&gt; dimensions and assume that no set of three points is collinear- this way any set of &lt;span class="math"&gt;\(d\)&lt;/span&gt; points forms a hyperplane. Firstly, we shall demonstrate that if a set of &lt;span class="math"&gt;\(d\)&lt;/span&gt; points is shattered in &lt;span class="math"&gt;\(\mathbb{R}^{d-1}\)&lt;/span&gt; dimensions then &lt;span class="math"&gt;\(d+1\)&lt;/span&gt; points are also shattered in &lt;span class="math"&gt;\(\mathbb{R}^d\)&lt;/span&gt;. We can use this to reduce the problem to two dimensions, where we have seen that &lt;span class="math"&gt;\(VC_{\text{dim}}=3\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Consider the representation in the picture below. Choose &lt;span class="math"&gt;\(d\)&lt;/span&gt; points and take the hyperplane formed by these. If the remaining point belongs to the hyperplane then we can consider the projection to &lt;span class="math"&gt;\(d-1\)&lt;/span&gt; dimensions, and we are left with the case of &lt;span class="math"&gt;\((d-1)+2\)&lt;/span&gt; points in &lt;span class="math"&gt;\(\mathbb{R}^{d-1}\)&lt;/span&gt;, which we shall analyse later. If this is not the case, then we can show that if the &lt;span class="math"&gt;\(d\)&lt;/span&gt; points on the hyperplane are separable then we can always find a hyperplane in &lt;span class="math"&gt;\(\mathbb{R}^d\)&lt;/span&gt; that separates all the points. In the figure below the dashed line on &lt;span class="math"&gt;\(H_d\)&lt;/span&gt; represents the hyperplane in &lt;span class="math"&gt;\(\mathbb{R}^{d-1}\)&lt;/span&gt; that separates the set of &lt;span class="math"&gt;\(d\)&lt;/span&gt; points. It is easy to see that any hyperplane that contains the remaining point and the dashed line (hyperplane in one lower dimension) is the solution to this problem.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" height="300" src="/images/hyperplanes_dplus1.png" style="display: block; margin: 0 auto" width="300"&gt;&lt;/p&gt;
&lt;p&gt;We shall consider now the case of &lt;span class="math"&gt;\(d+2\)&lt;/span&gt; points in &lt;span class="math"&gt;\(\mathbb{R}^d\)&lt;/span&gt;. For this purpose we shall use Radon's theorem that states that any set of &lt;span class="math"&gt;\(d+2\)&lt;/span&gt; points in &lt;span class="math"&gt;\(\mathbb{R}^d\)&lt;/span&gt; can be partitioned in two sets &lt;span class="math"&gt;\(X_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(X_2\)&lt;/span&gt; such that the corresponding convex hulls intersect. This implies that &lt;span class="math"&gt;\(d+2\)&lt;/span&gt; points in &lt;span class="math"&gt;\(\mathbb{R}^d\)&lt;/span&gt; cannot be shatered because if they were then we would have two non intersecting convex hulls separated by a plane, thus contradicting Radon's theorem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For &lt;span class="math"&gt;\(d+2\)&lt;/span&gt; points &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; in &lt;span class="math"&gt;\(\mathbb{R}^d\)&lt;/span&gt; one can always choose &lt;span class="math"&gt;\(d+2\)&lt;/span&gt; parameters &lt;span class="math"&gt;\(\alpha_i\)&lt;/span&gt; such that:
&lt;/p&gt;
&lt;div class="math"&gt;$$\sum_{i=1}^{d+2}\alpha_ix_i=0,\;\; \sum_{i=1}^{d+2}\alpha_i=0$$&lt;/div&gt;
&lt;p&gt;
The reason is because one has &lt;span class="math"&gt;\(d+2\)&lt;/span&gt; unknowns (&lt;span class="math"&gt;\(\alpha_i\)&lt;/span&gt;) for &lt;span class="math"&gt;\(d+1\)&lt;/span&gt; equations (&lt;span class="math"&gt;\(d\)&lt;/span&gt; coming from the first vector equation and an additional from the constraint on &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;). The second equation can be rewritten as a sum over positive &lt;span class="math"&gt;\(\alpha_{&amp;gt;}\)&lt;/span&gt; and negative &lt;span class="math"&gt;\(\alpha_{&amp;lt;}\)&lt;/span&gt;, that is, &lt;span class="math"&gt;\(\sum_{i}\alpha_i^{&amp;gt;}=\sum_{i}\alpha_i^{&amp;lt;}\)&lt;/span&gt;. Define &lt;span class="math"&gt;\(\alpha=\sum_i\alpha_i^{&amp;gt;}\)&lt;/span&gt;, then we have 
&lt;/p&gt;
&lt;div class="math"&gt;$$\sum_i\frac{\alpha_i^{&amp;gt;}}{\alpha}=\sum_i\frac{\alpha_i^{&amp;lt;}}{\alpha}$$&lt;/div&gt;
&lt;p&gt;
which is a sum over numbers in the interval &lt;span class="math"&gt;\((0,1]\)&lt;/span&gt;. The vector equation separates into two terms
&lt;/p&gt;
&lt;div class="math"&gt;$$\sum_{i}\frac{\alpha_i^{&amp;gt;}}{\alpha}x_i=\sum_i\frac{\alpha_i^{&amp;lt;}}{\alpha}x_i$$&lt;/div&gt;
&lt;p&gt;
and each of the sets &lt;span class="math"&gt;\(X_1=\{x_i: \alpha_i^{&amp;gt;}\neq 0\}\)&lt;/span&gt; and &lt;span class="math"&gt;\(X_2=\{x_i: \alpha_i^{&amp;lt;}\neq 0\}\)&lt;/span&gt; form convex hulls. This means that &lt;span class="math"&gt;\(X_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(X_2\)&lt;/span&gt; intersect.&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;[1] &lt;em&gt;Foundations of machine learning&lt;/em&gt;, M. Mohri, A. Rostamizadeh, A. Talwalkar&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Machine Learning"></category><category term="data science"></category></entry><entry><title>"VC dimension"</title><link href="/vc-dimension.html" rel="alternate"></link><published>2020-04-30T00:00:00+02:00</published><updated>2020-04-30T00:00:00+02:00</updated><author><name>Joao Gomes</name></author><id>tag:None,2020-04-30:/vc-dimension.html</id><summary type="html">&lt;p&gt;The VC dimension is a very important concept in machine learning theory. It gives a measure of complexity based on combinatorial aspects. This concept is used to show how certain infinite hypothesis classes are PAC-learnable. Some of the main concepts are explained: growth function and shattering. I give examples and show how the VC dimension is used to bound the generalisation error.&lt;/p&gt;</summary><content type="html">&lt;h3&gt;&lt;strong&gt;Table of contents&lt;/strong&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#VC"&gt;VC dimension&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#growth"&gt;Growth function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#genbounds"&gt;Generalisation bounds&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a name="VC"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;1. VC dimension&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;
VC stands for Vapnik-Chervonenkis. The VC dimension plays the role of dimension of &lt;span class="math"&gt;\(\mathcal{H}\)&lt;/span&gt;, when &lt;span class="math"&gt;\(\mathcal{H}\)&lt;/span&gt; has infinite number of hypotheses. In the post on &lt;a href="/probably-approximately-correct-pac.html"&gt;PAC learnability&lt;/a&gt; we have shown that the circumference hypothesis is PAC learnable despite the class being infinite since each circumference is parametrised by a continuous parameter, the radius &lt;span class="math"&gt;\(R\)&lt;/span&gt;. One can find several other examples which depend on continuous parameters but they are nevertheless learnable. In this post we analyse necessary conditions for infinite classes to be PAC learnable.&lt;/p&gt;
&lt;p&gt;To do this, first we need to understand the concept of &lt;em&gt;shattering&lt;/em&gt;. Say we have  a set of hypotheses &lt;span class="math"&gt;\(\mathcal{H}=\{h_a(x)\}\)&lt;/span&gt; from a domain &lt;span class="math"&gt;\(\chi\)&lt;/span&gt; to &lt;span class="math"&gt;\(\{0,1\}\)&lt;/span&gt;. Here &lt;span class="math"&gt;\(a\)&lt;/span&gt; is(are) a continuous parameter(s). Consider a subset &lt;span class="math"&gt;\(C\subset \chi\)&lt;/span&gt; consisting of a number of points &lt;span class="math"&gt;\(C=\{c_1,c_2,\ldots,c_n\}\)&lt;/span&gt;. The restriction of a hypothesis &lt;span class="math"&gt;\(h_a(x)\in\mathcal{H}\)&lt;/span&gt; to &lt;span class="math"&gt;\(C\)&lt;/span&gt; is &lt;span class="math"&gt;\(\{h_a(c_1),h_a(c_2),\dots,h_a(c_n)\}\)&lt;/span&gt;. By dialling the continuous parameter &lt;span class="math"&gt;\(a\)&lt;/span&gt; we generate images of the restriction &lt;span class="math"&gt;\((h_a(c_1),h_a(c_2),\dots,h_a(c_n))=(1,0,1,\ldots),(0,0,1,\ldots),\ldots\)&lt;/span&gt;. Depending on the set &lt;span class="math"&gt;\(C\)&lt;/span&gt; we may or not generate all the possible images, which total to &lt;span class="math"&gt;\(2^n\)&lt;/span&gt;. When it generates all possible images we say that &lt;span class="math"&gt;\(\mathcal{H}\)&lt;/span&gt; &lt;em&gt;shatters&lt;/em&gt; &lt;span class="math"&gt;\(C\)&lt;/span&gt;. &lt;em&gt;The VC dimension is the dimension of the largest set &lt;span class="math"&gt;\(C\)&lt;/span&gt; that can be shattered.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Set of thresholds &lt;span class="math"&gt;\(h_a(x)=\mathbb{1}_{x\geq a}\)&lt;/span&gt;, which returns &lt;span class="math"&gt;\(1\)&lt;/span&gt; for &lt;span class="math"&gt;\(x\geq a\)&lt;/span&gt; and &lt;span class="math"&gt;\(0\)&lt;/span&gt; otherwise. Clearly for any &lt;span class="math"&gt;\(c_1\)&lt;/span&gt;, &lt;span class="math"&gt;\(h_a(c_1)\)&lt;/span&gt; spans &lt;span class="math"&gt;\(\{0,1\}\)&lt;/span&gt;. However, if we have an additional point &lt;span class="math"&gt;\(c_2&amp;gt;c_1\)&lt;/span&gt; then we cannot generate the image &lt;span class="math"&gt;\((h(c_1),h(c_2))=(1,0)\)&lt;/span&gt;. In fact generalising for arbitrary number of points with &lt;span class="math"&gt;\(c_1&amp;lt;c_2&amp;lt;\ldots&amp;lt;c_n\)&lt;/span&gt; we always have that if &lt;span class="math"&gt;\(h_(c_1)=1\)&lt;/span&gt; then all the reamining images are &lt;span class="math"&gt;\(h(c_2),\ldots,h(c_n)=1\)&lt;/span&gt;. Therefore the VC dimension is &lt;span class="math"&gt;\(VC_{\text{dim}}=1\)&lt;/span&gt;. Note that this the same set of hypothesis in the cirumference case &lt;a href="/probably-approximately-correct-pac.html"&gt;PAC learnability&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set of intervals &lt;span class="math"&gt;\(h_{a,b}(x)=\mathbb{1}_{a\leq x\leq b}\)&lt;/span&gt;, which returns one for a point inside the interval &lt;span class="math"&gt;\([a,b]\)&lt;/span&gt; and zero otherwise. Clearly &lt;span class="math"&gt;\(h_{a,b}\)&lt;/span&gt; shatters a single point. We can easily see that two points can also be shattered. However, a set with three points cannot be shattered. In the case we have &lt;span class="math"&gt;\(h_{a,b}(c_1)=1\)&lt;/span&gt; and &lt;span class="math"&gt;\(h_{a,b}(c_2)=0\)&lt;/span&gt; with &lt;span class="math"&gt;\(c_2&amp;gt;c_1\)&lt;/span&gt; a third point &lt;span class="math"&gt;\(c_3&amp;gt;c_2\)&lt;/span&gt; cannot have &lt;span class="math"&gt;\(h_{a,b}(c_3)=1\)&lt;/span&gt;. Therefore the VC dimension is &lt;span class="math"&gt;\(VC_{\text{dim}}=2\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set of hyperplanes in &lt;span class="math"&gt;\(\mathbb{R}^2\)&lt;/span&gt;. The hyperplane divides the space in two regions. A point falling on one side will have class zero, while if it falls on the other, will have class one. The same hyperplane can give rise to two different hypotheses by interchanging the labels between the sides. It is easy to see that one and two point set can be shattered. Consider now a three-point set. If they are collinear then there's always two combinations &lt;span class="math"&gt;\((1,0,1)\)&lt;/span&gt; and &lt;span class="math"&gt;\((0,1,0)\)&lt;/span&gt; that cannot be shattered. If they are not collinear, then the dichotomies with two ones and one zero, like &lt;span class="math"&gt;\((1,1,0)\)&lt;/span&gt;, and two zeros and one one, such as &lt;span class="math"&gt;\((0,0,1)\)&lt;/span&gt; can be generated. The remaining dichotomies &lt;span class="math"&gt;\((0,0,0)\)&lt;/span&gt; and &lt;span class="math"&gt;\((1,1,1)\)&lt;/span&gt; are generated by interchanging the sides. Therefore the set of three non-collinear points can be shattered. Consider now a set of four points and assume that three are non-collinear (if they are collinear then we fall back in the previous situation). The dichotomies depicted in the figure below (&lt;a href="#dichotomies"&gt;Fig.1&lt;/a&gt;) show two examples that cannot be generated. Thus showing that there is no four-point set that can be shattered. The VC dimension is therefore &lt;span class="math"&gt;\(VC_{\text{dim}}=3\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Hyperplanes in &lt;span class="math"&gt;\(\mathbb{R}^d\)&lt;/span&gt;. One can show that the VC dimension is &lt;span class="math"&gt;\(VC_{\text{dim}}=d+1\)&lt;/span&gt;. The demonstration can be found in the post &lt;a href="/hyperplanes-and-classification.html"&gt;Hyperplanes and classification&lt;/a&gt;. This will be very useful when studying support-vector-machines.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a name="dichotomies"&gt;&lt;/a&gt;
&lt;img alt="dichotomies" height="400" src="/images/hyperplane_dichotomies.png" style="display: block; margin: 0 auto" width="400"&gt;
  &lt;em&gt;Fig.1 Dichotomies that cannot be realised. a) The fourth point is in the interior of the triangle. b) The set forms a convex four-polygon.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The VC dimension provides a measure of how complex a hypothesis class can be. If the class is increasingly complex it allows for larger sets to be shattered. This measure is purely combinatorial and does not rely on which distribution the points are sampled from.  &lt;/p&gt;
&lt;p&gt;&lt;a name="growth"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;2. The growth function&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;
The growth function counts how many ways we can classify a set of fixed size using a hypothesis class. The proper definition is&lt;/p&gt;
&lt;div class="math"&gt;$$\Pi(m)=\text{max}_{\substack{x_1,\ldots,x_m \subseteq X}}|(h(x_1),\ldots,h(x_m)),h:\mathcal{H}|$$&lt;/div&gt;
&lt;p&gt;When the set &lt;span class="math"&gt;\(x_1,\ldots,x_m\)&lt;/span&gt; is shattered by &lt;span class="math"&gt;\(\mathcal{H}\)&lt;/span&gt; one has &lt;span class="math"&gt;\(\Pi(m)=2^m\)&lt;/span&gt;. If in addition this is the largest shattered set, then &lt;span class="math"&gt;\(\Pi(m)=2^{VC_{\text{dim}}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;One of the most important aspects of the growth function is that for &lt;span class="math"&gt;\(m&amp;gt;VC_{\text{dim}}\)&lt;/span&gt;, &lt;span class="math"&gt;\(\Pi(m)\)&lt;/span&gt; always has polynomial growth rather than exponential. This is demonstrated using the following statement:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sauer's Lemma:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Let &lt;span class="math"&gt;\(VC_{\text{dim}}=d\)&lt;/span&gt;. Then for all &lt;span class="math"&gt;\(m\)&lt;/span&gt;&lt;/em&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$ \Pi(m)\leq \sum_{i=0}^{d}\left(\begin{array}{c}m \\ i\end{array}\right)$$&lt;/div&gt;
&lt;p&gt;For &lt;span class="math"&gt;\(t\leq m\)&lt;/span&gt; we have&lt;/p&gt;
&lt;div class="math"&gt;$$\sum_{i=0}^{d}\left(\begin{array}{c}m \\ i\end{array}\right)\leq \sum_{i=0}^{m}\left(\begin{array}{c}m \\ i\end{array}\right)\left(\frac{m}{t}\right)^{d-i}=\left(\frac{m}{t}\right)^d\left(1+\frac{t}{m}\right)^m$$&lt;/div&gt;
&lt;p&gt;Using that &lt;span class="math"&gt;\(1+x\leq e^x, \forall x\)&lt;/span&gt;, we get
&lt;/p&gt;
&lt;div class="math"&gt;$$\left(\frac{m}{t}\right)^d\left(1+\frac{t}{m}\right)^m\leq \left(\frac{m}{t}\right)^d e^t$$&lt;/div&gt;
&lt;p&gt;Now we can set &lt;span class="math"&gt;\(t=d\)&lt;/span&gt; for which the bound becomes optimal, that is, &lt;span class="math"&gt;\(t^{-d} e^t\geq d^{-d}e^d\)&lt;/span&gt; (we can do this by finding the minimum of &lt;span class="math"&gt;\(t-d\ln(t)\)&lt;/span&gt;). Hence we obtain&lt;/p&gt;
&lt;div class="math"&gt;$$\Pi(m)\leq \left(\frac{m}{d}\right)^d e^d$$&lt;/div&gt;
&lt;p&gt;
&lt;a name="genbounds"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;3. The generalisation bound for infinite classes&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;
The Vapnik-Chervonenkis theorem (1971) states that, for any &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;,&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbb{P}(\text{sup}_{h\in \mathcal{H}}|L_S(h)-L_D(h)|&amp;gt;\epsilon)\leq 8\Pi(m)e^{-m\epsilon^2/32} \label{eq3}\tag{3}$$&lt;/div&gt;
&lt;p&gt;We can now understand the importance of the VC dimension. We have learnt that if VC dimension is finite than the growth function &lt;span class="math"&gt;\(\Pi(m)\)&lt;/span&gt; grows polynomially for &lt;span class="math"&gt;\(m&amp;gt;VC_{\text{dim}}\)&lt;/span&gt;. This implies from the inequality \eqref{eq3} that&lt;/p&gt;
&lt;div class="math"&gt;$$m\rightarrow \infty,\;|L_S(h)-L_D(h)|\rightarrow 0,\;\text{in propability}$$&lt;/div&gt;
&lt;p&gt;which means that we can find arbitrary &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; and &lt;span class="math"&gt;\(\delta\)&lt;/span&gt; such that for &lt;span class="math"&gt;\(m\geq m_{\mathcal{H}}\)&lt;/span&gt;, the sample complexity, the problem is PAC learnable.&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;
[1] &lt;em&gt;Understanding Machine Learning: from Theory to Algorithms&lt;/em&gt;, Shai Ben-David and Shai Shalev-Shwartz&lt;/p&gt;
&lt;p&gt;[2] &lt;em&gt;A probabilistic theory of pattern recognition&lt;/em&gt;, Luc Devroye, Laszlo Gyorfi, Gabor Lugosi&lt;/p&gt;
&lt;p&gt;[3] &lt;em&gt;Foundations of machine learning&lt;/em&gt;, M. Mohri, A. Rostamizadeh, A. Talwalkar&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Machine Learning"></category><category term="data science"></category></entry><entry><title>"Bayes Optimal Classifier"</title><link href="/bayes-optimal-classifier.html" rel="alternate"></link><published>2020-04-26T00:00:00+02:00</published><updated>2020-04-26T00:00:00+02:00</updated><author><name>Joao Gomes</name></author><id>tag:None,2020-04-26:/bayes-optimal-classifier.html</id><summary type="html">&lt;p&gt;I explain the Bayes optimal classifier and provide some numerical examples.&lt;/p&gt;</summary><content type="html">&lt;h3&gt;&lt;strong&gt;Table of contents&lt;/strong&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#bayes"&gt;Optimal classifier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#multiclass"&gt;Multiple classes&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a name="bayes"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span style="color:dark"&gt; &lt;strong&gt;1. Optimal classifier&lt;/strong&gt; &lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;
The Bayes optimal classifier is a binary predictor which has the lowest generalisation error. That is, for any other predictor &lt;span class="math"&gt;\(g\)&lt;/span&gt; we always have&lt;/p&gt;
&lt;div class="math"&gt;$$L(D,h_{\text{Bayes}})\leq L(D,g)$$&lt;/div&gt;
&lt;p&gt;The Bayes predictor is defined as follows:&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
h_{\text{Bayes}}=\text{argmax}_{y}P(y|x)
\end{equation}&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt;&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
L(D,g)=\sum_{x}\mathbb{1}\left[g(x)\neq y(x)\right]D(x,y)
\end{equation}&lt;/div&gt;
&lt;p&gt;Use the Bayes property &lt;span class="math"&gt;\(D(x,y)=D(y|x)D(x)\)&lt;/span&gt; and the fact that we have only two classes, say &lt;span class="math"&gt;\(y=0,1\)&lt;/span&gt;, then&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
L(D,g)=\sum_{x:g(x)=0}D(y=1|x)D(x)+\sum_{x:g(x)=1}D(y=0|x)D(x)\\
\end{equation}&lt;/div&gt;
&lt;p&gt;
Use the property that &lt;span class="math"&gt;\(a\geq \text{Min}(a,b)\)&lt;/span&gt; and write&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
L(D,g)\geq&amp;amp;&amp;amp;\sum_{x:g(x)=0}\text{Min}\big(D(y=1|x),D(y=0|x)\big)D(x)+\sum_{x:g(x)=1}\text{Min}\big(D(y=1|x),D(y=0|x)\big)D(x)\\
&amp;amp;&amp;amp;=\sum_{x}\text{Min}\big(D(y=1|x),D(y=0|x)\big)D(x)
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Note that the r.h.s is precisely the loss of the Bayes classifier. That  is,&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
L(D,h_{\text{Bayes}})&amp;amp;=&amp;amp;\sum_{x:h(x)=0}D(y=1|x)D(x)+\sum_{x:h(x)=1}D(y=0|x)D(x)\\
&amp;amp;=&amp;amp;\sum_{D(y=1|x)&amp;lt;D(y=0|x)}D(y=1|x)D(x)+\sum_{D(y=1|x)&amp;gt;D(y=0|x)}D(y=0|x)D(x)
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;a name="multiclass"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span style="color:dark"&gt; &lt;strong&gt;2. Multiple classes&lt;/strong&gt; &lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;
Can we generalise this to multi-classes? We can use &lt;span class="math"&gt;\(a\geq \text{Min}(a,b,c,\ldots)\)&lt;/span&gt; to write&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}\label{eq1}
L(D,g)\geq \sum_{x:g(x)\neq y_1}\text{Min}\big(D(y_1|x),D(y_2|x),\ldots\big)D(x)+\sum_{x:g(x)\neq y_2}\text{Min}\big(D(y_1|x),D(y_2|x),\ldots\big)D(x)+\ldots \tag{1}
\end{equation}&lt;/div&gt;
&lt;p&gt;Suppose we extend the Bayes optimal classifier to more classes by predicting the class that has higher probability. Then we have&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
L(D,h)=\sum_{x:h(x)=y_1\cup h(x)=y_2\ldots}D(y_0|x)D(x)+\sum_{x:h(x)=y_0\cup h(x)=y_2\ldots}D(y_1|x)D(x)+\ldots\\
\end{equation}&lt;/div&gt;
&lt;p&gt;Since &lt;span class="math"&gt;\(h(x)\)&lt;/span&gt; is a predictor the sets &lt;span class="math"&gt;\(S_i=\{x:h(x)=y_i\}\)&lt;/span&gt; are disjoint and so we can simplify the sums above. For example&lt;/p&gt;
&lt;div class="math"&gt;$$\sum_{x:h(x)=y_1\cup h(x)=y_2\ldots}D(y_0|x)D(x)=\sum_{x:h(x)=y_1}D(y_0|x)D(x)+\sum_{x:h(x)=y_2\ldots}D(y_0|x)D(x)+\ldots$$&lt;/div&gt;
&lt;p&gt;The issue we face now is that since we have multiple classes the maximum value does not determine uniquely the minimum value, and vice-versa, and hence we cannot apply the reasoning used in the binary case. Following similar steps as before, one can show that the multi-class Bayes classifier does not saturate the bound \eqref{eq1}. As a matter of fact there is no classifier that saturates the bound \eqref{eq1}. For that to happen we would need a classifier &lt;span class="math"&gt;\(h(x)\)&lt;/span&gt; such that when &lt;span class="math"&gt;\(h(x)=y_i\)&lt;/span&gt; we have &lt;span class="math"&gt;\(\text{Min}\big(D(y_1|x),D(y_2|x),\ldots\big)=D(y_{k\neq i}|x)\,\forall i,k\)&lt;/span&gt;. This means that for a fixed &lt;span class="math"&gt;\(i\)&lt;/span&gt; we have &lt;span class="math"&gt;\(D(y_{k\neq i}|x)=D(y_{j\neq i}|x)\, \forall k,j\neq i\)&lt;/span&gt;. It is then easy to see that this implies that &lt;span class="math"&gt;\(D(y_i|x)\)&lt;/span&gt; is a constant, independent of &lt;span class="math"&gt;\(x\)&lt;/span&gt;, contradicting our assumption.&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;Python implementation&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;seaborn&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sns&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.naive_bayes&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GaussianNB&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We compare three different hypotheses:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Optimal Bayes: &lt;span class="math"&gt;\(h_{\text{Bayes}}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Circumference hypothesis: &lt;span class="math"&gt;\(h\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Gaussian Naive Bayes: &lt;span class="math"&gt;\(h_{GNB}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;#P(y|x) definition&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;prob&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.3&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="c1"&gt;#prob of y=1&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt;

&lt;span class="c1"&gt;#coloring function&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;color&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;=&lt;/span&gt;&lt;span class="n"&gt;prob&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;blue&amp;#39;&lt;/span&gt; &lt;span class="c1"&gt;#y=1&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;red&amp;#39;&lt;/span&gt; &lt;span class="c1"&gt;#y=0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The code that defines the various hypotheses:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;h&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;=&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="c1"&gt;#if r=1 then h(x)=bayes(x)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;blue&amp;#39;&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;red&amp;#39;&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;bayes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;prob&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;blue&amp;#39;&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;red&amp;#39;&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;GNB&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;GaussianNB&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sample&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;ypred&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;ypred&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;errors&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="c1"&gt;#draw multiple samples from multivariate_normal&lt;/span&gt;
    &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multivariate_normal&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],[[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;class_y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;x&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;y&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sample&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Series&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;class_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;h_bayes&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bayes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;h&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;GNB&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;GNB&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;error_GNB&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sample&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;GNB&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;error_bayes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sample&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;h_bayes&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;error_h&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sample&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;h&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="n"&gt;errors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;error_h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;error_GNB&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;error_bayes&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;then check whether the other hypotheses have smaller error:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;errors&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note that these are the sample errors. Therefore, it is possible to find an error smaller than the Bayes error. However, if we take large samples it becomes almost improbable for that to happen.&lt;/p&gt;
&lt;p&gt;Some plots:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" height="600" src="/images/bayes_sample.png" style="display: block; margin: 0 auto" width="600"&gt;
&lt;img alt="" height="600" src="/images/optimal_bayes.png" style="display: block; margin: 0 auto" width="600"&gt;
&lt;img alt="" height="600" src="/images/optimal_bayes_GNB.png" style="display: block; margin: 0 auto" width="600"&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;
[1] &lt;em&gt;Understanding Machine Learning: from Theory to Algorithms&lt;/em&gt;, Shai Ben-David and Shai Shalev-Shwartz&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Machine Learning"></category><category term="data science"></category></entry><entry><title>"Probably Approximately Correct (PAC)"</title><link href="/probably-approximately-correct-pac.html" rel="alternate"></link><published>2020-04-14T00:00:00+02:00</published><updated>2020-04-14T00:00:00+02:00</updated><author><name>Joao Gomes</name></author><id>tag:None,2020-04-14:/probably-approximately-correct-pac.html</id><summary type="html">&lt;p&gt;In this post I explain some of the fundamentals of machine learning: PAC learnability, overfitting and generalisation bounds for classification problems. I show how these concepts work in detail for the problem of learning circumferences.&lt;/p&gt;</summary><content type="html">&lt;h3&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#pac"&gt;The learning problem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#proof"&gt;Finite hypothesis classes are PAC learnable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#agnostic"&gt;Agnostic learning&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a name="pac"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;1. The learning problem&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;
PAC stands for "probably approximately correct". In machine learning we want to find a hypothesis that is as close as possible to the ground truth. Since we only have access to a sample of the real distribution, the hypothesis that one builds is itself a function of the sample data, and therefore it is a random variable.  The problem that we want to solve is whether the sample error incurred in choosing a particular hypothesis  is approximately the same as the exact distribution error, within a certain confidence interval.&lt;/p&gt;
&lt;p&gt;Suppose we have a binary classification problem (the same applies for multi-class) with classes &lt;span class="math"&gt;\(y_i\in \{y_0,y_1\}\)&lt;/span&gt;, and we are given a training dataset &lt;span class="math"&gt;\(S\)&lt;/span&gt; with &lt;span class="math"&gt;\(m\)&lt;/span&gt; data-points. Each data-point is characterised by &lt;span class="math"&gt;\(Q\)&lt;/span&gt; features, and represented as a vector &lt;span class="math"&gt;\(q=(q_1,q_2,\ldots,q_Q)\)&lt;/span&gt;. We want to find a map &lt;span class="math"&gt;\(\mathcal{f}\)&lt;/span&gt; between these features and the corresponding class &lt;span class="math"&gt;\(y\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}\mathcal{f}: (q_1,q_2,\ldots,q_Q)\rightarrow \{y_0,y_1\}\end{equation}&lt;/div&gt;
&lt;p&gt;This map, however, does not always exist. There are problems for which we can only determine the class up to a certain confidence level. In this case we say that the learning problem is &lt;em&gt;agnostic&lt;/em&gt;, while when the map exists we say that the problem is &lt;em&gt;realisable&lt;/em&gt;. For example, image recognition is agnostic.&lt;/p&gt;
&lt;p&gt;Let us assume for the moment that such a map exists. The learner chooses a set of hypothesis &lt;span class="math"&gt;\(\mathcal{H}=\{h_1,\ldots,h_n\}\)&lt;/span&gt; and thus introduces &lt;em&gt;bias&lt;/em&gt; in the problem- a different learner may chose a different set of hypothesis. Then, in order to find the hypothesis that most accurately represents the data, the learner chooses one that has the smallest empirical risk, which is the error on the training set. That is, one tries to find the minimum of the sample loss function&lt;/p&gt;
&lt;div class="math"&gt;$$L_S(h)=\frac{1}{m}\sum_{i=1:m}\mathbb{1}\left[h(x_i)\neq y(x_i)\right],\;h\in \mathcal{H}$$&lt;/div&gt;
&lt;p&gt;with &lt;span class="math"&gt;\(\mathbb{1}(.)\)&lt;/span&gt; the Kronecker delta function. Denote the solution of this optimisation problem as &lt;span class="math"&gt;\(h_S\)&lt;/span&gt;. The true or &lt;em&gt;generalization error&lt;/em&gt; is defined instead as the unbiased average&lt;/p&gt;
&lt;div class="math"&gt;$$L(D,h)=\sum_x\mathbb{1}\left[h(x)\neq y(x)\right]D(x)$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(D(x)\)&lt;/span&gt; is a distribution, that the learner may or may not know. In the case of classification, the generalisation error is also the probability of misclassifying a point &lt;span class="math"&gt;\(L(D,h)=\mathbb{P}_{x\sim D(x)}(h(x)\neq y(x))\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If we choose appropriately &lt;span class="math"&gt;\(\mathcal{H}\)&lt;/span&gt; we may find &lt;span class="math"&gt;\(\text{min}\;L_S(h_S)=0\)&lt;/span&gt;. This can happen, for example, by memorising the data. In this case, we say that the hypothesis is &lt;em&gt;overfitting&lt;/em&gt; the data. Although memorising results in zero empirical error, the solution is not very instructive because it does not give information of how well it will perform on unseen data. The solution performs very well on the data because the learner used prior knowledge to choose an hypothesis set with sufficient capacity (or complexity) to accommodate the entire dataset. In the above minimisation problem, one should find a solution that does well (small error) on a large number of samples rather then having a very small error in a particular sample. Overfitting solutions should be avoided as they can lead to misleading conclusions. Instead, the learner should aim at obtaining a training error that is comparable to the error obtained with different samples.&lt;/p&gt;
&lt;p&gt;To make things practical, consider the problem of classifying points on a 2D plane as red or blue. The decision boundary is a circumference of radius &lt;span class="math"&gt;\(R\)&lt;/span&gt; concentric with the origin of the plane, which colours points that are inside as red and outside as blue. See figure below. The training dataset consists of &lt;span class="math"&gt;\(m\)&lt;/span&gt; data-points &lt;span class="math"&gt;\(\mathbb{x}=(x_1,x_2)\)&lt;/span&gt; sampled independently and identically distributed (i.i.d) from a distribution &lt;span class="math"&gt;\(D(x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" height="400" src="/images/PAC learning_1.png" style="display: block; margin: 0 auto" width="400"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Here the circumference &lt;span class="math"&gt;\(R\)&lt;/span&gt; denotes the ground truth which classifies points as red or blue, depending on whether they are inside or outside of the circle, respectively.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The learning problem is to find a hypothesis &lt;span class="math"&gt;\(h(x): x\rightarrow y=\{\text{blue},\text{red}\}\)&lt;/span&gt; that has small error on unseen data.   &lt;/p&gt;
&lt;p&gt;Assuming that the learner has prior knowledge of the ground truth (realisability assumption), one of the simplest algorithms is to consider the set of concentric circumferences and minimise the empirical risk. One can achieve this by drawing a decision boundary that is as close as possible to the most outward red (or inward blue data-points). This guarantees that when &lt;span class="math"&gt;\(m\rightarrow \infty\)&lt;/span&gt; we recover the exact decision boundary: the circumference &lt;span class="math"&gt;\(R\)&lt;/span&gt;.  The empirical risk minimisation problem gives the solution represented in the figure below by the circumference &lt;span class="math"&gt;\(R'\)&lt;/span&gt;. However, newly generated data-points may lie in between &lt;span class="math"&gt;\(R'\)&lt;/span&gt; and &lt;span class="math"&gt;\(R\)&lt;/span&gt;, and therefore would be misclassified.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" height="400" src="/images/circle_learning_epsilon.png" style="display: block; margin: 0 auto" width="400"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;a) The hypothesis &lt;span class="math"&gt;\(h\)&lt;/span&gt; is a circumference of radius &lt;span class="math"&gt;\(R'\)&lt;/span&gt; concentric with the origin and it is determined by the most outward red data-point. This ensures that all training set &lt;span class="math"&gt;\(S\)&lt;/span&gt; is correctly classified. b) The circumference of radius &lt;span class="math"&gt;\(R_{\epsilon}\)&lt;/span&gt; corresponds to a hypothesis &lt;span class="math"&gt;\(h_{\epsilon}\)&lt;/span&gt; that has generalization error &lt;span class="math"&gt;\(L(D,h_{\epsilon})=\epsilon\)&lt;/span&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Given that this is an overfitting solution, one has to be careful of how well it generalises. It is possible that the generalisation error is small for such a solution, but one has to be confident of how common this situation may be. If the sample that led to that solution is a rare event then we should not trust its predictions. Therefore we are interested in bounding the probability of making a bad prediction, that is,&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}\label{eq1}\mathbb{P}_{S \sim D^m(x)}(L(D,h_S)&amp;gt;\epsilon)&amp;lt;\delta \tag{1}\end{equation}&lt;/div&gt;
&lt;p&gt;Conversely, this tells us with confidence of at least &lt;span class="math"&gt;\(1-\delta\)&lt;/span&gt; that&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}\label{eq2}L(D,h_S)\leq\epsilon\tag{2}\end{equation}&lt;/div&gt;
&lt;p&gt;A &lt;em&gt;PAC learnable hypothesis&lt;/em&gt; is a hypothesis for which one can put a bound on the probability of the form \eqref{eq1} with &lt;span class="math"&gt;\(\epsilon, \delta\)&lt;/span&gt; arbitrary.&lt;/p&gt;
&lt;p&gt;In  the case of the circumference example, define &lt;span class="math"&gt;\(R_{\epsilon}\)&lt;/span&gt; for which &lt;span class="math"&gt;\(L(D,h_{\epsilon})=\epsilon\)&lt;/span&gt; with &lt;span class="math"&gt;\(h_{\epsilon}\)&lt;/span&gt; the corresponding solution. Therefore any hypothesis corresponding to a radius less than &lt;span class="math"&gt;\(R_{\epsilon}\)&lt;/span&gt; leads to a generalisation error larger than &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;. The probability of sampling a point and falling in the region between &lt;span class="math"&gt;\(R_{\epsilon}\)&lt;/span&gt; and &lt;span class="math"&gt;\(R\)&lt;/span&gt; is precisely &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;. Conversely the probability of falling outside that region is &lt;span class="math"&gt;\(1-\epsilon\)&lt;/span&gt;. It is then easy to see that the probability that we need equals&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbb{P}_{S \sim D^m(x)}(L(D,h_S)&amp;gt;\epsilon)=(1-\epsilon)^m$$&lt;/div&gt;
&lt;p&gt;Using the bound &lt;span class="math"&gt;\(1-\epsilon&amp;lt;e^{-\epsilon}\)&lt;/span&gt; we can choose &lt;span class="math"&gt;\(\delta=e^{-\epsilon m}\)&lt;/span&gt;, and thus equivalently &lt;span class="math"&gt;\(\epsilon=\frac{1}{m}\ln\left(\frac{1}{\delta}\right)\)&lt;/span&gt;. Hence using equation \eqref{eq2}, we have&lt;/p&gt;
&lt;div class="math"&gt;$$L(D,h_S)\leq\frac{1}{m}\ln\left(\frac{1}{\delta}\right)$$&lt;/div&gt;
&lt;p&gt;with probability &lt;span class="math"&gt;\(1-\delta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a name="proof"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;2. Finite hypothesis classes are PAC learnable&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;
Let us assume that we have a finite hypothesis class with &lt;span class="math"&gt;\(N\)&lt;/span&gt; hypothesis, that is, &lt;span class="math"&gt;\(\mathcal{H}_N=\{h_1,\ldots,h_N\}\)&lt;/span&gt;, and that this class is realisable, meaning that it contains a &lt;span class="math"&gt;\(h^\star\)&lt;/span&gt; for which &lt;span class="math"&gt;\(L_S(h^\star)=0\;\forall S\)&lt;/span&gt;. We want to upper bound the generalisation error of a hypothesis &lt;span class="math"&gt;\(h_S\)&lt;/span&gt; obtained using empirical risk minimisation, that is, we want to find a bound of the form&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbb{P}_{x\sim D(x)}(S: L(D,h_S)&amp;gt;\epsilon)&amp;lt;\delta\tag{3}\label{eq3}$$&lt;/div&gt;
&lt;p&gt;Define &lt;span class="math"&gt;\(\mathcal{H}_B\)&lt;/span&gt; as the set of hypotheses that have generalisation error larger than &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; (it does not necessarily minimise the emprirical risk). We call this the set of bad hypotheses&lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{H}_B=\{h\in \mathcal{H}_N: L(D,h)&amp;gt;\epsilon\}$$&lt;/div&gt;
&lt;p&gt;Similarly one can define the set of misleading training sets, as those that lead to a hypothesis &lt;span class="math"&gt;\(h_S\in \mathcal{H}_B\)&lt;/span&gt; with &lt;span class="math"&gt;\(L_S(h_S)=0\)&lt;/span&gt;. That is,&lt;/p&gt;
&lt;div class="math"&gt;$$M=\{S: h\exists \mathcal{H}_B, L_S(h)=0\}$$&lt;/div&gt;
&lt;p&gt;Since we assume the class is realisable, the hypothesis &lt;span class="math"&gt;\(h_S\)&lt;/span&gt; in equation &lt;span class="math"&gt;\(\eqref{eq3}\)&lt;/span&gt; must have &lt;span class="math"&gt;\(L_S(h_S)=0\)&lt;/span&gt;, and therefore the sample data is a misleading dataset. So we need the probability of sampling a misleading dataset &lt;span class="math"&gt;\(S\in M\)&lt;/span&gt;. Using&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
M=\cup_{h\in \mathcal{H}_B} \{S: L_S(h)=0\}
\end{align}$$&lt;/div&gt;
&lt;p&gt;and the property &lt;span class="math"&gt;\(\mathbb{P}(A\cup B)&amp;lt;\mathbb{P}(A)+\mathbb{P}(B)\)&lt;/span&gt;, we have&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
\mathbb{P}(S\in M)\leq \sum_{h\in \mathcal{H}_B} \mathbb{P}(S: L_S(h)=0)
\end{align}$$&lt;/div&gt;
&lt;p&gt;Now for each &lt;span class="math"&gt;\(h\in\mathcal{H}\)&lt;/span&gt; we can put a bound on &lt;span class="math"&gt;\(\mathbb{P}(S: L_S(h)=0)\)&lt;/span&gt;. Since we want &lt;span class="math"&gt;\(L(D,h)&amp;gt;\epsilon\)&lt;/span&gt;, the probability of misclassifying a data-point is larger than &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;, and conversely a point will correctly classified with probability &lt;span class="math"&gt;\(1-\leq \epsilon\)&lt;/span&gt;. Therefore, as the solution is always overfitting and so all the points are correctly classified, we have&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbb{P}(S: L_S(h)=0)\leq (1-\epsilon)^m$$&lt;/div&gt;
&lt;p&gt;The final bound becomes&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
\mathbb{P}(S\in M)\leq \sum_{h\in \mathcal{H}_B}(1-\epsilon)^m\leq |\mathcal{H}|(1-\epsilon)^m\leq |\mathcal{H}|e^{-\epsilon m}
\end{align}$$&lt;/div&gt;
&lt;p&gt;Setting &lt;span class="math"&gt;\(\delta=\mid\mathcal{H}\mid e^{-\epsilon m}\)&lt;/span&gt;, we have with a probability of at least &lt;span class="math"&gt;\(1-\delta\)&lt;/span&gt; that&lt;/p&gt;
&lt;div class="math"&gt;$$L(D,h_S)\leq \frac{1}{m}\ln\left(\frac{\mid\mathcal{H}\mid}{\delta}\right)$$&lt;/div&gt;
&lt;p&gt;&lt;a name="agnostic"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;3. Agnostic learning&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;
In agnostic learning we do not have anymore an exact mapping between the features and the classes. Instead the classes themselves are sampled from a probability distribution given the features, that is, we have &lt;span class="math"&gt;\(P(y|x)\)&lt;/span&gt;. In the realisable example this probability is always &lt;span class="math"&gt;\(P(y|x)=0,1\)&lt;/span&gt;. Given this we extend the distribution to both the features and the classes so we have &lt;span class="math"&gt;\(D(x,y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The definition of generalisation error is slightly changed to
&lt;/p&gt;
&lt;div class="math"&gt;$$L(D,h)=\sum_{x,y}\mathbb{1}(h(x)\neq y)D(x,y)$$&lt;/div&gt;
&lt;p&gt;Because we do not have anymore the realisability condition, showing that a problem is PAC learnable is a bit more complicated. For this purpose we use one of the most useful inequalities in statistics:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Hoeffding's Inequality:&lt;/em&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbb{P}(|\bar{x}-\langle x\rangle|&amp;gt;\epsilon)\leq 2e^{-2 m\epsilon^2/(b-a)^2}$$&lt;/div&gt;
&lt;p&gt;for a random variable &lt;span class="math"&gt;\(x\)&lt;/span&gt; and any distribution. Here &lt;span class="math"&gt;\(\bar{x}\)&lt;/span&gt; is the sample mean, &lt;span class="math"&gt;\(\langle x \rangle\)&lt;/span&gt; is the distribution average and &lt;span class="math"&gt;\(a\leq x\leq b\)&lt;/span&gt;. We can apply this property to the empirical loss and the generalisation loss. Since they are quantities between zero and one (they are probabilities), we have&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbb{P}(|L_S(h)-L(D,h)|&amp;gt;\epsilon)\leq 2e^{-2 m\epsilon^2}$$&lt;/div&gt;
&lt;p&gt;We are interested in the probability of sampling a training set which gives a misleading prediction. So we want&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbb{P}_{S\sim D^m}(h\exists \mathcal{H}, |L_S(h)-L(D,h)|&amp;gt;\epsilon)\leq \sum_{h\in \mathcal{H}} \mathbb{P}_{S\sim D^m}(|L_S(h)-L(D,h)|&amp;gt;\epsilon)$$&lt;/div&gt;
&lt;p&gt;and thus using Hoeffding's inequality we have
&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbb{P}_{S\sim D^m}(h\exists \mathcal{H}, |L_S(h)-L(D,h)|&amp;gt;\epsilon)\leq \mid\mathcal{H}\mid 2e^{-2\epsilon^2m}
$$&lt;/div&gt;
&lt;p&gt;
We set &lt;span class="math"&gt;\(\delta=2\mid\mathcal{H}\mid e^{-2 m\epsilon^2}\)&lt;/span&gt;, and conclude&lt;/p&gt;
&lt;div class="math"&gt;$$|L_S(h)-L(D,h)|\leq \sqrt{\frac{1}{2m}\ln\left(\frac{2\mid\mathcal{H}\mid}{\delta}\right)},\;\forall h\in \mathcal{H}$$&lt;/div&gt;
&lt;p&gt;Say that we have &lt;span class="math"&gt;\(L(D,h)&amp;gt;L_S(h)\)&lt;/span&gt; for &lt;span class="math"&gt;\(h=h_S\)&lt;/span&gt;, the solution we obtain after minimising the empirical loss, then&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}\label{eq4}L(D,h)\leq L_S(h)+\sqrt{\frac{1}{2m}\ln\left(\frac{2\mid\mathcal{H}\mid}{\delta}\right)}\tag{4}\end{equation}&lt;/div&gt;
&lt;p&gt;This equation demonstrates clearly the trouble with overfitting. To memorise the data we need to use hypothesis classes with large dimension, so the solution has enough capacity to accommodate each data-point. This makes the second term on r.h.s of the inequality \eqref{eq4} very large, loosening the bound on the generalisation error instead of making it tighter. The fact is that we should minimise the empirical error together with that term, so we make the bound on the true error smaller. This leads us to the idea of regularisation in machine learning, whereby the empirical loss is endowed with correction terms that mitigate highly complex solutions.&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;
[1] &lt;em&gt;Understanding Machine Learning: from Theory to Algorithms&lt;/em&gt;, Shai Ben-David and Shai Shalev-Shwartz&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Machine Learning"></category><category term="data science"></category></entry></feed>