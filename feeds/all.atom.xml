<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Data Science and Machine Learning</title><link href="/" rel="alternate"></link><link href="/feeds/all.atom.xml" rel="self"></link><id>/</id><updated>2020-04-30T00:00:00+02:00</updated><entry><title>"VC dimension"</title><link href="/vc-dimension.html" rel="alternate"></link><published>2020-04-30T00:00:00+02:00</published><updated>2020-04-30T00:00:00+02:00</updated><author><name>Joao Gomes</name></author><id>tag:None,2020-04-30:/vc-dimension.html</id><summary type="html">&lt;p&gt;I explain VC dimension.&lt;/p&gt;</summary><content type="html">&lt;h3&gt;&lt;strong&gt;Table of contents&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;1.&lt;a href="#VC"&gt;VC dimension&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name="bayes"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;1. VC dimension and PAC learnability&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;
VC stands for Vapnik-Chervonenkis. The VC dimension plays the role of dimension of &lt;span class="math"&gt;\(\mathcal{H}\)&lt;/span&gt;, when &lt;span class="math"&gt;\(\mathcal{H}\)&lt;/span&gt; has infinite number of hypotheses. In the post on &lt;a href="/probably-approximately-correct-pac.html"&gt;PAC learnability&lt;/a&gt; we have shown that the circumference hypothesis is PAC learnable despite the class being infinite since each circumference is parametrised by a continuous parameter, the radius &lt;span class="math"&gt;\(R\)&lt;/span&gt;. One can find several other examples which depend on continuous parameters but they are nevertheless learnable. In this post we analyse necessary conditions for infinite classes to be PAC learnable.&lt;/p&gt;
&lt;p&gt;To do this, first we need to understand the concept of &lt;em&gt;shattering&lt;/em&gt;. Say we have  a set of hypotheses &lt;span class="math"&gt;\(\mathcal{H}=\{h_a(x)\}\)&lt;/span&gt; from a domain &lt;span class="math"&gt;\(\chi\)&lt;/span&gt; to &lt;span class="math"&gt;\(\{0,1\}\)&lt;/span&gt;. Here &lt;span class="math"&gt;\(a\)&lt;/span&gt; is(are) a continuous parameter(s). Consider a subset &lt;span class="math"&gt;\(C\subset \chi\)&lt;/span&gt; consisting of a number of points &lt;span class="math"&gt;\(C=\{c_1,c_2,\ldots,c_n\}\)&lt;/span&gt;. The restriction of a hypothesis &lt;span class="math"&gt;\(h_a(x)\in\mathcal{H}\)&lt;/span&gt; to &lt;span class="math"&gt;\(C\)&lt;/span&gt; is &lt;span class="math"&gt;\(\{h_a(c_1),h_a(c_2),\dots,h_a(c_n)\}\)&lt;/span&gt;. By dialling the continuous parameter &lt;span class="math"&gt;\(a\)&lt;/span&gt; we generate images of the restriction &lt;span class="math"&gt;\((h_a(c_1),h_a(c_2),\dots,h_a(c_n))=(1,0,1,\ldots),(0,0,1,\ldots),\ldots\)&lt;/span&gt;. Depending on the set &lt;span class="math"&gt;\(C\)&lt;/span&gt; we may or not generate all the possible images, which total to &lt;span class="math"&gt;\(2^n\)&lt;/span&gt;. When it generates all possible images we say that &lt;span class="math"&gt;\(\mathcal{H}\)&lt;/span&gt; &lt;em&gt;shatters&lt;/em&gt; &lt;span class="math"&gt;\(C\)&lt;/span&gt;. &lt;em&gt;The VC dimension is the dimension of the largest set &lt;span class="math"&gt;\(C\)&lt;/span&gt; that can be shatered.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Set of thresholds &lt;span class="math"&gt;\(h_a(x)=\mathbb{1}_{x\geq a}\)&lt;/span&gt;, which returns &lt;span class="math"&gt;\(1\)&lt;/span&gt; for &lt;span class="math"&gt;\(x\geq a\)&lt;/span&gt; and &lt;span class="math"&gt;\(0\)&lt;/span&gt; otherwise. Clearly for any &lt;span class="math"&gt;\(c_1\)&lt;/span&gt;, &lt;span class="math"&gt;\(h_a(c_1)\)&lt;/span&gt; spans &lt;span class="math"&gt;\(\{0,1\}\)&lt;/span&gt;. However, if we have an additional point &lt;span class="math"&gt;\(c_2&amp;gt;c_1\)&lt;/span&gt; then we cannot generate the image &lt;span class="math"&gt;\((h(c_1),h(c_2))=(1,0)\)&lt;/span&gt;. In fact generalising for arbitrary number of points with &lt;span class="math"&gt;\(c_1&amp;lt;c_2&amp;lt;\ldots&amp;lt;c_n\)&lt;/span&gt; we always have that if &lt;span class="math"&gt;\(h_(c_1)=1\)&lt;/span&gt; then all the reamining images are &lt;span class="math"&gt;\(h(c_2),\ldots,h(c_n)=1\)&lt;/span&gt;. Therefore the VC dimension is &lt;span class="math"&gt;\(VC_{\text{dim}}=1\)&lt;/span&gt;. Note that this the same set of hypothesis in the cirumference case &lt;a href="/probably-approximately-correct-pac.html"&gt;PAC learnability&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Machine Learning"></category><category term="data science"></category></entry><entry><title>"Bayes Optimal Classifier"</title><link href="/bayes-optimal-classifier.html" rel="alternate"></link><published>2020-04-26T00:00:00+02:00</published><updated>2020-04-26T00:00:00+02:00</updated><author><name>Joao Gomes</name></author><id>tag:None,2020-04-26:/bayes-optimal-classifier.html</id><summary type="html">&lt;p&gt;I explain what is the Bayes optimal classifier and provide a simple numerical example.&lt;/p&gt;</summary><content type="html">&lt;h3&gt;&lt;strong&gt;Table of contents&lt;/strong&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#bayes"&gt;Optimal classifier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#multiclass"&gt;Multiple classes&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a name="bayes"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span style="color:dark"&gt; &lt;strong&gt;1. Optimal classifier&lt;/strong&gt; &lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;
The Bayes optimal classifier is a binary predictor which has the lowest generalisation error. That is, for any other predictor &lt;span class="math"&gt;\(g\)&lt;/span&gt; we always have&lt;/p&gt;
&lt;div class="math"&gt;$$L(D,h_{\text{Bayes}})\leq L(D,g)$$&lt;/div&gt;
&lt;p&gt;The Bayes predictor is defined as follows:&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
h_{\text{Bayes}}=\text{argmax}_{y}P(y|x)
\end{equation}&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt;&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
L(D,g)=\sum_{x}\mathbb{1}\left[g(x)\neq y(x)\right]D(x,y)
\end{equation}&lt;/div&gt;
&lt;p&gt;Use the Bayes property &lt;span class="math"&gt;\(D(x,y)=D(y|x)D(x)\)&lt;/span&gt; and the fact that we have only two classes, say &lt;span class="math"&gt;\(y=0,1\)&lt;/span&gt;, then&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
L(D,g)=\sum_{x:g(x)=0}D(y=1|x)D(x)+\sum_{x:g(x)=1}D(y=0|x)D(x)\\
\end{equation}&lt;/div&gt;
&lt;p&gt;
Use the property that &lt;span class="math"&gt;\(a\geq \text{Min}(a,b)\)&lt;/span&gt; and write&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
L(D,g)\geq&amp;amp;&amp;amp;\sum_{x:g(x)=0}\text{Min}\big(D(y=1|x),D(y=0|x)\big)D(x)+\sum_{x:g(x)=1}\text{Min}\big(D(y=1|x),D(y=0|x)\big)D(x)\\
&amp;amp;&amp;amp;=\sum_{x}\text{Min}\big(D(y=1|x),D(y=0|x)\big)D(x)
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Note that the r.h.s is precisely the loss of the Bayes classifier. That  is,&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
L(D,h_{\text{Bayes}})&amp;amp;=&amp;amp;\sum_{x:h(x)=0}D(y=1|x)D(x)+\sum_{x:h(x)=1}D(y=0|x)D(x)\\
&amp;amp;=&amp;amp;\sum_{D(y=1|x)&amp;lt;D(y=0|x)}D(y=1|x)D(x)+\sum_{D(y=1|x)&amp;gt;D(y=0|x)}D(y=0|x)D(x)
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;a name="multiclass"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span style="color:dark"&gt; &lt;strong&gt;2. Multiple classes&lt;/strong&gt; &lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;
Can we generalise this to multi-classes? We can use &lt;span class="math"&gt;\(a\geq \text{Min}(a,b,c,\ldots)\)&lt;/span&gt; to write&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}\label{eq1}
L(D,g)\geq \sum_{x:g(x)\neq y_1}\text{Min}\big(D(y_1|x),D(y_2|x),\ldots\big)D(x)+\sum_{x:g(x)\neq y_2}\text{Min}\big(D(y_1|x),D(y_2|x),\ldots\big)D(x)+\ldots \tag{1}
\end{equation}&lt;/div&gt;
&lt;p&gt;Suppose we extend the Bayes optimal classifier to more classes by predicting the class that has higher probability. Then we have&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
L(D,h)=\sum_{x:h(x)=y_1\cup h(x)=y_2\ldots}D(y_0|x)D(x)+\sum_{x:h(x)=y_0\cup h(x)=y_2\ldots}D(y_1|x)D(x)+\ldots\\
\end{equation}&lt;/div&gt;
&lt;p&gt;Since &lt;span class="math"&gt;\(h(x)\)&lt;/span&gt; is a predictor the sets &lt;span class="math"&gt;\(S_i=\{x:h(x)=y_i\}\)&lt;/span&gt; are disjoint and so we can simplify the sums above. For example&lt;/p&gt;
&lt;div class="math"&gt;$$\sum_{x:h(x)=y_1\cup h(x)=y_2\ldots}D(y_0|x)D(x)=\sum_{x:h(x)=y_1}D(y_0|x)D(x)+\sum_{x:h(x)=y_2\ldots}D(y_0|x)D(x)+\ldots$$&lt;/div&gt;
&lt;p&gt;The issue we face now is that since we have multiple classes the maximum value does not determine uniquely the minimum value, and vice-versa, and hence we cannot apply the reasoning used in the binary case. Following similar steps as before, one can show that the multi-class Bayes classifier does not saturate the bound \eqref{eq1}. As a matter of fact there is no classifier that saturates the bound \eqref{eq1}. For that to happen we would need a classifier &lt;span class="math"&gt;\(h(x)\)&lt;/span&gt; such that when &lt;span class="math"&gt;\(h(x)=y_i\)&lt;/span&gt; we have &lt;span class="math"&gt;\(\text{Min}\big(D(y_1|x),D(y_2|x),\ldots\big)=D(y_{k\neq i}|x)\,\forall i,k\)&lt;/span&gt;. This means that for a fixed &lt;span class="math"&gt;\(i\)&lt;/span&gt; we have &lt;span class="math"&gt;\(D(y_{k\neq i}|x)=D(y_{j\neq i}|x)\, \forall k,j\neq i\)&lt;/span&gt;. It is then easy to see that this implies that &lt;span class="math"&gt;\(D(y_i|x)\)&lt;/span&gt; is a constant, independent of &lt;span class="math"&gt;\(x\)&lt;/span&gt;, contradicting our assumption.&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;Python implementation&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;seaborn&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sns&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.naive_bayes&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GaussianNB&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We compare three different hypotheses:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Optimal Bayes: &lt;span class="math"&gt;\(h_{\text{Bayes}}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Circumference hypothesis: &lt;span class="math"&gt;\(h\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Gaussian Naive Bayes: &lt;span class="math"&gt;\(h_{GNB}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;#P(y|x) definition&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;prob&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.3&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="c1"&gt;#prob of y=1&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt;

&lt;span class="c1"&gt;#coloring function&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;color&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;=&lt;/span&gt;&lt;span class="n"&gt;prob&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;blue&amp;#39;&lt;/span&gt; &lt;span class="c1"&gt;#y=1&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;red&amp;#39;&lt;/span&gt; &lt;span class="c1"&gt;#y=0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The code that defines the various hypotheses:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;h&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;=&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="c1"&gt;#if r=1 then h(x)=bayes(x)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;blue&amp;#39;&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;red&amp;#39;&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;bayes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;prob&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;blue&amp;#39;&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;red&amp;#39;&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;GNB&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;GaussianNB&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sample&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;ypred&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;ypred&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;errors&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="c1"&gt;#draw multiple samples from multivariate_normal&lt;/span&gt;
    &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multivariate_normal&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],[[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;class_y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;x&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;y&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sample&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Series&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;class_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;h_bayes&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bayes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;h&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;GNB&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;GNB&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;error_GNB&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sample&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;GNB&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;error_bayes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sample&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;h_bayes&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;error_h&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sample&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;h&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="n"&gt;errors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;error_h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;error_GNB&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;error_bayes&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;then check whether the other hypotheses have smaller error:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;errors&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note that these are the sample errors. Therefore, it is possible to find an error smaller than the Bayes error. However, if we take large samples it becomes almost improbable for that to happen.&lt;/p&gt;
&lt;p&gt;Some plots:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" height="600" src="/images/bayes_sample.png" style="display: block; margin: 0 auto" width="600"&gt;
&lt;img alt="" height="600" src="/images/optimal_bayes.png" style="display: block; margin: 0 auto" width="600"&gt;
&lt;img alt="" height="600" src="/images/optimal_bayes_GNB.png" style="display: block; margin: 0 auto" width="600"&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;
[1] &lt;em&gt;Understanding Machine Learning: from Theory to Algorithms&lt;/em&gt;, Shai Ben-David and Shai Shalev-Shwartz&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Machine Learning"></category><category term="data science"></category></entry><entry><title>"Probably Approximately Correct (PAC)"</title><link href="/probably-approximately-correct-pac.html" rel="alternate"></link><published>2020-04-14T00:00:00+02:00</published><updated>2020-04-14T00:00:00+02:00</updated><author><name>Joao Gomes</name></author><id>tag:None,2020-04-14:/probably-approximately-correct-pac.html</id><summary type="html">&lt;p&gt;In this post I explain some of the fundamentals of machine learning: PAC learnability, overfitting and generalisation bounds for classification problems. I show how these concepts work in detail for the problem of learning circumferences.&lt;/p&gt;</summary><content type="html">&lt;h3&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#pac"&gt;The learning problem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#proof"&gt;Finite hypothesis classes are PAC learnable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#agnostic"&gt;Agnostic learning&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a name="pac"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;1. The learning problem&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;
PAC stands for "probably approximately correct". In machine learning we want to find a hypothesis that is as close as possible to the ground truth. Since we only have access to a sample of the real distribution, the hypothesis that one builds is itself a function of the sample data, and therefore it is a random variable.  The problem that we want to solve is whether the sample error incurred in choosing a particular hypothesis  is approximately the same as the exact distribution error, within a certain confidence interval.&lt;/p&gt;
&lt;p&gt;Suppose we have a binary classification problem (the same applies for multi-class) with classes &lt;span class="math"&gt;\(y_i\in \{y_0,y_1\}\)&lt;/span&gt;, and we are given a training dataset &lt;span class="math"&gt;\(S\)&lt;/span&gt; with &lt;span class="math"&gt;\(m\)&lt;/span&gt; data-points. Each data-point is characterised by &lt;span class="math"&gt;\(Q\)&lt;/span&gt; features, and represented as a vector &lt;span class="math"&gt;\(q=(q_1,q_2,\ldots,q_Q)\)&lt;/span&gt;. We want to find a map &lt;span class="math"&gt;\(\mathcal{f}\)&lt;/span&gt; between these features and the corresponding class &lt;span class="math"&gt;\(y\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}\mathcal{f}: (q_1,q_2,\ldots,q_Q)\rightarrow \{y_0,y_1\}\end{equation}&lt;/div&gt;
&lt;p&gt;This map, however, does not always exist. There are problems for which we can only determine the class up to a certain confidence level. In this case we say that the learning problem is &lt;em&gt;agnostic&lt;/em&gt;, while when the map exists we say that the problem is &lt;em&gt;realisable&lt;/em&gt;. For example, image recognition is agnostic.&lt;/p&gt;
&lt;p&gt;Let us assume for the moment that such a map exists. The learner chooses a set of hypothesis &lt;span class="math"&gt;\(\mathcal{H}=\{h_1,\ldots,h_n\}\)&lt;/span&gt; and thus introduces &lt;em&gt;bias&lt;/em&gt; in the problem- a different learner may chose a different set of hypothesis. Then, in order to find the hypothesis that most accurately represents the data, the learner chooses one that has the smallest empirical risk, which is the error on the training set. That is, one tries to find the minimum of the sample loss function&lt;/p&gt;
&lt;div class="math"&gt;$$L_S(h)=\frac{1}{m}\sum_{i=1:m}\mathbb{1}\left[h(x_i)\neq y(x_i)\right],\;h\in \mathcal{H}$$&lt;/div&gt;
&lt;p&gt;with &lt;span class="math"&gt;\(\mathbb{1}(.)\)&lt;/span&gt; the Kronecker delta function. Denote the solution of this optimisation problem as &lt;span class="math"&gt;\(h_S\)&lt;/span&gt;. The true or &lt;em&gt;generalization error&lt;/em&gt; is defined instead as the unbiased average&lt;/p&gt;
&lt;div class="math"&gt;$$L(D,h)=\sum_x\mathbb{1}\left[h(x)\neq y(x)\right]D(x)$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(D(x)\)&lt;/span&gt; is a distribution, that the learner may or may not know. In the case of classification, the generalisation error is also the probability of misclassifying a point &lt;span class="math"&gt;\(L(D,h)=\mathbb{P}_{x\sim D(x)}(h(x)\neq y(x))\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If we choose appropriately &lt;span class="math"&gt;\(\mathcal{H}\)&lt;/span&gt; we may find &lt;span class="math"&gt;\(\text{min}\;L_S(h_S)=0\)&lt;/span&gt;. This can happen, for example, by memorising the data. In this case, we say that the hypothesis is &lt;em&gt;overfitting&lt;/em&gt; the data. Although memorising results in zero empirical error, the solution is not very instructive because it does not give information of how well it will perform on unseen data. The solution performs very well on the data because the learner used prior knowledge to choose an hypothesis set with sufficient capacity (or complexity) to accommodate the entire dataset. In the above minimisation problem, one should find a solution that does well (small error) on a large number of samples rather then having a very small error in a particular sample. Overfitting solutions should be avoided as they can lead to misleading conclusions. Instead, the learner should aim at obtaining a training error that is comparable to the error obtained with different samples.&lt;/p&gt;
&lt;p&gt;To make things practical, consider the problem of classifying points on a 2D plane as red or blue. The decision boundary is a circumference of radius &lt;span class="math"&gt;\(R\)&lt;/span&gt; concentric with the origin of the plane, which colours points that are inside as red and outside as blue. See figure below. The training dataset consists of &lt;span class="math"&gt;\(m\)&lt;/span&gt; data-points &lt;span class="math"&gt;\(\mathbb{x}=(x_1,x_2)\)&lt;/span&gt; sampled independently and identically distributed (i.i.d) from a distribution &lt;span class="math"&gt;\(D(x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" height="400" src="/images/PAC learning_1.png" style="display: block; margin: 0 auto" width="400"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Here the circumference &lt;span class="math"&gt;\(R\)&lt;/span&gt; denotes the ground truth which classifies points as red or blue, depending on whether they are inside or outside of the circle, respectively.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The learning problem is to find a hypothesis &lt;span class="math"&gt;\(h(x): x\rightarrow y=\{\text{blue},\text{red}\}\)&lt;/span&gt; that has small error on unseen data.   &lt;/p&gt;
&lt;p&gt;Assuming that the learner has prior knowledge of the ground truth (realisability assumption), one of the simplest algorithms is to consider the set of concentric circumferences and minimise the empirical risk. One can achieve this by drawing a decision boundary that is as close as possible to the most outward red (or inward blue data-points). This guarantees that when &lt;span class="math"&gt;\(m\rightarrow \infty\)&lt;/span&gt; we recover the exact decision boundary: the circumference &lt;span class="math"&gt;\(R\)&lt;/span&gt;.  The empirical risk minimisation problem gives the solution represented in the figure below by the circumference &lt;span class="math"&gt;\(R'\)&lt;/span&gt;. However, newly generated data-points may lie in between &lt;span class="math"&gt;\(R'\)&lt;/span&gt; and &lt;span class="math"&gt;\(R\)&lt;/span&gt;, and therefore would be misclassified.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" height="400" src="/images/circle_learning_epsilon.png" style="display: block; margin: 0 auto" width="400"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;a) The hypothesis &lt;span class="math"&gt;\(h\)&lt;/span&gt; is a circumference of radius &lt;span class="math"&gt;\(R'\)&lt;/span&gt; concentric with the origin and it is determined by the most outward red data-point. This ensures that all training set &lt;span class="math"&gt;\(S\)&lt;/span&gt; is correctly classified. b) The circumference of radius &lt;span class="math"&gt;\(R_{\epsilon}\)&lt;/span&gt; corresponds to a hypothesis &lt;span class="math"&gt;\(h_{\epsilon}\)&lt;/span&gt; that has generalization error &lt;span class="math"&gt;\(L(D,h_{\epsilon})=\epsilon\)&lt;/span&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Given that this is an overfitting solution, one has to be careful of how well it generalises. It is possible that the generalisation error is small for such a solution, but one has to be confident of how common this situation may be. If the sample that led to that solution is a rare event then we should not trust its predictions. Therefore we are interested in bounding the probability of making a bad prediction, that is,&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}\label{eq1}\mathbb{P}_{S \sim D^m(x)}(L(D,h_S)&amp;gt;\epsilon)&amp;lt;\delta \tag{1}\end{equation}&lt;/div&gt;
&lt;p&gt;Conversely, this tells us with confidence of at least &lt;span class="math"&gt;\(1-\delta\)&lt;/span&gt; that&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}\label{eq2}L(D,h_S)\leq\epsilon\tag{2}\end{equation}&lt;/div&gt;
&lt;p&gt;A &lt;em&gt;PAC learnable hypothesis&lt;/em&gt; is a hypothesis for which one can put a bound on the probability of the form \eqref{eq1} with &lt;span class="math"&gt;\(\epsilon, \delta\)&lt;/span&gt; arbitrary.&lt;/p&gt;
&lt;p&gt;In  the case of the circumference example, define &lt;span class="math"&gt;\(R_{\epsilon}\)&lt;/span&gt; for which &lt;span class="math"&gt;\(L(D,h_{\epsilon})=\epsilon\)&lt;/span&gt; with &lt;span class="math"&gt;\(h_{\epsilon}\)&lt;/span&gt; the corresponding solution. Therefore any hypothesis corresponding to a radius less than &lt;span class="math"&gt;\(R_{\epsilon}\)&lt;/span&gt; leads to a generalisation error larger than &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;. The probability of sampling a point and falling in the region between &lt;span class="math"&gt;\(R_{\epsilon}\)&lt;/span&gt; and &lt;span class="math"&gt;\(R\)&lt;/span&gt; is precisely &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;. Conversely the probability of falling outside that region is &lt;span class="math"&gt;\(1-\epsilon\)&lt;/span&gt;. It is then easy to see that the probability that we need equals&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbb{P}_{S \sim D^m(x)}(L(D,h_S)&amp;gt;\epsilon)=(1-\epsilon)^m$$&lt;/div&gt;
&lt;p&gt;Using the bound &lt;span class="math"&gt;\(1-\epsilon&amp;lt;e^{-\epsilon}\)&lt;/span&gt; we can choose &lt;span class="math"&gt;\(\delta=e^{-\epsilon m}\)&lt;/span&gt;, and thus equivalently &lt;span class="math"&gt;\(\epsilon=\frac{1}{m}\ln\left(\frac{1}{\delta}\right)\)&lt;/span&gt;. Hence using equation \eqref{eq2}, we have&lt;/p&gt;
&lt;div class="math"&gt;$$L(D,h_S)\leq\frac{1}{m}\ln\left(\frac{1}{\delta}\right)$$&lt;/div&gt;
&lt;p&gt;with probability &lt;span class="math"&gt;\(1-\delta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a name="proof"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;2. Finite hypothesis classes are PAC learnable&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;
Let us assume that we have a finite hypothesis class with &lt;span class="math"&gt;\(N\)&lt;/span&gt; hypothesis, that is, &lt;span class="math"&gt;\(\mathcal{H}_N=\{h_1,\ldots,h_N\}\)&lt;/span&gt;, and that this class is realisable, meaning that it contains a &lt;span class="math"&gt;\(h^\star\)&lt;/span&gt; for which &lt;span class="math"&gt;\(L_S(h^\star)=0\;\forall S\)&lt;/span&gt;. We want to upper bound the generalisation error of a hypothesis &lt;span class="math"&gt;\(h_S\)&lt;/span&gt; obtained using empirical risk minimisation, that is, we want to find a bound of the form&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbb{P}_{x\sim D(x)}(S: L(D,h_S)&amp;gt;\epsilon)&amp;lt;\delta\tag{3}\label{eq3}$$&lt;/div&gt;
&lt;p&gt;Define &lt;span class="math"&gt;\(\mathcal{H}_B\)&lt;/span&gt; as the set of hypotheses that have generalisation error larger than &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; (it does not necessarily minimise the emprirical risk). We call this the set of bad hypotheses&lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{H}_B=\{h\in \mathcal{H}_N: L(D,h)&amp;gt;\epsilon\}$$&lt;/div&gt;
&lt;p&gt;Similarly one can define the set of misleading training sets, as those that lead to a hypothesis &lt;span class="math"&gt;\(h_S\in \mathcal{H}_B\)&lt;/span&gt; with &lt;span class="math"&gt;\(L_S(h_S)=0\)&lt;/span&gt;. That is,&lt;/p&gt;
&lt;div class="math"&gt;$$M=\{S: h\exists \mathcal{H}_B, L_S(h)=0\}$$&lt;/div&gt;
&lt;p&gt;Since we assume the class is realisable, the hypothesis &lt;span class="math"&gt;\(h_S\)&lt;/span&gt; in equation &lt;span class="math"&gt;\(\eqref{eq3}\)&lt;/span&gt; must have &lt;span class="math"&gt;\(L_S(h_S)=0\)&lt;/span&gt;, and therefore the sample data is a misleading dataset. So we need the probability of sampling a misleading dataset &lt;span class="math"&gt;\(S\in M\)&lt;/span&gt;. Using&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
M=\cup_{h\in \mathcal{H}_B} \{S: L_S(h)=0\}
\end{align}$$&lt;/div&gt;
&lt;p&gt;and the property &lt;span class="math"&gt;\(\mathbb{P}(A\cup B)&amp;lt;\mathbb{P}(A)+\mathbb{P}(B)\)&lt;/span&gt;, we have&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
\mathbb{P}(S\in M)\leq \sum_{h\in \mathcal{H}_B} \mathbb{P}(S: L_S(h)=0)
\end{align}$$&lt;/div&gt;
&lt;p&gt;Now for each &lt;span class="math"&gt;\(h\in\mathcal{H}\)&lt;/span&gt; we can put a bound on &lt;span class="math"&gt;\(\mathbb{P}(S: L_S(h)=0)\)&lt;/span&gt;. Since we want &lt;span class="math"&gt;\(L(D,h)&amp;gt;\epsilon\)&lt;/span&gt;, the probability of misclassifying a data-point is larger than &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;, and conversely a point will correctly classified with probability &lt;span class="math"&gt;\(1-\leq \epsilon\)&lt;/span&gt;. Therefore, as the solution is always overfitting and so all the points are correctly classified, we have&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbb{P}(S: L_S(h)=0)\leq (1-\epsilon)^m$$&lt;/div&gt;
&lt;p&gt;The final bound becomes&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
\mathbb{P}(S\in M)\leq \sum_{h\in \mathcal{H}_B}(1-\epsilon)^m\leq |\mathcal{H}|(1-\epsilon)^m\leq |\mathcal{H}|e^{-\epsilon m}
\end{align}$$&lt;/div&gt;
&lt;p&gt;Setting &lt;span class="math"&gt;\(\delta=\mid\mathcal{H}\mid e^{-\epsilon m}\)&lt;/span&gt;, we have with a probability of at least &lt;span class="math"&gt;\(1-\delta\)&lt;/span&gt; that&lt;/p&gt;
&lt;div class="math"&gt;$$L(D,h_S)\leq \frac{1}{m}\ln\left(\frac{\mid\mathcal{H}\mid}{\delta}\right)$$&lt;/div&gt;
&lt;p&gt;&lt;a name="agnostic"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;3. Agnostic learning&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;
In agnostic learning we do not have anymore an exact mapping between the features and the classes. Instead the classes themselves are sampled from a probability distribution given the features, that is, we have &lt;span class="math"&gt;\(P(y|x)\)&lt;/span&gt;. In the realisable example this probability is always &lt;span class="math"&gt;\(P(y|x)=0,1\)&lt;/span&gt;. Given this we extend the distribution to both the features and the classes so we have &lt;span class="math"&gt;\(D(x,y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The definition of generalisation error is slightly changed to
&lt;/p&gt;
&lt;div class="math"&gt;$$L(D,h)=\sum_{x,y}\mathbb{1}(h(x)\neq y)D(x,y)$$&lt;/div&gt;
&lt;p&gt;Because we do not have anymore the realisability condition, showing that a problem is PAC learnable is a bit more complicated. For this purpose we use one of the most useful inequalities in statistics:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Hoeffding's Inequality:&lt;/em&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbb{P}(|\bar{x}-\langle x\rangle|&amp;gt;\epsilon)\leq 2e^{-2 m\epsilon^2/(b-a)^2}$$&lt;/div&gt;
&lt;p&gt;for a random variable &lt;span class="math"&gt;\(x\)&lt;/span&gt; and any distribution. Here &lt;span class="math"&gt;\(\bar{x}\)&lt;/span&gt; is the sample mean, &lt;span class="math"&gt;\(\langle x \rangle\)&lt;/span&gt; is the distribution average and &lt;span class="math"&gt;\(a\leq x\leq b\)&lt;/span&gt;. We can apply this property to the empirical loss and the generalisation loss. Since they are quantities between zero and one (they are probabilities), we have&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbb{P}(|L_S(h)-L(D,h)|&amp;gt;\epsilon)\leq 2e^{-2 m\epsilon^2}$$&lt;/div&gt;
&lt;p&gt;We are interested in the probability of sampling a training set which gives a misleading prediction. So we want&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbb{P}_{S\sim D^m}(h\exists \mathcal{H}, |L_S(h)-L(D,h)|&amp;gt;\epsilon)\leq \sum_{h\in \mathcal{H}} \mathbb{P}_{S\sim D^m}(|L_S(h)-L(D,h)|&amp;gt;\epsilon)$$&lt;/div&gt;
&lt;p&gt;and thus using Hoeffding's inequality we have
&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbb{P}_{S\sim D^m}(h\exists \mathcal{H}, |L_S(h)-L(D,h)|&amp;gt;\epsilon)\leq \mid\mathcal{H}\mid 2e^{-2\epsilon^2m}
$$&lt;/div&gt;
&lt;p&gt;
We set &lt;span class="math"&gt;\(\delta=2\mid\mathcal{H}\mid e^{-2 m\epsilon^2}\)&lt;/span&gt;, and conclude&lt;/p&gt;
&lt;div class="math"&gt;$$|L_S(h)-L(D,h)|\leq \sqrt{\frac{1}{2m}\ln\left(\frac{2\mid\mathcal{H}\mid}{\delta}\right)},\;\forall h\in \mathcal{H}$$&lt;/div&gt;
&lt;p&gt;Say that we have &lt;span class="math"&gt;\(L(D,h)&amp;gt;L_S(h)\)&lt;/span&gt; for &lt;span class="math"&gt;\(h=h_S\)&lt;/span&gt;, the solution we obtain after minimising the empirical loss, then&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}\label{eq4}L(D,h)\leq L_S(h)+\sqrt{\frac{1}{2m}\ln\left(\frac{2\mid\mathcal{H}\mid}{\delta}\right)}\tag{4}\end{equation}&lt;/div&gt;
&lt;p&gt;This equation demonstrates clearly the trouble with overfitting. To memorise the data we need to use hypothesis classes with large dimension, so the solution has enough capacity to accommodate each data-point. This makes the second term on r.h.s of the inequality \eqref{eq4} very large, loosening the bound on the generalisation error instead of making it tighter. The fact is that we should minimise the empirical error together with that term, so we make the bound on the true error smaller. This leads us to the idea of regularisation in machine learning, whereby the empirical loss is endowed with correction terms that mitigate highly complex solutions.&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;
[1] &lt;em&gt;Understanding Machine Learning: from Theory to Algorithms&lt;/em&gt;, Shai Ben-David and Shai Shalev-Shwartz&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Machine Learning"></category><category term="data science"></category></entry></feed>