<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Data Science and Machine Learning - Machine Learning</title><link href="/" rel="alternate"></link><link href="/feeds/machine-learning.atom.xml" rel="self"></link><id>/</id><updated>2020-05-26T00:00:00+02:00</updated><entry><title>"Curse of dimensionality"</title><link href="/curse-of-dimensionality.html" rel="alternate"></link><published>2020-05-26T00:00:00+02:00</published><updated>2020-05-26T00:00:00+02:00</updated><author><name>Joao Gomes</name></author><id>tag:None,2020-05-26:/curse-of-dimensionality.html</id><summary type="html">&lt;p&gt;We address the importance of dimensionality in machine learning.&lt;/p&gt;</summary><content type="html">&lt;h3&gt;&lt;strong&gt;Table of contents&lt;/strong&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#def1"&gt;Basic concept&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#def"&gt;Hughes phenomenon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#python"&gt;Python implementation&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a name="def1"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;1. Basic concept&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;All the machine learning models suffer from the same basic problem. If we have too many features as compared to the amount of data, then it is easier to overfit and the model will generalize poorly- this means that the model can more easily memorize the data because there are more features that can be used to differentiate the datapoints. Instead if we have too few features for the same amount of data, then it is  harder for the model to capture the relevant features and it will mostly certainly underfit. &lt;/p&gt;
&lt;p&gt;So what is the right amount of data versus number of features? A simple criterion is the following. Suppose we have a feature &lt;span class="math"&gt;\(x\)&lt;/span&gt; that can take &lt;span class="math"&gt;\(N\)&lt;/span&gt; distinct values. In a binary classification problem for example, if &lt;span class="math"&gt;\(m\)&lt;/span&gt;, the number of data-points, is very large, then we have enough points to calculate the empirical probabilities &lt;span class="math"&gt;\(P(c|x)\)&lt;/span&gt; with relative confidence, where &lt;span class="math"&gt;\(c=0,1\)&lt;/span&gt; is the class. As a classifier model we can use the set of empirical probabilites- the predictor is the class which gives higher probability. On the other hand, if &lt;span class="math"&gt;\(m\)&lt;/span&gt; is smaller than &lt;span class="math"&gt;\(N\)&lt;/span&gt; then the data is too sparse and we cannot rely on the empirical probabilities. Similarly, if we have an additional feature which can also take &lt;span class="math"&gt;\(N\)&lt;/span&gt; values, then we need &lt;span class="math"&gt;\(m\)&lt;/span&gt; to be larger than &lt;span class="math"&gt;\(N^2\)&lt;/span&gt;. In general, for &lt;span class="math"&gt;\(d\)&lt;/span&gt; dimensions we need &lt;span class="math"&gt;\(m\gg N^d\)&lt;/span&gt;. The same applies for continuous features. One can assume that &lt;span class="math"&gt;\(N=2^{64}\)&lt;/span&gt; for a 64-bit computer, and still the necessary data grows exponentially with the number of dimensions.&lt;/p&gt;
&lt;p&gt;A more detailed analysis, as explained in the following section, shows that there exists an optimal &lt;span class="math"&gt;\(N_{opt}\)&lt;/span&gt; for which the accuracy is the best possible. For &lt;span class="math"&gt;\(N&amp;gt;N_{opt}\)&lt;/span&gt; the model prediction deteriorates until it starts performing as well as an empirical model given by the relative frequencies of the classes, disregarding their features. That is, when the number of features is large, the data becomes so sparse that the best we can do is to draw the classes according to their probabilities &lt;span class="math"&gt;\(P(c=0,1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a name="def"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;2. Hughes phenomenon&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Suppose we have a binary classification problem with classes &lt;span class="math"&gt;\(c_1,c_2\)&lt;/span&gt; and a training set of &lt;span class="math"&gt;\(m\)&lt;/span&gt; samples with &lt;span class="math"&gt;\(n\)&lt;/span&gt; features &lt;span class="math"&gt;\(x_i\)&lt;/span&gt;. Intuitively having a very large dataset with only very few features, that is, &lt;span class="math"&gt;\(n\ll m\)&lt;/span&gt; may lead to difficulties to learning because there may not be enough information to correctly classify the samples. On the other hand, a small dataset as compared to a very large number of features, &lt;span class="math"&gt;\(n\gg m\)&lt;/span&gt;, means that we need a very complex hypothesis function which may lead to overfitting. What is the optimal number of features &lt;span class="math"&gt;\(n_{opt}\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;We use the Bayes optimal classifier. In this case we choose the class that has higher probability according to the rule&lt;/p&gt;
&lt;div class="math"&gt;$$\tilde{c}_i=\text{argmax}_{j=1,2}P(c_j|x)$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\tilde{c}_i\)&lt;/span&gt; is the predicted class and &lt;span class="math"&gt;\(P(c,x)\)&lt;/span&gt; is the true distribution. The accuracy of the Bayes optimal classifier is then
&lt;/p&gt;
&lt;div class="math"&gt;$$\sum_{x,c}\mathbb{1}_{c,\tilde{c}}P(c,x)=\sum_{x,\tilde{c}=\text{argmax P(c|x)}} P(\tilde{c},x)=\sum_x[\text{max}_c P(c|x)] P(x) =\sum_x [\text{max}_c P(x|c)P(c)]$$&lt;/div&gt;
&lt;p&gt;
Lets define &lt;span class="math"&gt;\(P(c_1)=p_{c_1}\)&lt;/span&gt; and &lt;span class="math"&gt;\(P(c_2)=p_{c_2}\)&lt;/span&gt;, then the Bayes accuracy has the form:
&lt;/p&gt;
&lt;div class="math"&gt;$$\sum_{x=x_1}^{x_n} \text{max}\left(P(x|c_1)p_{c_1},P(x|c_2)p_{c_2}\right)$$&lt;/div&gt;
&lt;p&gt;We ought to study the Bayes accuracy over all possible environment probabilities &lt;span class="math"&gt;\(P(x|c_1)\)&lt;/span&gt; and &lt;span class="math"&gt;\(P(x|c_2)\)&lt;/span&gt;. To do this we define&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}u_i&amp;amp;\equiv&amp;amp; P(x_i|c_1), i=1\ldots n\\ v_i&amp;amp;\equiv&amp;amp; P(x_i|c_2), i=1\ldots n\end{aligned}$$&lt;/div&gt;
&lt;p&gt;
and assume that &lt;span class="math"&gt;\(u,v\)&lt;/span&gt; are themselves random  variables. 
The measure can be calculated from
&lt;/p&gt;
&lt;div class="math"&gt;$$dP(u_1,u_2,\ldots,u_n,v_1,v_2,\ldots,v_n)=Ndu_1du_2\ldots du_{n-1}dv_1dv_2\ldots dv_{n-1}$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(N\)&lt;/span&gt; is a normalization constant. Note that because of the constraints &lt;span class="math"&gt;\(\sum_i u_i=1\)&lt;/span&gt; and &lt;span class="math"&gt;\(\sum_i v_i=1\)&lt;/span&gt;, the measure does not depend on &lt;span class="math"&gt;\(du_n\)&lt;/span&gt; and &lt;span class="math"&gt;\(dv_n\)&lt;/span&gt;. To find the normalization &lt;span class="math"&gt;\(N\)&lt;/span&gt; note that the variables &lt;span class="math"&gt;\(u_i,v_i\)&lt;/span&gt; live in the hypercube &lt;span class="math"&gt;\(0\leq u_i\leq 1\)&lt;/span&gt; and &lt;span class="math"&gt;\(0\leq v_i\leq 1\)&lt;/span&gt; and must obey the conditions &lt;span class="math"&gt;\(\sum_{i=1}^n u_i= 1\)&lt;/span&gt; and &lt;span class="math"&gt;\(\sum_{i=1}^nv_i= 1\)&lt;/span&gt;, respectively. Given this we calculate
&lt;/p&gt;
&lt;div class="math"&gt;$$1=N\int_0^1 du_1\int_{0}^{1-u_1}du_2\int_0^{1-u_1-u_2}du_3\ldots \int_0^1dv_1\int_0^{1-v_1}dv_2\int_0^{1-v_1-v_2}dv_3\ldots $$&lt;/div&gt;
&lt;p&gt;The integrals can be calculated easily to give &lt;span class="math"&gt;\(N=[(n-1)!]^2\)&lt;/span&gt;. One trick is to use the unconstrained integral &lt;span class="math"&gt;\(\prod_{i=1}^n \int_0^{\infty} dx_i e^{-\alpha x_i}\)&lt;/span&gt; and then use the change of variables &lt;span class="math"&gt;\(x_i=r u_i\)&lt;/span&gt; with &lt;span class="math"&gt;\(\sum_{i=1}^nu_i=1\)&lt;/span&gt; and integrate over &lt;span class="math"&gt;\(r\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The mean Bayes accuracy is therefore
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}&amp;amp;\int\Big(\sum_i \text{max}(u_ip_{c_1},v_ip_{c_2}) \Big)dP(u,v)= \\
 &amp;amp;=n(n-1)^2\int_0^1\int_0^1du_1dv_1(1-u_1)^{n-2}(1-v_1)^{n-2}\text{max}(u_1p_{c_1},v_1p_{c_2})\end{aligned}$$&lt;/div&gt;
&lt;p&gt;
By symmetry, the sum in the first equation splits into &lt;span class="math"&gt;\(n\)&lt;/span&gt; equal terms. The integrals over the remaining &lt;span class="math"&gt;\(u_2,\ldots u_n\)&lt;/span&gt; and &lt;span class="math"&gt;\(v_2,\ldots v_n\)&lt;/span&gt; give the term &lt;span class="math"&gt;\((1-u_1)^{n-2}(1-v_1)^{n-2}\)&lt;/span&gt; (one can use again the trick of the unconstrained integral &lt;span class="math"&gt;\(\prod_{i=1}^{n-1}\int_0^{\infty}dx_ie^{-\alpha x_i}\)&lt;/span&gt;, change variables to &lt;span class="math"&gt;\(x_i=ru_i\)&lt;/span&gt; and then use the constrain &lt;span class="math"&gt;\(\sum_{i=2}^{n}u_i=1-u_1\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;The integral above is relatively easy to calculate. However, we are mostly interested in the limit when &lt;span class="math"&gt;\(n\rightarrow \infty\)&lt;/span&gt;. To do this we change variables &lt;span class="math"&gt;\(u_1\rightarrow u_1/n\)&lt;/span&gt; and &lt;span class="math"&gt;\(v_1\rightarrow v_1/n\)&lt;/span&gt; and take &lt;span class="math"&gt;\(n\gg 1\)&lt;/span&gt;. This gives
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}&amp;amp;\sim \int_0^n\int_0^ndu_1dv_1(1-u_1/n)^{n}(1-v_1/n)^{n}\text{max}(u_1p_{c_1},v_1p_{c_2})\\
&amp;amp;\sim \int_0^{\infty}\int_0^{\infty}du_1dv_1e^{-u_1-v_1}\text{max}(u_1p_{c_1},v_1p_{c_2})\\&amp;amp;=1-p_{c_1}p_{c_2}\end{aligned}$$&lt;/div&gt;
&lt;p&gt;which means that the Bayes accuracy has a limiting value as the number of features is very large. &lt;/p&gt;
&lt;p&gt;In the case of a finite dataset, we can use the empirical distribution of &lt;span class="math"&gt;\(u_i\)&lt;/span&gt; and &lt;span class="math"&gt;\(v_i\)&lt;/span&gt;. Suppose we have &lt;span class="math"&gt;\(m_1\)&lt;/span&gt; datapoints with class &lt;span class="math"&gt;\(c_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(m_2\)&lt;/span&gt; points with class &lt;span class="math"&gt;\(c_2\)&lt;/span&gt;. Then we can estimate &lt;span class="math"&gt;\(P(x_i|c_1)\)&lt;/span&gt; by the fraction of points in class &lt;span class="math"&gt;\(c_1\)&lt;/span&gt; that have feature &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; and similarly for class &lt;span class="math"&gt;\(c_2\)&lt;/span&gt;, that is, 
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}&amp;amp;P(x_i|c_1)\simeq \frac{s_i}{m_1}\\
&amp;amp;P(x_i|c_2)\simeq \frac{r_i}{m_2}\end{aligned}$$&lt;/div&gt;
&lt;p&gt;
In turn the probabilities &lt;span class="math"&gt;\(p_{c_1}\)&lt;/span&gt; and &lt;span class="math"&gt;\(p_{c_2}\)&lt;/span&gt; are given by &lt;span class="math"&gt;\(m_1/m\)&lt;/span&gt; and &lt;span class="math"&gt;\(m_2/m\)&lt;/span&gt; respectively where &lt;span class="math"&gt;\(m\)&lt;/span&gt; is the total number of datapoints. The Bayes classification rule then consists in choosing class &lt;span class="math"&gt;\(c_1\)&lt;/span&gt; for feature &lt;span class="math"&gt;\(x_1\)&lt;/span&gt; provided &lt;span class="math"&gt;\(s_1p_{c_1}/m_1=s_1/m\)&lt;/span&gt; is larger than &lt;span class="math"&gt;\(r_1p_{c_2}/m_2=r_1/m\)&lt;/span&gt;, and class &lt;span class="math"&gt;\(c_2\)&lt;/span&gt; if it is smaller. When &lt;span class="math"&gt;\(s_1=r_1\)&lt;/span&gt; we choose class which has higher prior probability. &lt;/p&gt;
&lt;p&gt;The probability of drawing &lt;span class="math"&gt;\(s_1\)&lt;/span&gt; points in class &lt;span class="math"&gt;\(c_1\)&lt;/span&gt; with feature &lt;span class="math"&gt;\(x_1\)&lt;/span&gt;, &lt;span class="math"&gt;\(s_2\)&lt;/span&gt; points with feature &lt;span class="math"&gt;\(x_2\)&lt;/span&gt;, and so on, follows a multinomial distribution:
&lt;/p&gt;
&lt;div class="math"&gt;$$P(s_1,s_2,\ldots s_n|u_1,u_2,\ldots)=\frac{m_1!}{s_1!s_2!\ldots s_n!}u_1^{s_1}u_2^{s_2}\ldots u_n^{s_n}$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(s_1+s_2+\ldots s_n=m_1\)&lt;/span&gt;. Marginalizing over &lt;span class="math"&gt;\(s_2,\ldots s_n\)&lt;/span&gt; one obtains:
&lt;/p&gt;
&lt;div class="math"&gt;$$P(s_1|u_1)=\frac{m_1!}{s_1!(m_1-s_1)!}u_1^{s_1}(1-u_1)^{m_1-s_1}$$&lt;/div&gt;
&lt;p&gt;The mean Bayes accuracy is then:
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}&amp;amp; n\int\prod_{i=1}^{n-1}du_idv_i \sum_{s_1,r_1}\text{max}(u_1p_{c_1},v_1 p_{c_2})P(s_1|u_1)P(r_1|v_1)dP(u_1,v_1,\ldots)\\
&amp;amp;=n(n-1)^2\sum_{s_1&amp;gt;r_1}{m_1\choose s_1}{m_2\choose r_1}p_{c_1}\int du_1dv_1 u_1^{s_1+1}(1-u_1)^{m_1+n-s_1-2}v_1^{r_1}(1-v_1)^{m_2+n-r_1-2} \\
&amp;amp;+ n(n-1)^2\sum_{s_1\leq r_1}{m_1\choose s_1}{m_2\choose r_1}p_{c_2}\int du_1dv_1 u_1^{s_1}(1-u_1)^{m_1+n-s_1-2}v_1^{r_1+1}(1-v_1)^{m_2+n-r_1-2}\end{aligned}$$&lt;/div&gt;
&lt;p&gt;Using &lt;span class="math"&gt;\(\int_0^1 dx x^a (1-x)^b=a!b!/(a+b+1)!\)&lt;/span&gt; we calculate:
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}n(n-1)^2&amp;amp;\sum_{s_1&amp;gt;r_1}p_{c_1}{m_1\choose s_1}{m_2\choose r_1}\frac{(s_1+1)!(m_1+n-s_1-2)!}{(m_1+n)!}\frac{r_1!(m_2+n-r_1-2)!}{(m_2+n-1)!}\\
+n(n-1)^2&amp;amp;\sum_{s_1\leq r_1}p_{c_2}{m_1\choose s_1}{m_2\choose r_1}\frac{(r_1+1)!(m_2+n-r_1-2)!}{(m_2+n)!}\frac{s_1!(m_1+n-s_1-2)!}{(m_1+n-1)!}\end{aligned}$$&lt;/div&gt;
&lt;p&gt;With some work we can simplify the expression above:
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}n(n-1)^2&amp;amp;\sum_{s_1&amp;gt;r_1}p_{c_1}(s_1+1)\frac{(m_1+n-s_1-2)(m_1+n-s_1-2)\ldots (m_1-s_1+1)}{(m_1+n)(m_1+n-1)\ldots (m_1+1)}\times \frac{(m_2+n-r_1-2)(m_2+n-r_1-2)\ldots (m_2-r_1+1)}{(m_2+n-1)(m_2+n-2)\ldots (m_2+1)}\\
+n(n-1)^2&amp;amp;\sum_{s_1\leq r_1}p_{c_2}(s_1\leftrightarrow r_1,m_1\leftrightarrow m_2)\end{aligned}$$&lt;/div&gt;
&lt;p&gt;We can determine the limit &lt;span class="math"&gt;\(n\rightarrow \infty\)&lt;/span&gt; by using the Stirling's approximation of the factorial function:
&lt;/p&gt;
&lt;div class="math"&gt;$$n!\simeq \sqrt{2\pi n}\Big(\frac{n}{e}\Big)^{n}$$&lt;/div&gt;
&lt;p&gt;
For each &lt;span class="math"&gt;\(s_1,r_1\)&lt;/span&gt; term we calculate:
&lt;/p&gt;
&lt;div class="math"&gt;$${m_1\choose s_1}{m_2\choose r_1}\frac{(s_1+1)!(m_1+n-s_1-2)!}{(m_1+n)!}\frac{r_1!(m_2+n-r_1-2)!}{(m_2+n-1)!}\simeq (s_1+1)\frac{m_1!}{(m_1-s_1)!}\frac{m_2!}{(m_2-r_1)!}n^{-(s_1+r_1+3)}+\mathcal{O}(n^{-(s_1+r_1+4)})$$&lt;/div&gt;
&lt;p&gt;
and for the other term we interchange &lt;span class="math"&gt;\(s_1\leftrightarrow r_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(m_1\leftrightarrow m_2\)&lt;/span&gt;. Only the term with &lt;span class="math"&gt;\(s_1=r_1=0\)&lt;/span&gt; gives an order &lt;span class="math"&gt;\(\mathcal{O}(1)\)&lt;/span&gt; term and so we obtain:
&lt;/p&gt;
&lt;div class="math"&gt;$$\text{lim}_{n\rightarrow \infty}=p_{c_2}$$&lt;/div&gt;
&lt;p&gt;Below a plot of the curve for some values of &lt;span class="math"&gt;\(m=m_1+m_2\)&lt;/span&gt;:
&lt;img alt="" height="400" src="/images/p105p205.png" style="display: block; margin: 0 auto" width="400"&gt;&lt;/p&gt;
&lt;p&gt;ando also for different prior probabilities:
&lt;img alt="" height="400" src="/images/p102p208.png" style="display: block; margin: 0 auto" width="400"&gt;
We see in this case that the mean accuracy first increases then it deteriorates until it reaches a limiting value for large &lt;span class="math"&gt;\(n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a name="python"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;2. Python implementation&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;multiprocessing&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;mp&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;seaborn&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sns&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Define functions:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;term&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;s1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;r1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;frac&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;s1&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m2&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;r1&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m2&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;term&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;s1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;r1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;frac&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;s1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;r1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s1&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m2&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;term&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;s1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;r1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Respectively:
&lt;/p&gt;
&lt;div class="math"&gt;$$\text{term(m1,m2,s1,r1,n)}\equiv \frac{(m_1+n-s_1-2)(m_1+n-s_1-2)\ldots (m_1-s_1+1)}{(m_1+n-2)(m_1+n-3)\ldots (m_1+1)}\times (s_1\leftrightarrow r_1,m_1\leftrightarrow m_2)$$&lt;/div&gt;
&lt;p&gt;
and
&lt;/p&gt;
&lt;div class="math"&gt;$$\text{f(m1,m2,s1,r1,n)}\equiv \frac{n(n-1)^2(s_1+1)}{(m_1+n)(m_1+n-1)(m_2+n-1)}\text{term(m1,m2,s1,r1,n)}$$&lt;/div&gt;
&lt;p&gt;The final expression is calculated as :&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;p1&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;
&lt;span class="n"&gt;p2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;p1&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;g&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;
    &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m2&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;p1&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m2&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;p2&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note that calculating all the sums can be computationally expensive, especially for large values of &lt;span class="math"&gt;\(m_1,m_2\)&lt;/span&gt; and &lt;span class="math"&gt;\(n\)&lt;/span&gt;. We have use parallel processing to handle the calculation faster. Here is an example of how to implement this using the library &lt;em&gt;multiprocessing&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{}&lt;/span&gt;
&lt;span class="n"&gt;m_list&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;m_list&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;p1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;m2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;mp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Pool&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cpu_count&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;pool&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pool&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;,[(&lt;/span&gt;&lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;
[1] &lt;em&gt;On the mean accuracy of statistical pattern recognizers&lt;/em&gt;, Gordon F. Hughes, "Transactions on information theory", 1968&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Machine Learning"></category><category term="data science"></category></entry><entry><title>"Rademacher complexity"</title><link href="/rademacher-complexity.html" rel="alternate"></link><published>2020-05-02T00:00:00+02:00</published><updated>2020-05-02T00:00:00+02:00</updated><author><name>Joao Gomes</name></author><id>tag:None,2020-05-02:/rademacher-complexity.html</id><summary type="html">&lt;p&gt;The Rademacher complexity measures how a hypothesis correlates with noise. This gives a way to evaluate the capacity or complexity of a hypothesis class.&lt;/p&gt;</summary><content type="html">&lt;h3&gt;&lt;strong&gt;Table of contents&lt;/strong&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#def"&gt;Definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#bounds"&gt;Bounds&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a name="def"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;1. Definition&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The empirical Rademacher complexity of a hypothesis class &lt;span class="math"&gt;\(G=\{g\}\)&lt;/span&gt; is defined as an average over the training set &lt;span class="math"&gt;\(S=(z_1,\ldots,z_m)\)&lt;/span&gt; in the following way:&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{\mathcal{R}}(G)=E_{\sigma}\left[\text{sup}_{g\in G}\frac{1}{m}\sum_{i=1}^{m}\sigma_i g(z_i)\right]$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\sigma_i\)&lt;/span&gt; are &lt;span class="math"&gt;\(m\)&lt;/span&gt; independently and uniformly distributed random variables in the interval &lt;span class="math"&gt;\([-1,1]\)&lt;/span&gt;. Since &lt;span class="math"&gt;\(E(\sigma)=0\)&lt;/span&gt;, we see that the above average is the correlation between &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; and &lt;span class="math"&gt;\(g(z)\)&lt;/span&gt;. The Rademacher complexity therefore measures how well a hypothesis class correlates with noise. If a class has enough complexity, it will correlate more easily with noise and hence have higher rademacher complexity.&lt;/p&gt;
&lt;p&gt;The Rademacher complexity, rather than the empirical one, is in turn defined as the statistical average over the true distribution &lt;span class="math"&gt;\(D(z)^m\)&lt;/span&gt; on all the possible sets of size &lt;span class="math"&gt;\(m\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{R}_m(G)=E_{\sim D^m}(\hat{\mathcal{R}}(G))$$&lt;/div&gt;
&lt;p&gt;Note that this definition is explicitly dependent on &lt;span class="math"&gt;\(m\)&lt;/span&gt; because one cannot move the expectation in &lt;span class="math"&gt;\(z\)&lt;/span&gt; over to &lt;span class="math"&gt;\(g(z)\)&lt;/span&gt; inside the definition of the empirical Rademacher complexity.&lt;/p&gt;
&lt;p&gt;Example: suppose we have a linear classifier in two dimensions &lt;span class="math"&gt;\(g(x\in \mathbb{R}^2)\)&lt;/span&gt;, which is a line that classifies points as &lt;span class="math"&gt;\(\{-1,1\}\)&lt;/span&gt; depending on whether the point is above or below the line. If we have up to three points one can always choose a line that classifies all the points correctly. This is just a consequence of the fact that the VC dimension of &lt;span class="math"&gt;\(\mathbb{R}^2\)&lt;/span&gt; is three. Then the above supremum is attained by picking a classifier &lt;span class="math"&gt;\(g\)&lt;/span&gt; such that &lt;span class="math"&gt;\(\text{sup}_{g\in G} \sum_{i=1}^{m}\sigma_i g(z_i)=\sum_{i=1}^{m}|\sigma_i|\)&lt;/span&gt;, which is always possible if we have up to three points. The Rademacher complexity is simply &lt;span class="math"&gt;\(\mathcal{R}_{m\leq 3}=E_{\sigma}|\sigma|\)&lt;/span&gt;, and thus independent of &lt;span class="math"&gt;\(m\)&lt;/span&gt;. The same follows in higher dimensions. The Rademacher complexity is independent of &lt;span class="math"&gt;\(m\)&lt;/span&gt; if &lt;span class="math"&gt;\(m\)&lt;/span&gt; is less than the VC dimension. For &lt;span class="math"&gt;\(m\)&lt;/span&gt; bigger than the VC dimension we can find the following bound. &lt;/p&gt;
&lt;p&gt;&lt;a name="bounds"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;1. Bounds&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;One can determine several bounds on the Rademacher complexity. One of particular interest takes into account the growth function. Remember that the growth function &lt;span class="math"&gt;\(\Pi(m)\)&lt;/span&gt; is the maximal number of distinct ways of classifying a set of &lt;span class="math"&gt;\(m\)&lt;/span&gt; points &lt;span class="math"&gt;\(z_1,\ldots,z_m\)&lt;/span&gt; using an hypothesis class &lt;span class="math"&gt;\(\mathcal{H}\)&lt;/span&gt;. In order to calculate this bound we need the following lemma:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Massart's Lemma: let &lt;span class="math"&gt;\(A\subset \mathbb{R}^m\)&lt;/span&gt; be a finite set, and &lt;span class="math"&gt;\(r=\text{max}_{x\in A}\|x\|_2\)&lt;/span&gt;, then&lt;/em&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$E_{\sigma}\left[\frac{1}{m}\text{sup}_{x\in A}\sum_{i=1}^{m} \sigma_ix_i\right]\leq \frac{r\sqrt{2\ln|A|}}{m}$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\sigma_i\)&lt;/span&gt; are independent and uniformly distributed random variables in the interval &lt;span class="math"&gt;\([-1,1]\)&lt;/span&gt;. The proof goes by first using Jensen's inequality:&lt;/p&gt;
&lt;div class="math"&gt;$$\exp(t E_{\sigma}[\text{sup}_{x\in A}\sum_{i=1}^{m} \sigma_i x_i])\leq E_{\sigma}\exp(t\text{sup}_{x\in A}\sum_{i=1}^m \sigma_i x_i)\label{eq1}\tag{1}$$&lt;/div&gt;
&lt;p&gt;
Now since the exponential function is monotically increasing we have that:&lt;/p&gt;
&lt;div class="math"&gt;$$E_{\sigma}\exp(t\text{sup}_{x\in A}\sum_{i=1}^m \sigma_i x_i)=E_{\sigma}\text{sup}_{x\in A}\exp(t\sum_{i=1}^m \sigma_i x_i)\leq \sum_{x\in A} E_{\sigma}\exp(t\sum_{i=1}^m \sigma_i x_i)$$&lt;/div&gt;
&lt;p&gt;Next we use the inequality nr. 2 from Hoeffding's inequality post which states that for a random variable &lt;span class="math"&gt;\(w\in [a,b]\)&lt;/span&gt; with &lt;span class="math"&gt;\(E(w)=0\)&lt;/span&gt; we have:
&lt;/p&gt;
&lt;div class="math"&gt;$$E_w\exp(tw)\leq \exp(t^2(b-a)^2/8)$$&lt;/div&gt;
&lt;p&gt;This means that:
&lt;/p&gt;
&lt;div class="math"&gt;$$\sum_{x\in A} E_{\sigma}\exp(t\sum_{i=1}^m \sigma_i x_i)=\sum_{x\in A}\prod_iE_{\sigma_i}\exp(t \sigma_i x_i)\leq \sum_{x\in A} \exp(t^2x_i^2/2)\leq |A| \exp(t^2 r^2/2)$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(|A|\)&lt;/span&gt; is the "size" of the set &lt;span class="math"&gt;\(A\)&lt;/span&gt; and &lt;span class="math"&gt;\(r^2=\text{max}_{x\in A}\|x\|_2\)&lt;/span&gt;. Using this result in eq.\eqref{eq1} and taking the log on both sides of the inequality:
&lt;/p&gt;
&lt;div class="math"&gt;$$E_{\sigma}[\text{sup}_{x\in A}\sum_{i=1}^{m} \sigma_i x_i]\leq \frac{\ln|A|}{t}+\frac{r^2}{2}t$$&lt;/div&gt;
&lt;p&gt;. 
The optimal bound corresponds to &lt;span class="math"&gt;\(t=\sqrt{2\ln|A|/r^2}\)&lt;/span&gt;, which is the value where the function on the right side obtains its minimum. The final result is:
&lt;/p&gt;
&lt;div class="math"&gt;$$E_{\sigma}[\text{sup}_{x\in A}\sum_{i=1}^{m} \sigma_i x_i]\leq r\sqrt{2\ln |A|}$$&lt;/div&gt;
&lt;p&gt;We can apply this result to determine a bound on the Rademacher complexity for hypothesis classes with target &lt;span class="math"&gt;\(\{-1,1\}\)&lt;/span&gt;. So we have&lt;/p&gt;
&lt;div class="math"&gt;$$E_{D^m(z)}E_{\sigma}\left[\text{sup}_{g\in G}\frac{1}{m}\sum_{i=1}^{m}\sigma_i g(z_i)\right]\leq E_{D^m(z)}\frac{r}{m}\sqrt{2\ln |A|}$$&lt;/div&gt;
&lt;p&gt;
We can easily calculate &lt;span class="math"&gt;\(r^2=\sum_i^mx_i^2=m\)&lt;/span&gt; and thus &lt;span class="math"&gt;\(r=\sqrt{m}\)&lt;/span&gt;. Moreover we know that, by definition, &lt;span class="math"&gt;\(|A|\leq \Pi(m)\)&lt;/span&gt;, the growth function, and hence we find:&lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{R}_m\leq \sqrt{\frac{2\ln \Pi(m)}{m}}$$&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Machine Learning"></category><category term="data science"></category></entry><entry><title>"Hyperplanes and classification"</title><link href="/hyperplanes-and-classification.html" rel="alternate"></link><published>2020-05-01T00:00:00+02:00</published><updated>2020-05-01T00:00:00+02:00</updated><author><name>Joao Gomes</name></author><id>tag:None,2020-05-01:/hyperplanes-and-classification.html</id><summary type="html">&lt;p&gt;We study the &lt;span class="math"&gt;\(\{0,1\}\)&lt;/span&gt; classification problem in &lt;span class="math"&gt;\(\mathbb{R}^d\)&lt;/span&gt; using hyperplanes. We show that the VC dimension is &lt;span class="math"&gt;\(d+1\)&lt;/span&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;hr&gt;
&lt;h3&gt;&lt;strong&gt;1. Hyperplanes&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Consider a set of &lt;span class="math"&gt;\(d+1\)&lt;/span&gt; points in &lt;span class="math"&gt;\(\mathbb{R}^{d}\)&lt;/span&gt; dimensions and assume that no set of three points is collinear- this way any set of &lt;span class="math"&gt;\(d\)&lt;/span&gt; points forms a hyperplane. Firstly, we shall demonstrate that if a set of &lt;span class="math"&gt;\(d\)&lt;/span&gt; points is shattered in &lt;span class="math"&gt;\(\mathbb{R}^{d-1}\)&lt;/span&gt; dimensions then &lt;span class="math"&gt;\(d+1\)&lt;/span&gt; points are also shattered in &lt;span class="math"&gt;\(\mathbb{R}^d\)&lt;/span&gt;. We can use this to reduce the problem to two dimensions, where we have seen that &lt;span class="math"&gt;\(VC_{\text{dim}}=3\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Consider the representation in the picture below. Choose &lt;span class="math"&gt;\(d\)&lt;/span&gt; points and take the hyperplane formed by these. If the remaining point belongs to the hyperplane then we can consider the projection to &lt;span class="math"&gt;\(d-1\)&lt;/span&gt; dimensions, and we are left with the case of &lt;span class="math"&gt;\((d-1)+2\)&lt;/span&gt; points in &lt;span class="math"&gt;\(\mathbb{R}^{d-1}\)&lt;/span&gt;, which we shall analyse later. If this is not the case, then we can show that if the &lt;span class="math"&gt;\(d\)&lt;/span&gt; points on the hyperplane are separable then we can always find a hyperplane in &lt;span class="math"&gt;\(\mathbb{R}^d\)&lt;/span&gt; that separates all the points. In the figure below the dashed line on &lt;span class="math"&gt;\(H_d\)&lt;/span&gt; represents the hyperplane in &lt;span class="math"&gt;\(\mathbb{R}^{d-1}\)&lt;/span&gt; that separates the set of &lt;span class="math"&gt;\(d\)&lt;/span&gt; points. It is easy to see that any hyperplane that contains the remaining point and the dashed line (hyperplane in one lower dimension) is the solution to this problem.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" height="300" src="/images/hyperplanes_dplus1.png" style="display: block; margin: 0 auto" width="300"&gt;&lt;/p&gt;
&lt;p&gt;We shall consider now the case of &lt;span class="math"&gt;\(d+2\)&lt;/span&gt; points in &lt;span class="math"&gt;\(\mathbb{R}^d\)&lt;/span&gt;. For this purpose we shall use Radon's theorem that states that any set of &lt;span class="math"&gt;\(d+2\)&lt;/span&gt; points in &lt;span class="math"&gt;\(\mathbb{R}^d\)&lt;/span&gt; can be partitioned in two sets &lt;span class="math"&gt;\(X_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(X_2\)&lt;/span&gt; such that the corresponding convex hulls intersect. This implies that &lt;span class="math"&gt;\(d+2\)&lt;/span&gt; points in &lt;span class="math"&gt;\(\mathbb{R}^d\)&lt;/span&gt; cannot be shatered because if they were then we would have two non intersecting convex hulls separated by a plane, thus contradicting Radon's theorem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For &lt;span class="math"&gt;\(d+2\)&lt;/span&gt; points &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; in &lt;span class="math"&gt;\(\mathbb{R}^d\)&lt;/span&gt; one can always choose &lt;span class="math"&gt;\(d+2\)&lt;/span&gt; parameters &lt;span class="math"&gt;\(\alpha_i\)&lt;/span&gt; such that:
&lt;/p&gt;
&lt;div class="math"&gt;$$\sum_{i=1}^{d+2}\alpha_ix_i=0,\;\; \sum_{i=1}^{d+2}\alpha_i=0$$&lt;/div&gt;
&lt;p&gt;
The reason is because one has &lt;span class="math"&gt;\(d+2\)&lt;/span&gt; unknowns (&lt;span class="math"&gt;\(\alpha_i\)&lt;/span&gt;) for &lt;span class="math"&gt;\(d+1\)&lt;/span&gt; equations (&lt;span class="math"&gt;\(d\)&lt;/span&gt; coming from the first vector equation and an additional from the constraint on &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;). The second equation can be rewritten as a sum over positive &lt;span class="math"&gt;\(\alpha_{&amp;gt;}\)&lt;/span&gt; and negative &lt;span class="math"&gt;\(\alpha_{&amp;lt;}\)&lt;/span&gt;, that is, &lt;span class="math"&gt;\(\sum_{i}\alpha_i^{&amp;gt;}=\sum_{i}\alpha_i^{&amp;lt;}\)&lt;/span&gt;. Define &lt;span class="math"&gt;\(\alpha=\sum_i\alpha_i^{&amp;gt;}\)&lt;/span&gt;, then we have 
&lt;/p&gt;
&lt;div class="math"&gt;$$\sum_i\frac{\alpha_i^{&amp;gt;}}{\alpha}=\sum_i\frac{\alpha_i^{&amp;lt;}}{\alpha}$$&lt;/div&gt;
&lt;p&gt;
which is a sum over numbers in the interval &lt;span class="math"&gt;\((0,1]\)&lt;/span&gt;. The vector equation separates into two terms
&lt;/p&gt;
&lt;div class="math"&gt;$$\sum_{i}\frac{\alpha_i^{&amp;gt;}}{\alpha}x_i=\sum_i\frac{\alpha_i^{&amp;lt;}}{\alpha}x_i$$&lt;/div&gt;
&lt;p&gt;
and each of the sets &lt;span class="math"&gt;\(X_1=\{x_i: \alpha_i^{&amp;gt;}\neq 0\}\)&lt;/span&gt; and &lt;span class="math"&gt;\(X_2=\{x_i: \alpha_i^{&amp;lt;}\neq 0\}\)&lt;/span&gt; form convex hulls. This means that &lt;span class="math"&gt;\(X_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(X_2\)&lt;/span&gt; intersect.&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;[1] &lt;em&gt;Foundations of machine learning&lt;/em&gt;, M. Mohri, A. Rostamizadeh, A. Talwalkar&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Machine Learning"></category><category term="data science"></category></entry><entry><title>"VC dimension"</title><link href="/vc-dimension.html" rel="alternate"></link><published>2020-04-30T00:00:00+02:00</published><updated>2020-04-30T00:00:00+02:00</updated><author><name>Joao Gomes</name></author><id>tag:None,2020-04-30:/vc-dimension.html</id><summary type="html">&lt;p&gt;The VC dimension is a very important concept in machine learning theory. It gives a measure of complexity based on combinatorial aspects. This concept is used to show how certain infinite hypothesis classes are PAC-learnable. Some of the main concepts are explained: growth function and shattering. I give examples and show how the VC dimension is used to bound the generalisation error.&lt;/p&gt;</summary><content type="html">&lt;h3&gt;&lt;strong&gt;Table of contents&lt;/strong&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#VC"&gt;VC dimension&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#growth"&gt;Growth function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#genbounds"&gt;Generalisation bounds&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a name="VC"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;1. VC dimension&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;
VC stands for Vapnik-Chervonenkis. The VC dimension plays the role of dimension of &lt;span class="math"&gt;\(\mathcal{H}\)&lt;/span&gt;, when &lt;span class="math"&gt;\(\mathcal{H}\)&lt;/span&gt; has infinite number of hypotheses. In the post on &lt;a href="/probably-approximately-correct-pac.html"&gt;PAC learnability&lt;/a&gt; we have shown that the circumference hypothesis is PAC learnable despite the class being infinite since each circumference is parametrised by a continuous parameter, the radius &lt;span class="math"&gt;\(R\)&lt;/span&gt;. One can find several other examples which depend on continuous parameters but they are nevertheless learnable. In this post we analyse necessary conditions for infinite classes to be PAC learnable.&lt;/p&gt;
&lt;p&gt;To do this, first we need to understand the concept of &lt;em&gt;shattering&lt;/em&gt;. Say we have  a set of hypotheses &lt;span class="math"&gt;\(\mathcal{H}=\{h_a(x)\}\)&lt;/span&gt; from a domain &lt;span class="math"&gt;\(\chi\)&lt;/span&gt; to &lt;span class="math"&gt;\(\{0,1\}\)&lt;/span&gt;. Here &lt;span class="math"&gt;\(a\)&lt;/span&gt; is(are) a continuous parameter(s). Consider a subset &lt;span class="math"&gt;\(C\subset \chi\)&lt;/span&gt; consisting of a number of points &lt;span class="math"&gt;\(C=\{c_1,c_2,\ldots,c_n\}\)&lt;/span&gt;. The restriction of a hypothesis &lt;span class="math"&gt;\(h_a(x)\in\mathcal{H}\)&lt;/span&gt; to &lt;span class="math"&gt;\(C\)&lt;/span&gt; is &lt;span class="math"&gt;\(\{h_a(c_1),h_a(c_2),\dots,h_a(c_n)\}\)&lt;/span&gt;. By dialling the continuous parameter &lt;span class="math"&gt;\(a\)&lt;/span&gt; we generate images of the restriction &lt;span class="math"&gt;\((h_a(c_1),h_a(c_2),\dots,h_a(c_n))=(1,0,1,\ldots),(0,0,1,\ldots),\ldots\)&lt;/span&gt;. Depending on the set &lt;span class="math"&gt;\(C\)&lt;/span&gt; we may or not generate all the possible images, which total to &lt;span class="math"&gt;\(2^n\)&lt;/span&gt;. When it generates all possible images we say that &lt;span class="math"&gt;\(\mathcal{H}\)&lt;/span&gt; &lt;em&gt;shatters&lt;/em&gt; &lt;span class="math"&gt;\(C\)&lt;/span&gt;. &lt;em&gt;The VC dimension is the dimension of the largest set &lt;span class="math"&gt;\(C\)&lt;/span&gt; that can be shattered.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Set of thresholds &lt;span class="math"&gt;\(h_a(x)=\mathbb{1}_{x\geq a}\)&lt;/span&gt;, which returns &lt;span class="math"&gt;\(1\)&lt;/span&gt; for &lt;span class="math"&gt;\(x\geq a\)&lt;/span&gt; and &lt;span class="math"&gt;\(0\)&lt;/span&gt; otherwise. Clearly for any &lt;span class="math"&gt;\(c_1\)&lt;/span&gt;, &lt;span class="math"&gt;\(h_a(c_1)\)&lt;/span&gt; spans &lt;span class="math"&gt;\(\{0,1\}\)&lt;/span&gt;. However, if we have an additional point &lt;span class="math"&gt;\(c_2&amp;gt;c_1\)&lt;/span&gt; then we cannot generate the image &lt;span class="math"&gt;\((h(c_1),h(c_2))=(1,0)\)&lt;/span&gt;. In fact generalising for arbitrary number of points with &lt;span class="math"&gt;\(c_1&amp;lt;c_2&amp;lt;\ldots&amp;lt;c_n\)&lt;/span&gt; we always have that if &lt;span class="math"&gt;\(h_(c_1)=1\)&lt;/span&gt; then all the reamining images are &lt;span class="math"&gt;\(h(c_2),\ldots,h(c_n)=1\)&lt;/span&gt;. Therefore the VC dimension is &lt;span class="math"&gt;\(VC_{\text{dim}}=1\)&lt;/span&gt;. Note that this the same set of hypothesis in the cirumference case &lt;a href="/probably-approximately-correct-pac.html"&gt;PAC learnability&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set of intervals &lt;span class="math"&gt;\(h_{a,b}(x)=\mathbb{1}_{a\leq x\leq b}\)&lt;/span&gt;, which returns one for a point inside the interval &lt;span class="math"&gt;\([a,b]\)&lt;/span&gt; and zero otherwise. Clearly &lt;span class="math"&gt;\(h_{a,b}\)&lt;/span&gt; shatters a single point. We can easily see that two points can also be shattered. However, a set with three points cannot be shattered. In the case we have &lt;span class="math"&gt;\(h_{a,b}(c_1)=1\)&lt;/span&gt; and &lt;span class="math"&gt;\(h_{a,b}(c_2)=0\)&lt;/span&gt; with &lt;span class="math"&gt;\(c_2&amp;gt;c_1\)&lt;/span&gt; a third point &lt;span class="math"&gt;\(c_3&amp;gt;c_2\)&lt;/span&gt; cannot have &lt;span class="math"&gt;\(h_{a,b}(c_3)=1\)&lt;/span&gt;. Therefore the VC dimension is &lt;span class="math"&gt;\(VC_{\text{dim}}=2\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set of hyperplanes in &lt;span class="math"&gt;\(\mathbb{R}^2\)&lt;/span&gt;. The hyperplane divides the space in two regions. A point falling on one side will have class zero, while if it falls on the other, will have class one. The same hyperplane can give rise to two different hypotheses by interchanging the labels between the sides. It is easy to see that one and two point set can be shattered. Consider now a three-point set. If they are collinear then there's always two combinations &lt;span class="math"&gt;\((1,0,1)\)&lt;/span&gt; and &lt;span class="math"&gt;\((0,1,0)\)&lt;/span&gt; that cannot be shattered. If they are not collinear, then the dichotomies with two ones and one zero, like &lt;span class="math"&gt;\((1,1,0)\)&lt;/span&gt;, and two zeros and one one, such as &lt;span class="math"&gt;\((0,0,1)\)&lt;/span&gt; can be generated. The remaining dichotomies &lt;span class="math"&gt;\((0,0,0)\)&lt;/span&gt; and &lt;span class="math"&gt;\((1,1,1)\)&lt;/span&gt; are generated by interchanging the sides. Therefore the set of three non-collinear points can be shattered. Consider now a set of four points and assume that three are non-collinear (if they are collinear then we fall back in the previous situation). The dichotomies depicted in the figure below (&lt;a href="#dichotomies"&gt;Fig.1&lt;/a&gt;) show two examples that cannot be generated. Thus showing that there is no four-point set that can be shattered. The VC dimension is therefore &lt;span class="math"&gt;\(VC_{\text{dim}}=3\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Hyperplanes in &lt;span class="math"&gt;\(\mathbb{R}^d\)&lt;/span&gt;. One can show that the VC dimension is &lt;span class="math"&gt;\(VC_{\text{dim}}=d+1\)&lt;/span&gt;. The demonstration can be found in the post &lt;a href="/hyperplanes-and-classification.html"&gt;Hyperplanes and classification&lt;/a&gt;. This will be very useful when studying support-vector-machines.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a name="dichotomies"&gt;&lt;/a&gt;
&lt;img alt="dichotomies" height="400" src="/images/hyperplane_dichotomies.png" style="display: block; margin: 0 auto" width="400"&gt;
  &lt;em&gt;Fig.1 Dichotomies that cannot be realised. a) The fourth point is in the interior of the triangle. b) The set forms a convex four-polygon.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The VC dimension provides a measure of how complex a hypothesis class can be. If the class is increasingly complex it allows for larger sets to be shattered. This measure is purely combinatorial and does not rely on which distribution the points are sampled from.  &lt;/p&gt;
&lt;p&gt;&lt;a name="growth"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;2. The growth function&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;
The growth function counts how many ways we can classify a set of fixed size using a hypothesis class. The proper definition is&lt;/p&gt;
&lt;div class="math"&gt;$$\Pi(m)=\text{max}_{\substack{x_1,\ldots,x_m \subseteq X}}|(h(x_1),\ldots,h(x_m)),h:\mathcal{H}|$$&lt;/div&gt;
&lt;p&gt;When the set &lt;span class="math"&gt;\(x_1,\ldots,x_m\)&lt;/span&gt; is shattered by &lt;span class="math"&gt;\(\mathcal{H}\)&lt;/span&gt; one has &lt;span class="math"&gt;\(\Pi(m)=2^m\)&lt;/span&gt;. If in addition this is the largest shattered set, then &lt;span class="math"&gt;\(\Pi(m)=2^{VC_{\text{dim}}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;One of the most important aspects of the growth function is that for &lt;span class="math"&gt;\(m&amp;gt;VC_{\text{dim}}\)&lt;/span&gt;, &lt;span class="math"&gt;\(\Pi(m)\)&lt;/span&gt; always has polynomial growth rather than exponential. This is demonstrated using the following statement:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sauer's Lemma:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Let &lt;span class="math"&gt;\(VC_{\text{dim}}=d\)&lt;/span&gt;. Then for all &lt;span class="math"&gt;\(m\)&lt;/span&gt;&lt;/em&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$ \Pi(m)\leq \sum_{i=0}^{d}\left(\begin{array}{c}m \\ i\end{array}\right)$$&lt;/div&gt;
&lt;p&gt;For &lt;span class="math"&gt;\(t\leq m\)&lt;/span&gt; we have&lt;/p&gt;
&lt;div class="math"&gt;$$\sum_{i=0}^{d}\left(\begin{array}{c}m \\ i\end{array}\right)\leq \sum_{i=0}^{m}\left(\begin{array}{c}m \\ i\end{array}\right)\left(\frac{m}{t}\right)^{d-i}=\left(\frac{m}{t}\right)^d\left(1+\frac{t}{m}\right)^m$$&lt;/div&gt;
&lt;p&gt;Using that &lt;span class="math"&gt;\(1+x\leq e^x, \forall x\)&lt;/span&gt;, we get
&lt;/p&gt;
&lt;div class="math"&gt;$$\left(\frac{m}{t}\right)^d\left(1+\frac{t}{m}\right)^m\leq \left(\frac{m}{t}\right)^d e^t$$&lt;/div&gt;
&lt;p&gt;Now we can set &lt;span class="math"&gt;\(t=d\)&lt;/span&gt; for which the bound becomes optimal, that is, &lt;span class="math"&gt;\(t^{-d} e^t\geq d^{-d}e^d\)&lt;/span&gt; (we can do this by finding the minimum of &lt;span class="math"&gt;\(t-d\ln(t)\)&lt;/span&gt;). Hence we obtain&lt;/p&gt;
&lt;div class="math"&gt;$$\Pi(m)\leq \left(\frac{m}{d}\right)^d e^d$$&lt;/div&gt;
&lt;p&gt;
&lt;a name="genbounds"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;3. The generalisation bound for infinite classes&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;
The Vapnik-Chervonenkis theorem (1971) states that, for any &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;,&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbb{P}(\text{sup}_{h\in \mathcal{H}}|L_S(h)-L_D(h)|&amp;gt;\epsilon)\leq 8\Pi(m)e^{-m\epsilon^2/32} \label{eq3}\tag{3}$$&lt;/div&gt;
&lt;p&gt;We can now understand the importance of the VC dimension. We have learnt that if VC dimension is finite than the growth function &lt;span class="math"&gt;\(\Pi(m)\)&lt;/span&gt; grows polynomially for &lt;span class="math"&gt;\(m&amp;gt;VC_{\text{dim}}\)&lt;/span&gt;. This implies from the inequality \eqref{eq3} that&lt;/p&gt;
&lt;div class="math"&gt;$$m\rightarrow \infty,\;|L_S(h)-L_D(h)|\rightarrow 0,\;\text{in propability}$$&lt;/div&gt;
&lt;p&gt;which means that we can find arbitrary &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; and &lt;span class="math"&gt;\(\delta\)&lt;/span&gt; such that for &lt;span class="math"&gt;\(m\geq m_{\mathcal{H}}\)&lt;/span&gt;, the sample complexity, the problem is PAC learnable.&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;
[1] &lt;em&gt;Understanding Machine Learning: from Theory to Algorithms&lt;/em&gt;, Shai Ben-David and Shai Shalev-Shwartz&lt;/p&gt;
&lt;p&gt;[2] &lt;em&gt;A probabilistic theory of pattern recognition&lt;/em&gt;, Luc Devroye, Laszlo Gyorfi, Gabor Lugosi&lt;/p&gt;
&lt;p&gt;[3] &lt;em&gt;Foundations of machine learning&lt;/em&gt;, M. Mohri, A. Rostamizadeh, A. Talwalkar&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Machine Learning"></category><category term="data science"></category></entry><entry><title>"Bayes Optimal Classifier"</title><link href="/bayes-optimal-classifier.html" rel="alternate"></link><published>2020-04-26T00:00:00+02:00</published><updated>2020-04-26T00:00:00+02:00</updated><author><name>Joao Gomes</name></author><id>tag:None,2020-04-26:/bayes-optimal-classifier.html</id><summary type="html">&lt;p&gt;I explain the Bayes optimal classifier and provide some numerical examples.&lt;/p&gt;</summary><content type="html">&lt;h3&gt;&lt;strong&gt;Table of contents&lt;/strong&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#bayes"&gt;Optimal classifier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#multiclass"&gt;Multiple classes&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a name="bayes"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span style="color:dark"&gt; &lt;strong&gt;1. Optimal classifier&lt;/strong&gt; &lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;
The Bayes optimal classifier is a binary predictor which has the lowest generalisation error. That is, for any other predictor &lt;span class="math"&gt;\(g\)&lt;/span&gt; we always have&lt;/p&gt;
&lt;div class="math"&gt;$$L(D,h_{\text{Bayes}})\leq L(D,g)$$&lt;/div&gt;
&lt;p&gt;The Bayes predictor is defined as follows:&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
h_{\text{Bayes}}=\text{argmax}_{y}P(y|x)
\end{equation}&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt;&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
L(D,g)=\sum_{x}\mathbb{1}\left[g(x)\neq y(x)\right]D(x,y)
\end{equation}&lt;/div&gt;
&lt;p&gt;Use the Bayes property &lt;span class="math"&gt;\(D(x,y)=D(y|x)D(x)\)&lt;/span&gt; and the fact that we have only two classes, say &lt;span class="math"&gt;\(y=0,1\)&lt;/span&gt;, then&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
L(D,g)=\sum_{x:g(x)=0}D(y=1|x)D(x)+\sum_{x:g(x)=1}D(y=0|x)D(x)\\
\end{equation}&lt;/div&gt;
&lt;p&gt;
Use the property that &lt;span class="math"&gt;\(a\geq \text{Min}(a,b)\)&lt;/span&gt; and write&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
L(D,g)\geq&amp;amp;&amp;amp;\sum_{x:g(x)=0}\text{Min}\big(D(y=1|x),D(y=0|x)\big)D(x)+\sum_{x:g(x)=1}\text{Min}\big(D(y=1|x),D(y=0|x)\big)D(x)\\
&amp;amp;&amp;amp;=\sum_{x}\text{Min}\big(D(y=1|x),D(y=0|x)\big)D(x)
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Note that the r.h.s is precisely the loss of the Bayes classifier. That  is,&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
L(D,h_{\text{Bayes}})&amp;amp;=&amp;amp;\sum_{x:h(x)=0}D(y=1|x)D(x)+\sum_{x:h(x)=1}D(y=0|x)D(x)\\
&amp;amp;=&amp;amp;\sum_{D(y=1|x)&amp;lt;D(y=0|x)}D(y=1|x)D(x)+\sum_{D(y=1|x)&amp;gt;D(y=0|x)}D(y=0|x)D(x)
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;a name="multiclass"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span style="color:dark"&gt; &lt;strong&gt;2. Multiple classes&lt;/strong&gt; &lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;
Can we generalise this to multi-classes? We can use &lt;span class="math"&gt;\(a\geq \text{Min}(a,b,c,\ldots)\)&lt;/span&gt; to write&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}\label{eq1}
L(D,g)\geq \sum_{x:g(x)\neq y_1}\text{Min}\big(D(y_1|x),D(y_2|x),\ldots\big)D(x)+\sum_{x:g(x)\neq y_2}\text{Min}\big(D(y_1|x),D(y_2|x),\ldots\big)D(x)+\ldots \tag{1}
\end{equation}&lt;/div&gt;
&lt;p&gt;Suppose we extend the Bayes optimal classifier to more classes by predicting the class that has higher probability. Then we have&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
L(D,h)=\sum_{x:h(x)=y_1\cup h(x)=y_2\ldots}D(y_0|x)D(x)+\sum_{x:h(x)=y_0\cup h(x)=y_2\ldots}D(y_1|x)D(x)+\ldots\\
\end{equation}&lt;/div&gt;
&lt;p&gt;Since &lt;span class="math"&gt;\(h(x)\)&lt;/span&gt; is a predictor the sets &lt;span class="math"&gt;\(S_i=\{x:h(x)=y_i\}\)&lt;/span&gt; are disjoint and so we can simplify the sums above. For example&lt;/p&gt;
&lt;div class="math"&gt;$$\sum_{x:h(x)=y_1\cup h(x)=y_2\ldots}D(y_0|x)D(x)=\sum_{x:h(x)=y_1}D(y_0|x)D(x)+\sum_{x:h(x)=y_2\ldots}D(y_0|x)D(x)+\ldots$$&lt;/div&gt;
&lt;p&gt;The issue we face now is that since we have multiple classes the maximum value does not determine uniquely the minimum value, and vice-versa, and hence we cannot apply the reasoning used in the binary case. Following similar steps as before, one can show that the multi-class Bayes classifier does not saturate the bound \eqref{eq1}. As a matter of fact there is no classifier that saturates the bound \eqref{eq1}. For that to happen we would need a classifier &lt;span class="math"&gt;\(h(x)\)&lt;/span&gt; such that when &lt;span class="math"&gt;\(h(x)=y_i\)&lt;/span&gt; we have &lt;span class="math"&gt;\(\text{Min}\big(D(y_1|x),D(y_2|x),\ldots\big)=D(y_{k\neq i}|x)\,\forall i,k\)&lt;/span&gt;. This means that for a fixed &lt;span class="math"&gt;\(i\)&lt;/span&gt; we have &lt;span class="math"&gt;\(D(y_{k\neq i}|x)=D(y_{j\neq i}|x)\, \forall k,j\neq i\)&lt;/span&gt;. It is then easy to see that this implies that &lt;span class="math"&gt;\(D(y_i|x)\)&lt;/span&gt; is a constant, independent of &lt;span class="math"&gt;\(x\)&lt;/span&gt;, contradicting our assumption.&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;Python implementation&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;seaborn&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sns&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.naive_bayes&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GaussianNB&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We compare three different hypotheses:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Optimal Bayes: &lt;span class="math"&gt;\(h_{\text{Bayes}}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Circumference hypothesis: &lt;span class="math"&gt;\(h\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Gaussian Naive Bayes: &lt;span class="math"&gt;\(h_{GNB}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;#P(y|x) definition&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;prob&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.3&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="c1"&gt;#prob of y=1&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt;

&lt;span class="c1"&gt;#coloring function&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;color&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;=&lt;/span&gt;&lt;span class="n"&gt;prob&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;blue&amp;#39;&lt;/span&gt; &lt;span class="c1"&gt;#y=1&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;red&amp;#39;&lt;/span&gt; &lt;span class="c1"&gt;#y=0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The code that defines the various hypotheses:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;h&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;=&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="c1"&gt;#if r=1 then h(x)=bayes(x)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;blue&amp;#39;&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;red&amp;#39;&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;bayes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;prob&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;blue&amp;#39;&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;red&amp;#39;&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;GNB&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;GaussianNB&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sample&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;ypred&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;ypred&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;errors&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="c1"&gt;#draw multiple samples from multivariate_normal&lt;/span&gt;
    &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multivariate_normal&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],[[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;class_y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;x&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;y&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sample&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Series&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;class_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;h_bayes&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bayes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;h&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;GNB&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;GNB&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;error_GNB&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sample&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;GNB&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;error_bayes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sample&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;h_bayes&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;error_h&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sample&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;h&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="n"&gt;errors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;error_h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;error_GNB&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;error_bayes&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;then check whether the other hypotheses have smaller error:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;errors&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note that these are the sample errors. Therefore, it is possible to find an error smaller than the Bayes error. However, if we take large samples it becomes almost improbable for that to happen.&lt;/p&gt;
&lt;p&gt;Some plots:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" height="600" src="/images/bayes_sample.png" style="display: block; margin: 0 auto" width="600"&gt;
&lt;img alt="" height="600" src="/images/optimal_bayes.png" style="display: block; margin: 0 auto" width="600"&gt;
&lt;img alt="" height="600" src="/images/optimal_bayes_GNB.png" style="display: block; margin: 0 auto" width="600"&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;
[1] &lt;em&gt;Understanding Machine Learning: from Theory to Algorithms&lt;/em&gt;, Shai Ben-David and Shai Shalev-Shwartz&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Machine Learning"></category><category term="data science"></category></entry><entry><title>"Probably Approximately Correct (PAC)"</title><link href="/probably-approximately-correct-pac.html" rel="alternate"></link><published>2020-04-14T00:00:00+02:00</published><updated>2020-04-14T00:00:00+02:00</updated><author><name>Joao Gomes</name></author><id>tag:None,2020-04-14:/probably-approximately-correct-pac.html</id><summary type="html">&lt;p&gt;In this post I explain some of the fundamentals of machine learning: PAC learnability, overfitting and generalisation bounds for classification problems. I show how these concepts work in detail for the problem of learning circumferences.&lt;/p&gt;</summary><content type="html">&lt;h3&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#pac"&gt;The learning problem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#proof"&gt;Finite hypothesis classes are PAC learnable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#agnostic"&gt;Agnostic learning&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a name="pac"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;1. The learning problem&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;
PAC stands for "probably approximately correct". In machine learning we want to find a hypothesis that is as close as possible to the ground truth. Since we only have access to a sample of the real distribution, the hypothesis that one builds is itself a function of the sample data, and therefore it is a random variable.  The problem that we want to solve is whether the sample error incurred in choosing a particular hypothesis  is approximately the same as the exact distribution error, within a certain confidence interval.&lt;/p&gt;
&lt;p&gt;Suppose we have a binary classification problem (the same applies for multi-class) with classes &lt;span class="math"&gt;\(y_i\in \{y_0,y_1\}\)&lt;/span&gt;, and we are given a training dataset &lt;span class="math"&gt;\(S\)&lt;/span&gt; with &lt;span class="math"&gt;\(m\)&lt;/span&gt; data-points. Each data-point is characterised by &lt;span class="math"&gt;\(Q\)&lt;/span&gt; features, and represented as a vector &lt;span class="math"&gt;\(q=(q_1,q_2,\ldots,q_Q)\)&lt;/span&gt;. We want to find a map &lt;span class="math"&gt;\(\mathcal{f}\)&lt;/span&gt; between these features and the corresponding class &lt;span class="math"&gt;\(y\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}\mathcal{f}: (q_1,q_2,\ldots,q_Q)\rightarrow \{y_0,y_1\}\end{equation}&lt;/div&gt;
&lt;p&gt;This map, however, does not always exist. There are problems for which we can only determine the class up to a certain confidence level. In this case we say that the learning problem is &lt;em&gt;agnostic&lt;/em&gt;, while when the map exists we say that the problem is &lt;em&gt;realisable&lt;/em&gt;. For example, image recognition is agnostic.&lt;/p&gt;
&lt;p&gt;Let us assume for the moment that such a map exists. The learner chooses a set of hypothesis &lt;span class="math"&gt;\(\mathcal{H}=\{h_1,\ldots,h_n\}\)&lt;/span&gt; and thus introduces &lt;em&gt;bias&lt;/em&gt; in the problem- a different learner may chose a different set of hypothesis. Then, in order to find the hypothesis that most accurately represents the data, the learner chooses one that has the smallest empirical risk, which is the error on the training set. That is, one tries to find the minimum of the sample loss function&lt;/p&gt;
&lt;div class="math"&gt;$$L_S(h)=\frac{1}{m}\sum_{i=1:m}\mathbb{1}\left[h(x_i)\neq y(x_i)\right],\;h\in \mathcal{H}$$&lt;/div&gt;
&lt;p&gt;with &lt;span class="math"&gt;\(\mathbb{1}(.)\)&lt;/span&gt; the Kronecker delta function. Denote the solution of this optimisation problem as &lt;span class="math"&gt;\(h_S\)&lt;/span&gt;. The true or &lt;em&gt;generalization error&lt;/em&gt; is defined instead as the unbiased average&lt;/p&gt;
&lt;div class="math"&gt;$$L(D,h)=\sum_x\mathbb{1}\left[h(x)\neq y(x)\right]D(x)$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(D(x)\)&lt;/span&gt; is a distribution, that the learner may or may not know. In the case of classification, the generalisation error is also the probability of misclassifying a point &lt;span class="math"&gt;\(L(D,h)=\mathbb{P}_{x\sim D(x)}(h(x)\neq y(x))\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If we choose appropriately &lt;span class="math"&gt;\(\mathcal{H}\)&lt;/span&gt; we may find &lt;span class="math"&gt;\(\text{min}\;L_S(h_S)=0\)&lt;/span&gt;. This can happen, for example, by memorising the data. In this case, we say that the hypothesis is &lt;em&gt;overfitting&lt;/em&gt; the data. Although memorising results in zero empirical error, the solution is not very instructive because it does not give information of how well it will perform on unseen data. The solution performs very well on the data because the learner used prior knowledge to choose an hypothesis set with sufficient capacity (or complexity) to accommodate the entire dataset. In the above minimisation problem, one should find a solution that does well (small error) on a large number of samples rather then having a very small error in a particular sample. Overfitting solutions should be avoided as they can lead to misleading conclusions. Instead, the learner should aim at obtaining a training error that is comparable to the error obtained with different samples.&lt;/p&gt;
&lt;p&gt;To make things practical, consider the problem of classifying points on a 2D plane as red or blue. The decision boundary is a circumference of radius &lt;span class="math"&gt;\(R\)&lt;/span&gt; concentric with the origin of the plane, which colours points that are inside as red and outside as blue. See figure below. The training dataset consists of &lt;span class="math"&gt;\(m\)&lt;/span&gt; data-points &lt;span class="math"&gt;\(\mathbb{x}=(x_1,x_2)\)&lt;/span&gt; sampled independently and identically distributed (i.i.d) from a distribution &lt;span class="math"&gt;\(D(x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" height="400" src="/images/PAC learning_1.png" style="display: block; margin: 0 auto" width="400"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Here the circumference &lt;span class="math"&gt;\(R\)&lt;/span&gt; denotes the ground truth which classifies points as red or blue, depending on whether they are inside or outside of the circle, respectively.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The learning problem is to find a hypothesis &lt;span class="math"&gt;\(h(x): x\rightarrow y=\{\text{blue},\text{red}\}\)&lt;/span&gt; that has small error on unseen data.   &lt;/p&gt;
&lt;p&gt;Assuming that the learner has prior knowledge of the ground truth (realisability assumption), one of the simplest algorithms is to consider the set of concentric circumferences and minimise the empirical risk. One can achieve this by drawing a decision boundary that is as close as possible to the most outward red (or inward blue data-points). This guarantees that when &lt;span class="math"&gt;\(m\rightarrow \infty\)&lt;/span&gt; we recover the exact decision boundary: the circumference &lt;span class="math"&gt;\(R\)&lt;/span&gt;.  The empirical risk minimisation problem gives the solution represented in the figure below by the circumference &lt;span class="math"&gt;\(R'\)&lt;/span&gt;. However, newly generated data-points may lie in between &lt;span class="math"&gt;\(R'\)&lt;/span&gt; and &lt;span class="math"&gt;\(R\)&lt;/span&gt;, and therefore would be misclassified.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" height="400" src="/images/circle_learning_epsilon.png" style="display: block; margin: 0 auto" width="400"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;a) The hypothesis &lt;span class="math"&gt;\(h\)&lt;/span&gt; is a circumference of radius &lt;span class="math"&gt;\(R'\)&lt;/span&gt; concentric with the origin and it is determined by the most outward red data-point. This ensures that all training set &lt;span class="math"&gt;\(S\)&lt;/span&gt; is correctly classified. b) The circumference of radius &lt;span class="math"&gt;\(R_{\epsilon}\)&lt;/span&gt; corresponds to a hypothesis &lt;span class="math"&gt;\(h_{\epsilon}\)&lt;/span&gt; that has generalization error &lt;span class="math"&gt;\(L(D,h_{\epsilon})=\epsilon\)&lt;/span&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Given that this is an overfitting solution, one has to be careful of how well it generalises. It is possible that the generalisation error is small for such a solution, but one has to be confident of how common this situation may be. If the sample that led to that solution is a rare event then we should not trust its predictions. Therefore we are interested in bounding the probability of making a bad prediction, that is,&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}\label{eq1}\mathbb{P}_{S \sim D^m(x)}(L(D,h_S)&amp;gt;\epsilon)&amp;lt;\delta \tag{1}\end{equation}&lt;/div&gt;
&lt;p&gt;Conversely, this tells us with confidence of at least &lt;span class="math"&gt;\(1-\delta\)&lt;/span&gt; that&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}\label{eq2}L(D,h_S)\leq\epsilon\tag{2}\end{equation}&lt;/div&gt;
&lt;p&gt;A &lt;em&gt;PAC learnable hypothesis&lt;/em&gt; is a hypothesis for which one can put a bound on the probability of the form \eqref{eq1} with &lt;span class="math"&gt;\(\epsilon, \delta\)&lt;/span&gt; arbitrary.&lt;/p&gt;
&lt;p&gt;In  the case of the circumference example, define &lt;span class="math"&gt;\(R_{\epsilon}\)&lt;/span&gt; for which &lt;span class="math"&gt;\(L(D,h_{\epsilon})=\epsilon\)&lt;/span&gt; with &lt;span class="math"&gt;\(h_{\epsilon}\)&lt;/span&gt; the corresponding solution. Therefore any hypothesis corresponding to a radius less than &lt;span class="math"&gt;\(R_{\epsilon}\)&lt;/span&gt; leads to a generalisation error larger than &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;. The probability of sampling a point and falling in the region between &lt;span class="math"&gt;\(R_{\epsilon}\)&lt;/span&gt; and &lt;span class="math"&gt;\(R\)&lt;/span&gt; is precisely &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;. Conversely the probability of falling outside that region is &lt;span class="math"&gt;\(1-\epsilon\)&lt;/span&gt;. It is then easy to see that the probability that we need equals&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbb{P}_{S \sim D^m(x)}(L(D,h_S)&amp;gt;\epsilon)=(1-\epsilon)^m$$&lt;/div&gt;
&lt;p&gt;Using the bound &lt;span class="math"&gt;\(1-\epsilon&amp;lt;e^{-\epsilon}\)&lt;/span&gt; we can choose &lt;span class="math"&gt;\(\delta=e^{-\epsilon m}\)&lt;/span&gt;, and thus equivalently &lt;span class="math"&gt;\(\epsilon=\frac{1}{m}\ln\left(\frac{1}{\delta}\right)\)&lt;/span&gt;. Hence using equation \eqref{eq2}, we have&lt;/p&gt;
&lt;div class="math"&gt;$$L(D,h_S)\leq\frac{1}{m}\ln\left(\frac{1}{\delta}\right)$$&lt;/div&gt;
&lt;p&gt;with probability &lt;span class="math"&gt;\(1-\delta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a name="proof"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;2. Finite hypothesis classes are PAC learnable&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;
Let us assume that we have a finite hypothesis class with &lt;span class="math"&gt;\(N\)&lt;/span&gt; hypothesis, that is, &lt;span class="math"&gt;\(\mathcal{H}_N=\{h_1,\ldots,h_N\}\)&lt;/span&gt;, and that this class is realisable, meaning that it contains a &lt;span class="math"&gt;\(h^\star\)&lt;/span&gt; for which &lt;span class="math"&gt;\(L_S(h^\star)=0\;\forall S\)&lt;/span&gt;. We want to upper bound the generalisation error of a hypothesis &lt;span class="math"&gt;\(h_S\)&lt;/span&gt; obtained using empirical risk minimisation, that is, we want to find a bound of the form&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbb{P}_{x\sim D(x)}(S: L(D,h_S)&amp;gt;\epsilon)&amp;lt;\delta\tag{3}\label{eq3}$$&lt;/div&gt;
&lt;p&gt;Define &lt;span class="math"&gt;\(\mathcal{H}_B\)&lt;/span&gt; as the set of hypotheses that have generalisation error larger than &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; (it does not necessarily minimise the emprirical risk). We call this the set of bad hypotheses&lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{H}_B=\{h\in \mathcal{H}_N: L(D,h)&amp;gt;\epsilon\}$$&lt;/div&gt;
&lt;p&gt;Similarly one can define the set of misleading training sets, as those that lead to a hypothesis &lt;span class="math"&gt;\(h_S\in \mathcal{H}_B\)&lt;/span&gt; with &lt;span class="math"&gt;\(L_S(h_S)=0\)&lt;/span&gt;. That is,&lt;/p&gt;
&lt;div class="math"&gt;$$M=\{S: h\exists \mathcal{H}_B, L_S(h)=0\}$$&lt;/div&gt;
&lt;p&gt;Since we assume the class is realisable, the hypothesis &lt;span class="math"&gt;\(h_S\)&lt;/span&gt; in equation &lt;span class="math"&gt;\(\eqref{eq3}\)&lt;/span&gt; must have &lt;span class="math"&gt;\(L_S(h_S)=0\)&lt;/span&gt;, and therefore the sample data is a misleading dataset. So we need the probability of sampling a misleading dataset &lt;span class="math"&gt;\(S\in M\)&lt;/span&gt;. Using&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
M=\cup_{h\in \mathcal{H}_B} \{S: L_S(h)=0\}
\end{align}$$&lt;/div&gt;
&lt;p&gt;and the property &lt;span class="math"&gt;\(\mathbb{P}(A\cup B)&amp;lt;\mathbb{P}(A)+\mathbb{P}(B)\)&lt;/span&gt;, we have&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
\mathbb{P}(S\in M)\leq \sum_{h\in \mathcal{H}_B} \mathbb{P}(S: L_S(h)=0)
\end{align}$$&lt;/div&gt;
&lt;p&gt;Now for each &lt;span class="math"&gt;\(h\in\mathcal{H}\)&lt;/span&gt; we can put a bound on &lt;span class="math"&gt;\(\mathbb{P}(S: L_S(h)=0)\)&lt;/span&gt;. Since we want &lt;span class="math"&gt;\(L(D,h)&amp;gt;\epsilon\)&lt;/span&gt;, the probability of misclassifying a data-point is larger than &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;, and conversely a point will correctly classified with probability &lt;span class="math"&gt;\(1-\leq \epsilon\)&lt;/span&gt;. Therefore, as the solution is always overfitting and so all the points are correctly classified, we have&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbb{P}(S: L_S(h)=0)\leq (1-\epsilon)^m$$&lt;/div&gt;
&lt;p&gt;The final bound becomes&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
\mathbb{P}(S\in M)\leq \sum_{h\in \mathcal{H}_B}(1-\epsilon)^m\leq |\mathcal{H}|(1-\epsilon)^m\leq |\mathcal{H}|e^{-\epsilon m}
\end{align}$$&lt;/div&gt;
&lt;p&gt;Setting &lt;span class="math"&gt;\(\delta=\mid\mathcal{H}\mid e^{-\epsilon m}\)&lt;/span&gt;, we have with a probability of at least &lt;span class="math"&gt;\(1-\delta\)&lt;/span&gt; that&lt;/p&gt;
&lt;div class="math"&gt;$$L(D,h_S)\leq \frac{1}{m}\ln\left(\frac{\mid\mathcal{H}\mid}{\delta}\right)$$&lt;/div&gt;
&lt;p&gt;&lt;a name="agnostic"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;3. Agnostic learning&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;
In agnostic learning we do not have anymore an exact mapping between the features and the classes. Instead the classes themselves are sampled from a probability distribution given the features, that is, we have &lt;span class="math"&gt;\(P(y|x)\)&lt;/span&gt;. In the realisable example this probability is always &lt;span class="math"&gt;\(P(y|x)=0,1\)&lt;/span&gt;. Given this we extend the distribution to both the features and the classes so we have &lt;span class="math"&gt;\(D(x,y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The definition of generalisation error is slightly changed to
&lt;/p&gt;
&lt;div class="math"&gt;$$L(D,h)=\sum_{x,y}\mathbb{1}(h(x)\neq y)D(x,y)$$&lt;/div&gt;
&lt;p&gt;Because we do not have anymore the realisability condition, showing that a problem is PAC learnable is a bit more complicated. For this purpose we use one of the most useful inequalities in statistics:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Hoeffding's Inequality:&lt;/em&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbb{P}(|\bar{x}-\langle x\rangle|&amp;gt;\epsilon)\leq 2e^{-2 m\epsilon^2/(b-a)^2}$$&lt;/div&gt;
&lt;p&gt;for a random variable &lt;span class="math"&gt;\(x\)&lt;/span&gt; and any distribution. Here &lt;span class="math"&gt;\(\bar{x}\)&lt;/span&gt; is the sample mean, &lt;span class="math"&gt;\(\langle x \rangle\)&lt;/span&gt; is the distribution average and &lt;span class="math"&gt;\(a\leq x\leq b\)&lt;/span&gt;. We can apply this property to the empirical loss and the generalisation loss. Since they are quantities between zero and one (they are probabilities), we have&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbb{P}(|L_S(h)-L(D,h)|&amp;gt;\epsilon)\leq 2e^{-2 m\epsilon^2}$$&lt;/div&gt;
&lt;p&gt;We are interested in the probability of sampling a training set which gives a misleading prediction. So we want&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbb{P}_{S\sim D^m}(h\exists \mathcal{H}, |L_S(h)-L(D,h)|&amp;gt;\epsilon)\leq \sum_{h\in \mathcal{H}} \mathbb{P}_{S\sim D^m}(|L_S(h)-L(D,h)|&amp;gt;\epsilon)$$&lt;/div&gt;
&lt;p&gt;and thus using Hoeffding's inequality we have
&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbb{P}_{S\sim D^m}(h\exists \mathcal{H}, |L_S(h)-L(D,h)|&amp;gt;\epsilon)\leq \mid\mathcal{H}\mid 2e^{-2\epsilon^2m}
$$&lt;/div&gt;
&lt;p&gt;
We set &lt;span class="math"&gt;\(\delta=2\mid\mathcal{H}\mid e^{-2 m\epsilon^2}\)&lt;/span&gt;, and conclude&lt;/p&gt;
&lt;div class="math"&gt;$$|L_S(h)-L(D,h)|\leq \sqrt{\frac{1}{2m}\ln\left(\frac{2\mid\mathcal{H}\mid}{\delta}\right)},\;\forall h\in \mathcal{H}$$&lt;/div&gt;
&lt;p&gt;Say that we have &lt;span class="math"&gt;\(L(D,h)&amp;gt;L_S(h)\)&lt;/span&gt; for &lt;span class="math"&gt;\(h=h_S\)&lt;/span&gt;, the solution we obtain after minimising the empirical loss, then&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}\label{eq4}L(D,h)\leq L_S(h)+\sqrt{\frac{1}{2m}\ln\left(\frac{2\mid\mathcal{H}\mid}{\delta}\right)}\tag{4}\end{equation}&lt;/div&gt;
&lt;p&gt;This equation demonstrates clearly the trouble with overfitting. To memorise the data we need to use hypothesis classes with large dimension, so the solution has enough capacity to accommodate each data-point. This makes the second term on r.h.s of the inequality \eqref{eq4} very large, loosening the bound on the generalisation error instead of making it tighter. The fact is that we should minimise the empirical error together with that term, so we make the bound on the true error smaller. This leads us to the idea of regularisation in machine learning, whereby the empirical loss is endowed with correction terms that mitigate highly complex solutions.&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;
[1] &lt;em&gt;Understanding Machine Learning: from Theory to Algorithms&lt;/em&gt;, Shai Ben-David and Shai Shalev-Shwartz&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Machine Learning"></category><category term="data science"></category></entry></feed>