<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>"Linear Discriminant Analysis" â€” Data Science and Machine Learning</title>
	<meta name="description" content="Title: "Linear Discriminant Analysis"; Date: 2020-08-20; Author: Joao Gomes">
	<meta name="author" content="Joao Gomes">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
	<!--[if lt IE 9]>
		<script src="/theme/html5.js"></script>
		<![endif]-->
	<link href="/theme/css/ipython.css" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/bootswatch/3.2.0/simplex/bootstrap.min.css" rel="stylesheet">
	<link href="/theme/css/local.css" rel="stylesheet">
	<link href="/theme/css/pygments.css" rel="stylesheet">
</head>
<body>
<div class="container">
	<div class="page-header">
		<h1><a href="/">Data Science and Machine Learning</a>
			<br>	</div>
	<div class="row">
		<div class="col-md-8 col-md-offset-2">
<div class="article" itemscope itemtype="http://schema.org/BlogPosting">
	<div class="text-center article-header">
		<h1 itemprop="name headline" class="article-title">"Linear Discriminant Analysis"</h1>
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<h4 itemprop="name">Joao Gomes</h4>
		</span>
		<time datetime="2020-08-20T00:00:00+02:00" itemprop="datePublished">Thu 20 August 2020</time>
	</div>
	<div>
		Category:
		<span itemprop="articleSection">
			<a href="/category/machine-learning.html" rel="category">Machine Learning</a>
		</span>
	</div>
 
	<div>
		Tags:
		<span itemprop="keywords">
			<a href="/tag/data-science.html" rel="tag">data science</a>
		</span>
	</div>
	<div itemprop="articleBody" class="article-body"><ol>
<li><a href="#def1">LDA</a></li>
<li><a href="#decision">Decision Boundary</a></li>
<li><a href="#dimension">Dimensionality Reduction</a></li>
<li><a href="#quadratic">Quadratic Discriminant Analysis</a></li>
<li><a href="#python">Python implementation</a></li>
</ol>
<p><a name="def1"></a></p>
<h3><strong>1. LDA</strong></h3>
<p>The LDA or linear discriminant analysis is an algorithm whereby the probability has the following form</p>
<div class="math">$$p(x|c)=\frac{1}{\sqrt{(2\pi)^d\text{det}(\Sigma)}}e^{-\frac{1}{2}(x-\mu_c)^t\Sigma^{-1}(x-\mu_c)}$$</div>
<p>
 where <span class="math">\(c\)</span> is the class and <span class="math">\(p(c)=\pi_c\)</span>.
Using Bayes theorem we calculate
</p>
<div class="math">$$p(c|x)=\frac{p(x|c)\pi_c}{\sum_k p(x|k)\pi_k}=\frac{e^{-\frac{1}{2}(x-\mu_c)^t\Sigma^{-1}(x-\mu_c)}\pi_c}{\sum_k e^{-\frac{1}{2}(x-\mu_k)^t\Sigma^{-1}(x-\mu_k)}\pi_k}=\frac{e^{-\mu_c^t\Sigma^{-1}x-\frac{1}{2}\mu_c^t\Sigma^{-1}\mu_c }\pi_c}{\sum_ke^{-\mu_k^t\Sigma^{-1}x-\frac{1}{2}\mu_k^t\Sigma^{-1}\mu_k }\pi_k}$$</div>
<p>Note that this has precisely the form of the logistic regression probability. We can conclude right away that the predicted classes form simply connected convex sets. However, to train the LDA algorithm we use instead the probability <span class="math">\(p(x,c)\)</span> rather than <span class="math">\(p(c|x)\)</span> as in the logistic case. We proceed as usual by minimizing the log loss function
</p>
<div class="math">$$\mathcal{L}=-\sum_i \ln p(x_i,c_i)=-\sum_i \ln(\pi_{c_i}) +\sum_i \frac{1}{2}(x_i-\mu_{c_i})^t\Sigma^{-1}(x_i-\mu_{c_i})+\frac{N}{2}\ln\text{det}(\Sigma)+\frac{Nd}{2}\ln(2\pi)$$</div>
<p>
Using the property
</p>
<div class="math">$$\frac{\partial}{\partial \Sigma^{-1}_{ij}}\ln\text{det}\Sigma=-\Sigma_{ij}$$</div>
<p>
we calculate
</p>
<div class="math">$$\frac{\partial}{\partial \Sigma^{-1}_{ij}}\mathcal{L}=0\iff \Sigma_{ij}=\frac{1}{N}\sum_k(x_k-\mu_{c_k})_i(x_k-\mu_{c_k})_j$$</div>
<p>
While the other parameters are calculated as
</p>
<div class="math">$$\frac{\partial}{\partial \mu_c}\mathcal{L}=0\iff \frac{1}{N_c}\sum_{k: y=c} x_k$$</div>
<p>
where the sum is over the <span class="math">\(N_c\)</span> datapoints with class <span class="math">\(c\)</span>, and
</p>
<div class="math">$$\frac{\partial}{\partial \pi_c}\mathcal{L}=0\iff \pi_c=\frac{N_c}{N}$$</div>
<p>
<a name="decision"></a></p>
<h3><strong>2. Decision Boundary</strong></h3>
<p>The predictor is determined by the maximum of <span class="math">\(p(c|x)\)</span>. As we have seen above, this probability has the same form as the logistic regression. This means that also, for LDA, the regions of the predicted class are singly connected convex sets. </p>
<p><a name="dimension"></a></p>
<h3><strong>3. Dimensionality Reduction</strong></h3>
<p>The PCA, or principal component analysis, is an algorithm that reduces the dimensionality of the dataset while keeping the most relevant features. However, the PCA analysis does not discriminate over the classes, which may lead to a lack of predictability in supervised learning problems. The right projection keeps the classes separated in the example below, which is a better projection than the one on the left.</p>
<p><img alt="" height="400" src="/images/lda_projection.png" style="display: block; margin: 0 auto" width="400"> </p>
<p>Besides being a Gaussian classifier, the LDA can be used to reduce the data dimensionally. The basic idea is to find a projection axis that maximizes the "between" class variance while, at the same time, minimizes the "within" class variance. That is, we make the gaussians more narrow, and at the same time, the centers become farther apart from each other.  <br>
Consider the covariance matrix given by
</p>
<div class="math">$$\Sigma_{ij}=\frac{1}{N}\sum_x (x_i-\bar{x}_i)(x_j-\bar{x}_j)$$</div>
<p>
We can write this as
</p>
<div class="math">$$\begin{aligned}\sum_x (x_i-\bar{x}_i)(x_j-\bar{x}_j)&amp;=\sum_c\sum_{x\in \text{class }c} (x_i-\bar{x}_i)(x_j-\bar{x}_j)\\
&amp;=\sum_c\sum_{x\in \text{class }c}(x_i-\bar{x}^c_i)(x_j-\bar{x}^c_j)+\sum_c N_c(\bar{x}^c_i-\bar{x}_i)(\bar{x}^c_j-\bar{x}_j)\end{aligned}$$</div>
<p>
where <span class="math">\(\bar{x}^c\)</span> is the average of <span class="math">\(x\)</span> within class <span class="math">\(c\)</span> and <span class="math">\(N_c\)</span> is the number of points in class <span class="math">\(c\)</span>. The first term above is known as "within" class covariance, which we denote as <span class="math">\(\textbf{W}\)</span>, and the second term as "between" class covariance, denoted as <span class="math">\(\textbf{B}\)</span>. We want to maximize the quotient
</p>
<div class="math">$$\text{max}_v\frac{v^T\textbf{B}v}{v^T\textbf{W}v}$$</div>
<p>
For that purpose we consider the eigenvalues <span class="math">\(\lambda\)</span> and eigenvectores <span class="math">\(v_{\lambda}\)</span> of <span class="math">\(\textbf{W}^{-1}\textbf{B}\)</span>. The quotient becomes
</p>
<div class="math">$$\frac{v_{\lambda}^T\textbf{W}\textbf{W}^{-1}\textbf{B}v_{\lambda}}{v_{\lambda}^T\textbf{W}v_{\lambda}}=\lambda$$</div>
<p>
It is easy to show that the stationary directions of the quotient correspond to the eigen-directions. Hence the direction of best projection is along the eigenvector with largest eingenvalue.</p>
<p><a name="quadratic"></a></p>
<h3><strong>4. Quadratic Discriminant Analysis</strong></h3>
<p>In QDA or quadratic discriminant analysis the covariance matrix is not necessarily constant across the various classes, instead we have 
</p>
<div class="math">$$p(x|c)=\frac{1}{\sqrt{(2\pi)^d\text{det}(\Sigma_c)}}e^{-\frac{1}{2}(x-\mu_c)^t\Sigma_c^{-1}(x-\mu_c)}$$</div>
<p>
This means that the likelihood <span class="math">\(p(c|x)\)</span> now depends on the covariance matrix, that is,</p>
<div class="math">$$p(c|x)=\frac{p(x|c)\pi_c}{\sum_k p(x|k)\pi_k}=\frac{1}{\sqrt{(2\pi)^d\text{det}(\Sigma_c)}}\frac{e^{-\frac{1}{2}(x-\mu_c)^t\Sigma_c^{-1}(x-\mu_c)}\pi_c}{\sum_k p(x|k)\pi_k}$$</div>
<p><a name="python"></a></p>
<h3><strong>5. Python Implementation </strong></h3>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LDAmodel</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">sigma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">mu</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">prior_prob</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mu</span><span class="o">=</span><span class="n">mu</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prior_prob</span><span class="o">=</span><span class="n">prior_prob</span>
        <span class="k">if</span> <span class="n">sigma</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">inv</span><span class="o">=</span><span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">det</span><span class="o">=</span><span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">inv</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">sigma</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">det</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">sigma</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">mu</span><span class="o">==</span><span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">nc</span><span class="o">=</span><span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="o">=</span><span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">nc</span><span class="o">=</span><span class="n">mu</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="o">=</span><span class="n">mu</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="o">=</span><span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span><span class="o">=</span><span class="kc">None</span>

    <span class="k">def</span> <span class="nf">means</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
        <span class="n">yset</span><span class="o">=</span><span class="nb">set</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">means</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">yset</span><span class="p">),</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">yset</span><span class="p">:</span>
            <span class="n">means</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">x</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">means</span>

    <span class="k">def</span> <span class="nf">var</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
        <span class="n">yset</span><span class="o">=</span><span class="nb">set</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">d</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">var</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">d</span><span class="p">,</span><span class="n">d</span><span class="p">))</span>
        <span class="n">means</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">means</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">yset</span><span class="p">:</span>
            <span class="n">c</span><span class="o">=</span><span class="n">x</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="n">means</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">var</span><span class="o">+=</span><span class="n">np</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">c</span><span class="p">,</span><span class="n">c</span><span class="p">,</span><span class="n">axes</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">var</span><span class="o">=</span><span class="n">var</span><span class="o">/</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">var</span>

    <span class="k">def</span> <span class="nf">priors</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
        <span class="n">priors</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">yset</span><span class="o">=</span><span class="nb">set</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">yset</span><span class="p">:</span>
            <span class="n">priors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="p">(</span><span class="n">y</span><span class="o">==</span><span class="n">i</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">priors</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mu</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">means</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prior_prob</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">priors</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inv</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sigma</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">det</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sigma</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nc</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mu</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">inv</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nc</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nc</span><span class="p">):</span>
            <span class="n">v</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inv</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=-</span><span class="mf">0.5</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">v</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">probs</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="bp">self</span><span class="o">.</span><span class="n">nc</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nc</span><span class="p">):</span>
            <span class="n">t</span><span class="o">=</span><span class="n">x</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">inv</span><span class="p">)</span>
            <span class="n">w</span><span class="o">=</span><span class="p">(</span><span class="n">t</span><span class="o">*</span><span class="n">w</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">probs</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="o">*</span><span class="n">w</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">priors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">probs</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">probs</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span><span class="o">/</span><span class="p">((</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">det</span><span class="p">)))</span>
        <span class="n">Z</span><span class="o">=</span><span class="n">probs</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">probs</span><span class="o">/</span><span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</pre></div>


<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
	<hr>
	<h2>Comments</h2>
</div>
		</div>
	</div> 	<!-- <hr> -->
</div> <!-- /container -->
<footer class="aw-footer bg-danger">
	<div class="container"> <!-- footer -->
		<div class="row">
			<div class="col-md-10 col-md-offset-1">
				<div class="row">
					<div class="col-md-3">
						<h4>Navigation</h4>
						<ul class="list-unstyled my-list-style">
							<li><a href="">Data Science and Machine Learning</a></li>
							<li><a href="/pages/about-me.html"><i class="fa fa-About me "></i> About me</a></li>
							<li><a href="/feeds/all.atom.xml" type="application/atom+xml"><i class="fa fa-rss "></i> atom</a></li>
						</ul>
					</div>
					<div class="col-md-3">
						<h4>Author</h4>
						<ul class="list-unstyled my-list-style">
							<li><a href="https://www.linkedin.com/in/joaomvg/">LinkedIn</a></li>
							<li><a href="https://github.com/joaomvg">GitHub</a></li>
						</ul>
					</div>
					<div class="col-md-3">
						<h4>Categories</h4>
						<ul class="list-unstyled my-list-style">
							<li><a href="/category/data-science.html">Data Science (1)</a></li>
							<li><a href="/category/machine-learning.html">Machine Learning (21)</a></li>
							<li><a href="/category/python.html">Python (1)</a></li>
							<li><a href="/category/statistics.html">Statistics (2)</a></li>
						</ul>
					</div>
					<div class="col-md-3">
						<h4>Links</h4>
						<ul class="list-unstyled my-list-style">
							<li><a href="archives.html">Archives</a></li>
							<li><a href="tags.html">Tags</a></li>
						</ul>
					</div>
				</div>
			</div>
		</div>
	</div>
</footer>
<div class="container">
	<div class="row">
		<div class="col-md-12 text-center center-block aw-bottom">
			<p>&copy; Joao Gomes 2016</p>
			<p>Powered by Pelican</p>
		</div>
	</div>
</div>
<!-- JavaScript -->
<script src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
<script type="text/javascript">
jQuery(document).ready(function($) {
	$("div.collapseheader").click(function () {
		$header = $(this).children("span").first();
		$codearea = $(this).children(".input_area");
		$codearea.slideToggle(500, function () {
			$header.text(function () {
				return $codearea.is(":visible") ? "Collapse Code" : "Expand Code";
			});
		});
	});
});
</script>
</body>
</html>