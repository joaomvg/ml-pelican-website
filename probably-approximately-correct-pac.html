<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>"Probably Approximately Correct (PAC)" â€” Data Science and Machine Learning</title>
	<meta name="description" content="Title: "Probably Approximately Correct (PAC)"; Date: 2020-04-14; Author: Joao Gomes">
	<meta name="author" content="Joao Gomes">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
	<!--[if lt IE 9]>
		<script src="/theme/html5.js"></script>
		<![endif]-->
	<link href="/theme/css/ipython.css" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/bootswatch/3.2.0/simplex/bootstrap.min.css" rel="stylesheet">
	<link href="/theme/css/local.css" rel="stylesheet">
	<link href="/theme/css/pygments.css" rel="stylesheet">
</head>
<body>
<div class="container">
	<div class="page-header">
		<h1><a href="/">Data Science and Machine Learning</a>
			<br>	</div>
	<div class="row">
		<div class="col-md-8 col-md-offset-2">
<div class="article" itemscope itemtype="http://schema.org/BlogPosting">
	<div class="text-center article-header">
		<h1 itemprop="name headline" class="article-title">"Probably Approximately Correct (PAC)"</h1>
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<h4 itemprop="name">Joao Gomes</h4>
		</span>
		<time datetime="2020-04-14T00:00:00+02:00" itemprop="datePublished">Tue 14 April 2020</time>
	</div>
	<div>
		Category:
		<span itemprop="articleSection">
			<a href="/category/machine-learning.html" rel="category">Machine Learning</a>
		</span>
	</div>
 
	<div>
		Tags:
		<span itemprop="keywords">
			<a href="/tag/data-science.html" rel="tag">data science</a>
		</span>
	</div>
	<div itemprop="articleBody" class="article-body"><h3><strong>Table of Contents</strong></h3>
<ol>
<li><a href="#pac">The learning problem</a></li>
<li><a href="#proof">Finite hypothesis classes are PAC learnable</a></li>
<li><a href="#agnostic">Agnostic learning</a></li>
</ol>
<p><a name="pac"></a></p>
<h3><strong>1. The learning problem</strong></h3>
<p><br/>
PAC stands for "probably approximately correct". In machine learning we want to find a hypothesis that is as close as possible to the ground truth. Since we only have access to a sample of the real distribution, the hypothesis that one builds is itself a function of the sample data, and therefore it is a random variable.  The problem that we want to solve is whether the sample error incurred in choosing a particular hypothesis  is approximately the same as the exact distribution error, within a certain confidence interval.</p>
<p>Suppose we have a binary classification problem (the same applies for multi-class) with classes <span class="math">\(y_i\in \{y_0,y_1\}\)</span>, and we are given a training dataset <span class="math">\(S\)</span> with <span class="math">\(m\)</span> data-points. Each data-point is characterised by <span class="math">\(Q\)</span> features, and represented as a vector <span class="math">\(q=(q_1,q_2,\ldots,q_Q)\)</span>. We want to find a map <span class="math">\(\mathcal{f}\)</span> between these features and the corresponding class <span class="math">\(y\)</span>:</p>
<div class="math">\begin{equation}\mathcal{f}: (q_1,q_2,\ldots,q_Q)\rightarrow \{y_0,y_1\}\end{equation}</div>
<p>This map, however, does not always exist. There are problems for which we can only determine the class up to a certain confidence level. In this case we say that the learning problem is <em>agnostic</em>, while when the map exists we say that the problem is <em>realisable</em>. For example, image recognition is agnostic.</p>
<p>Let us assume for the moment that such a map exists. The learner chooses a set of hypothesis <span class="math">\(\mathcal{H}=\{h_1,\ldots,h_n\}\)</span> and thus introduces <em>bias</em> in the problem- a different learner may chose a different set of hypothesis. Then, in order to find the hypothesis that most accurately represents the data, the learner chooses one that has the smallest empirical risk, which is the error on the training set. That is, one tries to find the minimum of the sample loss function</p>
<div class="math">$$L_S(h)=\frac{1}{m}\sum_{i=1:m}\mathbb{1}\left[h(x_i)\neq y(x_i)\right],\;h\in \mathcal{H}$$</div>
<p>with <span class="math">\(\mathbb{1}(.)\)</span> the Kronecker delta function. Denote the solution of this optimisation problem as <span class="math">\(h_S\)</span>. The true or <em>generalization error</em> is defined instead as the unbiased average</p>
<div class="math">$$L(D,h)=\sum_x\mathbb{1}\left[h(x)\neq y(x)\right]D(x)$$</div>
<p>where <span class="math">\(D(x)\)</span> is a distribution, that the learner may or may not know. In the case of classification, the generalisation error is also the probability of misclassifying a point <span class="math">\(L(D,h)=\mathbb{P}_{x\sim D(x)}(h(x)\neq y(x))\)</span>.</p>
<p>If we choose appropriately <span class="math">\(\mathcal{H}\)</span> we may find <span class="math">\(\text{min}\;L_S(h_S)=0\)</span>. This can happen, for example, by memorising the data. In this case, we say that the hypothesis is <em>overfitting</em> the data. Although memorising results in zero empirical error, the solution is not very instructive because it does not give information of how well it will perform on unseen data. The solution performs very well on the data because the learner used prior knowledge to choose an hypothesis set with sufficient capacity (or complexity) to accommodate the entire dataset. In the above minimisation problem, one should find a solution that does well (small error) on a large number of samples rather then having a very small error in a particular sample. Overfitting solutions should be avoided as they can lead to misleading conclusions. Instead, the learner should aim at obtaining a training error that is comparable to the error obtained with different samples.</p>
<p>To make things practical, consider the problem of classifying points on a 2D plane as red or blue. The decision boundary is a circumference of radius <span class="math">\(R\)</span> concentric with the origin of the plane, which colours points that are inside as red and outside as blue. See figure below. The training dataset consists of <span class="math">\(m\)</span> data-points <span class="math">\(\mathbb{x}=(x_1,x_2)\)</span> sampled independently and identically distributed (i.i.d) from a distribution <span class="math">\(D(x)\)</span>.</p>
<p><img alt="" height="400" src="/images/PAC learning_1.png" style="display: block; margin: 0 auto" width="400"></p>
<p><em>Here the circumference <span class="math">\(R\)</span> denotes the ground truth which classifies points as red or blue, depending on whether they are inside or outside of the circle, respectively.</em></p>
<p>The learning problem is to find a hypothesis <span class="math">\(h(x): x\rightarrow y=\{\text{blue},\text{red}\}\)</span> that has small error on unseen data.   </p>
<p>Assuming that the learner has prior knowledge of the ground truth (realisability assumption), one of the simplest algorithms is to consider the set of concentric circumferences and minimise the empirical risk. One can achieve this by drawing a decision boundary that is as close as possible to the most outward red (or inward blue data-points). This guarantees that when <span class="math">\(m\rightarrow \infty\)</span> we recover the exact decision boundary: the circumference <span class="math">\(R\)</span>.  The empirical risk minimisation problem gives the solution represented in the figure below by the circumference <span class="math">\(R'\)</span>. However, newly generated data-points may lie in between <span class="math">\(R'\)</span> and <span class="math">\(R\)</span>, and therefore would be misclassified.</p>
<p><img alt="" height="400" src="/images/circle_learning_epsilon.png" style="display: block; margin: 0 auto" width="400"></p>
<p><em>a) The hypothesis <span class="math">\(h\)</span> is a circumference of radius <span class="math">\(R'\)</span> concentric with the origin and it is determined by the most outward red data-point. This ensures that all training set <span class="math">\(S\)</span> is correctly classified. b) The circumference of radius <span class="math">\(R_{\epsilon}\)</span> corresponds to a hypothesis <span class="math">\(h_{\epsilon}\)</span> that has generalization error <span class="math">\(L(D,h_{\epsilon})=\epsilon\)</span>.</em></p>
<p>Given that this is an overfitting solution, one has to be careful of how well it generalises. It is possible that the generalisation error is small for such a solution, but one has to be confident of how common this situation may be. If the sample that led to that solution is a rare event then we should not trust its predictions. Therefore we are interested in bounding the probability of making a bad prediction, that is,</p>
<div class="math">\begin{equation}\label{eq1}\mathbb{P}_{S \sim D^m(x)}(L(D,h_S)&gt;\epsilon)&lt;\delta \tag{1}\end{equation}</div>
<p>Conversely, this tells us with confidence of at least <span class="math">\(1-\delta\)</span> that</p>
<div class="math">\begin{equation}\label{eq2}L(D,h_S)\leq\epsilon\tag{2}\end{equation}</div>
<p>A <em>PAC learnable hypothesis</em> is a hypothesis for which one can put a bound on the probability of the form \eqref{eq1} with <span class="math">\(\epsilon, \delta\)</span> arbitrary.</p>
<p>In  the case of the circumference example, define <span class="math">\(R_{\epsilon}\)</span> for which <span class="math">\(L(D,h_{\epsilon})=\epsilon\)</span> with <span class="math">\(h_{\epsilon}\)</span> the corresponding solution. Therefore any hypothesis corresponding to a radius less than <span class="math">\(R_{\epsilon}\)</span> leads to a generalisation error larger than <span class="math">\(\epsilon\)</span>. The probability of sampling a point and falling in the region between <span class="math">\(R_{\epsilon}\)</span> and <span class="math">\(R\)</span> is precisely <span class="math">\(\epsilon\)</span>. Conversely the probability of falling outside that region is <span class="math">\(1-\epsilon\)</span>. It is then easy to see that the probability that we need equals</p>
<div class="math">$$\mathbb{P}_{S \sim D^m(x)}(L(D,h_S)&gt;\epsilon)=(1-\epsilon)^m$$</div>
<p>Using the bound <span class="math">\(1-\epsilon&lt;e^{-\epsilon}\)</span> we can choose <span class="math">\(\delta=e^{-\epsilon m}\)</span>, and thus equivalently <span class="math">\(\epsilon=\frac{1}{m}\ln\left(\frac{1}{\delta}\right)\)</span>. Hence using equation \eqref{eq2}, we have</p>
<div class="math">$$L(D,h_S)\leq\frac{1}{m}\ln\left(\frac{1}{\delta}\right)$$</div>
<p>with probability <span class="math">\(1-\delta\)</span>.</p>
<p><a name="proof"></a></p>
<h3><strong>2. Finite hypothesis classes are PAC learnable</strong></h3>
<p><br/>
Let us assume that we have a finite hypothesis class with <span class="math">\(N\)</span> hypothesis, that is, <span class="math">\(\mathcal{H}_N=\{h_1,\ldots,h_N\}\)</span>, and that this class is realisable, meaning that it contains a <span class="math">\(h^\star\)</span> for which <span class="math">\(L_S(h^\star)=0\;\forall S\)</span>. We want to upper bound the generalisation error of a hypothesis <span class="math">\(h_S\)</span> obtained using empirical risk minimisation, that is, we want to find a bound of the form</p>
<div class="math">$$\mathbb{P}_{x\sim D(x)}(S: L(D,h_S)&gt;\epsilon)&lt;\delta\tag{3}\label{eq3}$$</div>
<p>Define <span class="math">\(\mathcal{H}_B\)</span> as the set of hypotheses that have generalisation error larger than <span class="math">\(\epsilon\)</span> (it does not necessarily minimise the emprirical risk). We call this the set of bad hypotheses</p>
<div class="math">$$\mathcal{H}_B=\{h\in \mathcal{H}_N: L(D,h)&gt;\epsilon\}$$</div>
<p>Similarly one can define the set of misleading training sets, as those that lead to a hypothesis <span class="math">\(h_S\in \mathcal{H}_B\)</span> with <span class="math">\(L_S(h_S)=0\)</span>. That is,</p>
<div class="math">$$M=\{S: h\exists \mathcal{H}_B, L_S(h)=0\}$$</div>
<p>Since we assume the class is realisable, the hypothesis <span class="math">\(h_S\)</span> in equation <span class="math">\(\eqref{eq3}\)</span> must have <span class="math">\(L_S(h_S)=0\)</span>, and therefore the sample data is a misleading dataset. So we need the probability of sampling a misleading dataset <span class="math">\(S\in M\)</span>. Using</p>
<div class="math">$$\begin{align}
M=\cup_{h\in \mathcal{H}_B} \{S: L_S(h)=0\}
\end{align}$$</div>
<p>and the property <span class="math">\(\mathbb{P}(A\cup B)&lt;\mathbb{P}(A)+\mathbb{P}(B)\)</span>, we have</p>
<div class="math">$$\begin{align}
\mathbb{P}(S\in M)\leq \sum_{h\in \mathcal{H}_B} \mathbb{P}(S: L_S(h)=0)
\end{align}$$</div>
<p>Now for each <span class="math">\(h\in\mathcal{H}\)</span> we can put a bound on <span class="math">\(\mathbb{P}(S: L_S(h)=0)\)</span>. Since we want <span class="math">\(L(D,h)&gt;\epsilon\)</span>, the probability of misclassifying a data-point is larger than <span class="math">\(\epsilon\)</span>, and conversely a point will correctly classified with probability <span class="math">\(1-\leq \epsilon\)</span>. Therefore, as the solution is always overfitting and so all the points are correctly classified, we have</p>
<div class="math">$$\mathbb{P}(S: L_S(h)=0)\leq (1-\epsilon)^m$$</div>
<p>The final bound becomes</p>
<div class="math">$$\begin{align}
\mathbb{P}(S\in M)\leq \sum_{h\in \mathcal{H}_B}(1-\epsilon)^m\leq |\mathcal{H}|(1-\epsilon)^m\leq |\mathcal{H}|e^{-\epsilon m}
\end{align}$$</div>
<p>Setting <span class="math">\(\delta=\mid\mathcal{H}\mid e^{-\epsilon m}\)</span>, we have with a probability of at least <span class="math">\(1-\delta\)</span> that</p>
<div class="math">$$L(D,h_S)\leq \frac{1}{m}\ln\left(\frac{\mid\mathcal{H}\mid}{\delta}\right)$$</div>
<p><a name="agnostic"></a></p>
<h3><strong>3. Agnostic learning</strong></h3>
<p><br/>
In agnostic learning we do not have anymore an exact mapping between the features and the classes. Instead the classes themselves are sampled from a probability distribution given the features, that is, we have <span class="math">\(P(y|x)\)</span>. In the realisable example this probability is always <span class="math">\(P(y|x)=0,1\)</span>. Given this we extend the distribution to both the features and the classes so we have <span class="math">\(D(x,y)\)</span>.</p>
<p>The definition of generalisation error is slightly changed to
</p>
<div class="math">$$L(D,h)=\sum_{x,y}\mathbb{1}(h(x)\neq y)D(x,y)$$</div>
<p>Because we do not have anymore the realisability condition, showing that a problem is PAC learnable is a bit more complicated. For this purpose we use one of the most useful inequalities in statistics:</p>
<p><em>Hoeffding's Inequality:</em>
</p>
<div class="math">$$\mathbb{P}(|\bar{x}-\langle x\rangle|&gt;\epsilon)\leq 2e^{-2 m\epsilon^2/(b-a)^2}$$</div>
<p>for a random variable <span class="math">\(x\)</span> and any distribution. Here <span class="math">\(\bar{x}\)</span> is the sample mean, <span class="math">\(\langle x \rangle\)</span> is the distribution average and <span class="math">\(a\leq x\leq b\)</span>. We can apply this property to the empirical loss and the generalisation loss. Since they are quantities between zero and one (they are probabilities), we have</p>
<div class="math">$$\mathbb{P}(|L_S(h)-L(D,h)|&gt;\epsilon)\leq 2e^{-2 m\epsilon^2}$$</div>
<p>We are interested in the probability of sampling a training set which gives a misleading prediction. So we want</p>
<div class="math">$$\mathbb{P}_{S\sim D^m}(h\exists \mathcal{H}, |L_S(h)-L(D,h)|&gt;\epsilon)\leq \sum_{h\in \mathcal{H}} \mathbb{P}_{S\sim D^m}(|L_S(h)-L(D,h)|&gt;\epsilon)$$</div>
<p>and thus using Hoeffding's inequality we have
</p>
<div class="math">$$\mathbb{P}_{S\sim D^m}(h\exists \mathcal{H}, |L_S(h)-L(D,h)|&gt;\epsilon)\leq \mid\mathcal{H}\mid 2e^{-2\epsilon^2m}
$$</div>
<p>
We set <span class="math">\(\delta=2\mid\mathcal{H}\mid e^{-2 m\epsilon^2}\)</span>, and conclude</p>
<div class="math">$$|L_S(h)-L(D,h)|\leq \sqrt{\frac{1}{2m}\ln\left(\frac{2\mid\mathcal{H}\mid}{\delta}\right)},\;\forall h\in \mathcal{H}$$</div>
<p>Say that we have <span class="math">\(L(D,h)&gt;L_S(h)\)</span> for <span class="math">\(h=h_S\)</span>, the solution we obtain after minimising the empirical loss, then</p>
<div class="math">\begin{equation}\label{eq4}L(D,h)\leq L_S(h)+\sqrt{\frac{1}{2m}\ln\left(\frac{2\mid\mathcal{H}\mid}{\delta}\right)}\tag{4}\end{equation}</div>
<p>This equation demonstrates clearly the trouble with overfitting. To memorise the data we need to use hypothesis classes with large dimension, so the solution has enough capacity to accommodate each data-point. This makes the second term on r.h.s of the inequality \eqref{eq4} very large, loosening the bound on the generalisation error instead of making it tighter. The fact is that we should minimise the empirical error together with that term, so we make the bound on the true error smaller. This leads us to the idea of regularisation in machine learning, whereby the empirical loss is endowed with correction terms that mitigate highly complex solutions.</p>
<h3><strong>References</strong></h3>
<p><br/>
[1] <em>Understanding Machine Learning: from Theory to Algorithms</em>, Shai Ben-David and Shai Shalev-Shwartz</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
	<hr>
	<h2>Comments</h2>
</div>
		</div>
	</div> 	<!-- <hr> -->
</div> <!-- /container -->
<footer class="aw-footer bg-danger">
	<div class="container"> <!-- footer -->
		<div class="row">
			<div class="col-md-10 col-md-offset-1">
				<div class="row">
					<div class="col-md-3">
						<h4>Navigation</h4>
						<ul class="list-unstyled my-list-style">
							<li><a href="">Data Science and Machine Learning</a></li>
							<li><a href="/pages/about-me.html"><i class="fa fa-About me "></i> About me</a></li>
							<li><a href="/feeds/all.atom.xml" type="application/atom+xml"><i class="fa fa-rss "></i> atom</a></li>
						</ul>
					</div>
					<div class="col-md-3">
						<h4>Author</h4>
						<ul class="list-unstyled my-list-style">
							<li><a href="https://www.linkedin.com/in/joaomvg/">LinkedIn</a></li>
							<li><a href="https://github.com/joaomvg">GitHub</a></li>
						</ul>
					</div>
					<div class="col-md-3">
						<h4>Categories</h4>
						<ul class="list-unstyled my-list-style">
							<li><a href="/category/data-science.html">Data Science (1)</a></li>
							<li><a href="/category/machine-learning.html">Machine Learning (17)</a></li>
							<li><a href="/category/statistics.html">Statistics (2)</a></li>
						</ul>
					</div>
					<div class="col-md-3">
						<h4>Links</h4>
						<ul class="list-unstyled my-list-style">
							<li><a href="archives.html">Archives</a></li>
							<li><a href="tags.html">Tags</a></li>
						</ul>
					</div>
				</div>
			</div>
		</div>
	</div>
</footer>
<div class="container">
	<div class="row">
		<div class="col-md-12 text-center center-block aw-bottom">
			<p>&copy; Joao Gomes 2016</p>
			<p>Powered by Pelican</p>
		</div>
	</div>
</div>
<!-- JavaScript -->
<script src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
<script type="text/javascript">
jQuery(document).ready(function($) {
	$("div.collapseheader").click(function () {
		$header = $(this).children("span").first();
		$codearea = $(this).children(".input_area");
		$codearea.slideToggle(500, function () {
			$header.text(function () {
				return $codearea.is(":visible") ? "Collapse Code" : "Expand Code";
			});
		});
	});
});
</script>
</body>
</html>