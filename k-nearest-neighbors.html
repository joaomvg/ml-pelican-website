<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>"K-Nearest Neighbors" â€” Data Science and Machine Learning</title>
	<meta name="description" content="Title: "K-Nearest Neighbors"; Date: 2020-07-29; Author: Joao Gomes">
	<meta name="author" content="Joao Gomes">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
	<!--[if lt IE 9]>
		<script src="/theme/html5.js"></script>
		<![endif]-->
	<link href="/theme/css/ipython.css" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/bootswatch/3.2.0/simplex/bootstrap.min.css" rel="stylesheet">
	<link href="/theme/css/local.css" rel="stylesheet">
	<link href="/theme/css/pygments.css" rel="stylesheet">
</head>
<body>
<div class="container">
	<div class="page-header">
		<h1><a href="/">Data Science and Machine Learning</a>
			<br>	</div>
	<div class="row">
		<div class="col-md-8 col-md-offset-2">
<div class="article" itemscope itemtype="http://schema.org/BlogPosting">
	<div class="text-center article-header">
		<h1 itemprop="name headline" class="article-title">"K-Nearest Neighbors"</h1>
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<h4 itemprop="name">Joao Gomes</h4>
		</span>
		<time datetime="2020-07-29T00:00:00+02:00" itemprop="datePublished">Wed 29 July 2020</time>
	</div>
	<div>
		Category:
		<span itemprop="articleSection">
			<a href="/category/machine-learning.html" rel="category">Machine Learning</a>
		</span>
	</div>
 
	<div>
		Tags:
		<span itemprop="keywords">
			<a href="/tag/data-science.html" rel="tag">data science</a>
		</span>
	</div>
	<div itemprop="articleBody" class="article-body"><ol>
<li><a href="#def1">KNN Algorithm</a></li>
<li><a href="#decision">Decision Boundary</a></li>
<li><a href="#curse">Curse of dimensionality</a></li>
<li><a href="#python">Python implementation: Classification</a></li>
</ol>
<p><a name="def1"></a></p>
<h3><strong>1. KNN algorithm</strong></h3>
<p>The nearest-neighbors algorithm considers the <span class="math">\(K\)</span> nearest neighbors of a datapoint <span class="math">\(x\)</span> to predict its label. In the figure below, we have represented a binary classification problem (colors red and green for classes 0,1 respectively) with datapoints living in a 2-dimensional feature space .</p>
<p><img alt="" height="300" src="/images/knn.png" style="display: block; margin: 0 auto" width="300"></p>
<p>The algorithm consists in attributing the majority class amongts the <span class="math">\(K\)</span>-nearest neighbors. In the example above we consider the 3 nearest neighbors using euclidean distances. Mathematically the predictor <span class="math">\(\hat{y}\)</span> is given by
</p>
<div class="math">$$\hat{y}(x)=\text{argmax}_{0,1}\{n_0(x),n_1(x): x\in D_K(x)\}$$</div>
<p>
where <span class="math">\(D_K(x)\)</span> is the set of <span class="math">\(K\)</span>-nearest neighbors and <span class="math">\(n_{0,1}(x)\)</span> are the number of neighbors in <span class="math">\(D_K\)</span> with class <span class="math">\(0,1\)</span> respectively. The ratio <span class="math">\(n_{0,1}/K\)</span> are the corresponding probabilities. For a multiclass problem the predictor follows a similar logic except that we choose the majority class for which <span class="math">\(n_i(x)\)</span> is the maximum, with <span class="math">\(i\)</span> denoting the possible classes. </p>
<p>A probabilistic approach to nearest neighbors is as follows. We consider the distribution
</p>
<div class="math">$$p(x|c)=\frac{1}{N_c\sqrt{2\pi\sigma^2}^{D/2}}\sum_{n\in\text{class c},n=1}^{n=N_c}e^{-\frac{\|x-\mu_n\|^2}{2\sigma^2}}$$</div>
<p>
with <span class="math">\(N_c\)</span> the number of points with class <span class="math">\(c\)</span> which have coordinates <span class="math">\(\mu_c\)</span>, and <span class="math">\(x\)</span> lives in <span class="math">\(D\)</span> dimensions. The probabilities <span class="math">\(p(c)\)</span> are determined from the observed frequencies, that is,
</p>
<div class="math">$$p(c=0)=\frac{N_0}{N_0+N_1},\;p(c=1)=\frac{N_1}{N_0+N_1}$$</div>
<p>
The ratio of the likelihoods is then<br>
</p>
<div class="math">$$\frac{p(c=1|x)}{p(c=0|x)}=\frac{p(x|c=1)p(c=1)}{p(x|c=0)p(c=0)}=\frac{\sum_{n=1}^{n=N_1}e^{-\frac{\|x-\mu_n\|^2}{2\sigma^2}}}{\sum_{n=1}^{n=N_0}e^{-\frac{\|x-\mu_n\|^2}{2\sigma^2}}}$$</div>
<p>Take <span class="math">\(d(x)\)</span> as the largest distance within the set of <span class="math">\(K\)</span>-nearest neighbors of the datapoint <span class="math">\(x\)</span>. If the variance <span class="math">\(\sigma\)</span> is of order <span class="math">\(\sim d\)</span> then the exponentials with arguments <span class="math">\(\|x-\mu\|^2&gt;d^2\)</span> can be neglected while for <span class="math">\(\|x-\mu\|^2&lt;d^2\)</span> the exponential becomes of order one, and so we approximate
</p>
<div class="math">$$\frac{\sum_{n=1}^{n=N_1}e^{-\frac{\|x-\mu_n\|^2}{2\sigma^2}}}{\sum_{n=1}^{n=N_0}e^{-\frac{\|x-\mu_n\|^2}{2\sigma^2}}}\simeq \frac{\sum_{i\in D_K^1(x)} e^{-\frac{\|x-\mu_i\|^2}{2\sigma^2}}}{\sum_{j\in D_K^0(x)} e^{-\frac{\|x-\mu_j\|^2}{2\sigma^2}}}\sim\frac{\#i}{\#j}$$</div>
<p>
where <span class="math">\(D^{0,1}_K(x)\)</span> are the nearest neihgbors of <span class="math">\(x\)</span> with classes <span class="math">\(0,1\)</span> respectively, and <span class="math">\(\#i+\#j=K\)</span>. In theory this would reproduce the K-nearest neighbors predictor. However, this would require that for each <span class="math">\(x\)</span> the threshold <span class="math">\(d\)</span> is approximately constant, which may not happen in practice. The algorithm is however exact as <span class="math">\(\sigma\rightarrow 0\)</span> for which only the nearest neighbor is picked.</p>
<p>In regression we calculate instead the average of <span class="math">\(K\)</span>-nearest neighbor targets. That is,
</p>
<div class="math">$$\hat{y}(x)=\frac{1}{K}\sum_{i\in D_K(x)}y_i$$</div>
<p>Consider different datasets whereby the positions of the datapoints <span class="math">\(x\)</span> do not change but the target <span class="math">\(y\)</span> is drawn randomly as <span class="math">\(f+\epsilon\)</span> where <span class="math">\(f\)</span> is the true target and <span class="math">\(\epsilon\)</span> is a normally distributed random variable with mean zero and variance <span class="math">\(\sigma^2\)</span>. The bias is thus calculated as
</p>
<div class="math">$$\text{Bias}(x)=f(x)-\text{E}[\hat{f}(x)]=f(x)-\frac{1}{K}\sum_{i\in D_K(x)}f(x_i)$$</div>
<p> 
For <span class="math">\(K\)</span> small the nearest neighbors will have targets <span class="math">\(f(x_i)\)</span> that are approximately equal to <span class="math">\(f(x)\)</span>, by continuity. As such, the bias is small for small values of <span class="math">\(K\)</span>. However, as <span class="math">\(K\)</span> grows we are probing datapoints that are farther and farther away and thus more distinct from <span class="math">\(f(x)\)</span>, which in general will make the bias increase. </p>
<p>On the other hand, the variance at a point <span class="math">\(x\)</span>, that is,
</p>
<div class="math">$$\text{Var}(\hat{f})|_x=\text{E}[(\hat{f}(x)-\text{E}[\hat{f}(x)])^2]$$</div>
<p>
becomes equal to
</p>
<div class="math">$$\text{Var}(\hat{f})=\frac{\sigma^2}{K}$$</div>
<p>
Therefore, for large values of <span class="math">\(K\)</span> the variance decreases, while it is larger for smaller values of <span class="math">\(K\)</span>.</p>
<p><a name="decision"></a></p>
<h3><strong>2. Decision Boundary</strong></h3>
<p>In the picture below, we draw the decision boundary for a <span class="math">\(K=1\)</span> nearest neighbor. For any point located inside the polygon (hard lines) the nearest neighbor is <span class="math">\(P_1\)</span> and so the predicted target is <span class="math">\(f(P_1)\)</span> in that region.
<img alt="" height="400" src="/images/knn_decision.png" style="display: block; margin: 0 auto" width="400"></p>
<p>To construct the decision boundary we draw lines joining each point to <span class="math">\(P_1\)</span> and for each of these we draw the corresponding bisector. For example, consider the points <span class="math">\(P_1\)</span> and <span class="math">\(P_2\)</span>. For any point along the bisector of <span class="math">\(\overline{P_1P_2}\)</span>, the distance to <span class="math">\(P_1\)</span> is the same as the distance to <span class="math">\(P_2\)</span>. Therefore, the polygon formed by drawing all the bisectors bounds a region in which the nearest point is <span class="math">\(P_1\)</span>. </p>
<p>For <span class="math">\(K&gt;1\)</span>, we have to proceed slightly different. First we construct the <span class="math">\(K=1\)</span> decision boundary- this determines the nearest neighbor. Call this point <span class="math">\(N_1\)</span>, the first neighbor. Second, we pretend that the point <span class="math">\(N_1\)</span> is not part of the dataset and proceed as in the first step. The corresponding nearest neighbor <span class="math">\(N_2\)</span> is then the second nearest neighbor while including <span class="math">\(N_1\)</span>. We proceed iteratively after <span class="math">\(K\)</span> steps. The decision boundary is then determined by joining the <span class="math">\(K=1\)</span> polygons of each <span class="math">\(N_1,N_2,\ldots N_K\)</span>.</p>
<p><a name="curse"></a></p>
<h3><strong>3. Curse of Dimensionality</strong></h3>
<p>In this section we discuss the K-nearest neighbors algorithm in higher dimensions. 
Consider a sphere of radius <span class="math">\(r=1\)</span> in <span class="math">\(d\)</span> dimensions. We want to calculate the probability of finding the nearest neighbor at a distance <span class="math">\(r&lt;=1\)</span> from centre. This probability density is calculated as follows. Let <span class="math">\(p_r\)</span> be the probability of finding a point at a distance <span class="math">\(r\)</span> and <span class="math">\(p_{&gt;r}\)</span> the probability of finding a point at a distance <span class="math">\(&gt;r\)</span>. Then the probability that we want can be written as 
</p>
<div class="math">$$\begin{aligned}&amp;Np_r p_{&gt;r}^{N-1}+\frac{1}{2}N(N-1)p_r^2p_{&gt;r}^{N-2}+\ldots\\
&amp;=(p_r+p_{&gt;r})^N-p_{&gt;r}^N\end{aligned}$$</div>
<p>
which is the probability of finding at least one point at <span class="math">\(r\)</span> and none for <span class="math">\(&lt; r\)</span>. The probability <span class="math">\(p_r\)</span> is infinitesimally small, since
</p>
<div class="math">$$p_r=r^{d-1}dr$$</div>
<p>
while <span class="math">\(p_{&gt;r}=(1-r^d)\)</span>. Hence, we can expand the expression above and determine the probability density
</p>
<div class="math">$$\frac{dP(r)}{dr}=N r^{d-1}(1-r^{d})^{N-1}$$</div>
<p>Take <span class="math">\(d\gg1\)</span>. The probability density has a maximum at 
</p>
<div class="math">$$r^*=\frac{1}{N^{1/d}}$$</div>
<p>
For the K-nearest neighbors algorithm to perform well, there should exist at each point a sufficiently large number of neighbors at distances <span class="math">\(\epsilon\ll 1\)</span>, so one can use the continuity property of a smooth function. Therefore if we insist that <span class="math">\(r^*=\epsilon\ll 1\)</span>, this implies that <span class="math">\(N\)</span> must be exponentially large as a function of <span class="math">\(d\)</span>. In other words, for higher dimensions the probability of finding a neighbor at distance <span class="math">\(\epsilon\)</span> is smaller because there is more "space" available. To compensate that, we need an exponentially larger number of datapoints.</p>
<p><a name="python"></a></p>
<h3><strong>4. Python Implementation: Classification</strong></h3>
<p>Define KNN class with fit and call methods. The fit method memorizes the training data and the call method retrieves the predictor.</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">KNN</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">k</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="o">=</span><span class="n">k</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="o">=</span><span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="o">=</span><span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classes</span><span class="o">=</span><span class="kc">None</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="o">=</span><span class="n">x</span>
        <span class="n">classes</span><span class="o">=</span><span class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classes</span><span class="o">=</span><span class="p">{</span><span class="n">a</span><span class="p">:</span><span class="n">b</span> <span class="k">for</span> <span class="n">b</span><span class="p">,</span><span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">classes</span><span class="p">)}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="nb">len</span><span class="p">(</span><span class="n">classes</span><span class="p">)))</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
            <span class="n">j</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">classes</span><span class="p">[</span><span class="n">a</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="o">=</span><span class="mi">1</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">==</span><span class="mi">1</span><span class="p">:</span>
            <span class="n">t</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">t</span><span class="o">=</span><span class="n">x</span>
        <span class="n">y_pred</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">t</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">z</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
            <span class="n">z</span><span class="o">=</span><span class="n">z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">norm</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="o">-</span><span class="n">z</span>
            <span class="n">norm</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">norm</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">args</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">argpartition</span><span class="p">(</span><span class="n">norm</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">)[:</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">]</span>
            <span class="n">ypred</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">args</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">ypred</span><span class="o">=</span><span class="n">ypred</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
            <span class="n">y_pred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">ypred</span>

        <span class="k">return</span> <span class="n">y_pred</span>
</pre></div>


<p>As an example, load Iris dataset and also the built-in SKlearn K-nearest neighbors algorithm.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>

<span class="n">iris</span><span class="o">=</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">features</span><span class="o">=</span><span class="n">iris</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>
<span class="n">target</span><span class="o">=</span><span class="n">iris</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span>

<span class="c1">#train &amp; test split</span>
<span class="n">indices</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">features</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="n">l</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">0.2</span><span class="o">*</span><span class="n">features</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">xtrain</span><span class="o">=</span><span class="n">features</span><span class="p">[</span><span class="n">indices</span><span class="p">][:</span><span class="o">-</span><span class="n">l</span><span class="p">]</span>
<span class="n">xtest</span><span class="o">=</span><span class="n">features</span><span class="p">[</span><span class="n">indices</span><span class="p">][</span><span class="o">-</span><span class="n">l</span><span class="p">:]</span>

<span class="n">ytrain</span><span class="o">=</span><span class="n">target</span><span class="p">[</span><span class="n">indices</span><span class="p">][:</span><span class="o">-</span><span class="n">l</span><span class="p">]</span>
<span class="n">ytest</span><span class="o">=</span><span class="n">target</span><span class="p">[</span><span class="n">indices</span><span class="p">][</span><span class="o">-</span><span class="n">l</span><span class="p">:]</span>

<span class="n">knn</span><span class="o">=</span><span class="n">KNN</span><span class="p">(</span><span class="mi">30</span><span class="p">)</span> <span class="c1">#the class above</span>
<span class="n">Kneighbor</span><span class="o">=</span><span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span> <span class="c1">#the SKlearn class</span>

<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span><span class="n">ytrain</span><span class="p">)</span>
<span class="n">accuracy_score</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span><span class="n">knn</span><span class="p">(</span><span class="n">xtest</span><span class="p">))</span>

<span class="n">Kneighbor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span><span class="n">ytrain</span><span class="p">)</span>
<span class="n">accuracy_score</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span><span class="n">Kneighbor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xtest</span><span class="p">))</span>
</pre></div>


<p>Retrieving exactly the same accuracy.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
	<hr>
	<h2>Comments</h2>
</div>
		</div>
	</div> 	<!-- <hr> -->
</div> <!-- /container -->
<footer class="aw-footer bg-danger">
	<div class="container"> <!-- footer -->
		<div class="row">
			<div class="col-md-10 col-md-offset-1">
				<div class="row">
					<div class="col-md-3">
						<h4>Navigation</h4>
						<ul class="list-unstyled my-list-style">
							<li><a href="">Data Science and Machine Learning</a></li>
							<li><a href="/pages/about-me.html"><i class="fa fa-About me "></i> About me</a></li>
							<li><a href="/feeds/all.atom.xml" type="application/atom+xml"><i class="fa fa-rss "></i> atom</a></li>
						</ul>
					</div>
					<div class="col-md-3">
						<h4>Author</h4>
						<ul class="list-unstyled my-list-style">
							<li><a href="https://www.linkedin.com/in/joaomvg/">LinkedIn</a></li>
							<li><a href="https://github.com/joaomvg">GitHub</a></li>
						</ul>
					</div>
					<div class="col-md-3">
						<h4>Categories</h4>
						<ul class="list-unstyled my-list-style">
							<li><a href="/category/data-science.html">Data Science (1)</a></li>
							<li><a href="/category/machine-learning.html">Machine Learning (18)</a></li>
							<li><a href="/category/statistics.html">Statistics (2)</a></li>
						</ul>
					</div>
					<div class="col-md-3">
						<h4>Links</h4>
						<ul class="list-unstyled my-list-style">
							<li><a href="archives.html">Archives</a></li>
							<li><a href="tags.html">Tags</a></li>
						</ul>
					</div>
				</div>
			</div>
		</div>
	</div>
</footer>
<div class="container">
	<div class="row">
		<div class="col-md-12 text-center center-block aw-bottom">
			<p>&copy; Joao Gomes 2016</p>
			<p>Powered by Pelican</p>
		</div>
	</div>
</div>
<!-- JavaScript -->
<script src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
<script type="text/javascript">
jQuery(document).ready(function($) {
	$("div.collapseheader").click(function () {
		$header = $(this).children("span").first();
		$codearea = $(this).children(".input_area");
		$codearea.slideToggle(500, function () {
			$header.text(function () {
				return $codearea.is(":visible") ? "Collapse Code" : "Expand Code";
			});
		});
	});
});
</script>
</body>
</html>